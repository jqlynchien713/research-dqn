{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG\n",
    "> Experiment list: Interleaving + Diff / Interleaving + Diff + Param Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../../interleaving')\n",
    "\n",
    "import interleaving as interleaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import line_profiler\n",
    "from functools import reduce\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "  np.random.seed(seed)  \n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Available Files\n",
    "\n",
    "||Title|File Name|\n",
    "|---|-----|---------|\n",
    "|order|Stream group by streamers|`streamer_stream_dict.pkl`|\n",
    "||Stream list|`stream_list.pkl`|\n",
    "|Context|Final user context|`user_context.pkl`|\n",
    "||Streamer context|`streamer.pkl`|\n",
    "||Item context|`item_pca_df.pkl`|\n",
    "|Aux|stream: items list|`stream_item_dict.pkl`|\n",
    "||stream: users list|`stream_users_dict.pkl`|\n",
    "||item list the consumer actually bought|`user_bought_dict.pkl`|\n",
    "|Explore-Diff|PCA based diff vector|`diff_vectors_pca.pkl`|\n",
    "||SVD based diff vector|`diff_vectors_svd.pkl`|\n",
    "||VAE based diff vector|`diff_vectors_vae.pkl`|\n",
    "|Threshold|VAE reconstruction error df|`vae_recons_df.pkl`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# STREAMER_STREAM_DICT = pd.read_pickle('../../data/streamer_stream_dict.pkl')\n",
    "STREAM_LIST = pd.read_pickle('../../data/stream_list.pkl')\n",
    "USER_CONTEXT = pd.read_pickle('../../data/user_context.pkl')\n",
    "# STREAMER = pd.read_pickle('../../data/streamer.pkl')\n",
    "ITEM_PCA_DF = pd.read_pickle('../../data/item_pca_df.pkl')\n",
    "STREAM_ITEM_DICT = pd.read_pickle('../../data/stream_item_dict.pkl')\n",
    "USER_BOUGHT_DICT = pd.read_pickle('../../data/user_bought_dict.pkl')\n",
    "STREAM_USER_DICT = pd.read_pickle('../../data/stream_users_dict.pkl')\n",
    "# Current used diff\n",
    "ITEM_DIFF = pd.read_pickle('../../data/item_diff_vectors.pkl')\n",
    "STREAM_ORDER_CNT = pd.read_pickle('../../data/stream_order_cnt_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFF_PCA = pd.read_pickle('../../data/diff_vectors_pca.pkl')\n",
    "# DIFF_SVD = pd.read_pickle('../../data/diff_vectors_svd.pkl')\n",
    "# DIFF_VAE = pd.read_pickle('../../data/diff_vectors_vae.pkl')\n",
    "# VAE_RECONS_DF = pd.read_pickle('../../data/vae_recons_df.pkl')\n",
    "# VAE_RECONS64_DF = pd.read_pickle('../../data/vae_recons_64_df.pkl')\n",
    "# MUL150_PCA = pd.read_pickle('../../data/mul150_pca_diff.pkl')\n",
    "# MUL150_VAE = pd.read_pickle('../../data/mul150_vae_diff.pkl')\n",
    "# MUL64_PCA = pd.read_pickle('../../data/mul64_pca_diff.pkl')\n",
    "# MUL64_VAE = pd.read_pickle('../../data/mul64_vae_diff.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "\n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length.\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item.\n",
    "            self.start = (self.start + 1) % self.maxlen\n",
    "        else:\n",
    "            # This should never happen.\n",
    "            raise RuntimeError()\n",
    "        self.data[(self.start + self.length - 1) % self.maxlen] = v\n",
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, limit=10000):\n",
    "    self.limit = limit\n",
    "    # init 4 RingBuffers: states/actions/rewards/ongoing\n",
    "    self.states_buffer = RingBuffer(limit)\n",
    "    self.next_states_buffer = RingBuffer(limit)\n",
    "    self.actions_buffer = RingBuffer(limit)\n",
    "    self.rewards_buffer = RingBuffer(limit)\n",
    "    self.ongoings_buffer = RingBuffer(limit)\n",
    "    \n",
    "  def __len__(self):\n",
    "    assert len(self.states_buffer) == len(self.next_states_buffer) == len(self.actions_buffer) == \\\n",
    "           len(self.rewards_buffer) == len(self.ongoings_buffer)\n",
    "    return len(self.states_buffer)\n",
    "\n",
    "  def remember(self, state, actions, reward, next_state, ongoing):\n",
    "    assert len(self.states_buffer) == len(self.next_states_buffer) == len(self.actions_buffer) == \\\n",
    "           len(self.rewards_buffer) == len(self.ongoings_buffer)\n",
    "    # append each element separately in different list\n",
    "    self.states_buffer.append(state)\n",
    "    self.actions_buffer.append(actions)\n",
    "    self.rewards_buffer.append(reward)\n",
    "    self.next_states_buffer.append(next_state)\n",
    "    self.ongoings_buffer.append(ongoing)\n",
    "    \n",
    "  def get_batch(self, batch_size=100):\n",
    "    assert len(self.states_buffer) == len(self.next_states_buffer) == len(self.actions_buffer) == \\\n",
    "           len(self.rewards_buffer) == len(self.ongoings_buffer)\n",
    "    # same for sample_and_split\n",
    "    states0_batch = []\n",
    "    states1_batch = []\n",
    "    actions_batch = []\n",
    "    rewards_batch = []\n",
    "    ongoings_batch = []\n",
    "    batch_indexs = np.random.randint(0, len(self.states_buffer), size=min(len(self.states_buffer), batch_size))\n",
    "    for i in batch_indexs:\n",
    "      states0_batch.append(USER_CONTEXT.xs(self.states_buffer[i]))\n",
    "      states1_batch.append(USER_CONTEXT.xs(self.next_states_buffer[i]))\n",
    "      actions_batch.append(self.actions_buffer[i])\n",
    "      rewards_batch.append(self.rewards_buffer[i])\n",
    "      ongoings_batch.append(self.ongoings_buffer[i])\n",
    "    assert len(states0_batch) == len(states1_batch) == len(actions_batch) == \\\n",
    "           len(rewards_batch) == len(ongoings_batch)\n",
    "    return np.array(states0_batch), np.array(actions_batch), np.array(rewards_batch), np.array(states1_batch), np.array(ongoings_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "class AnnealedGaussianProcess(RandomProcess):\n",
    "    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n",
    "\n",
    "\n",
    "# Based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckProcess(AnnealedGaussianProcess):\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=None, n_steps_annealing=1000):\n",
    "        super(OrnsteinUhlenbeckProcess, self).__init__(mu=mu, sigma=sigma, sigma_min=sigma_min, n_steps_annealing=n_steps_annealing)\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def fanin_init(size, fanin=None):\n",
    "    fanin = fanin or size[0]\n",
    "    v = 1. / np.sqrt(fanin)\n",
    "    return torch.Tensor(size).uniform_(-v, v)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, mode, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.fc1 = nn.Linear(state_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, action_dim)\n",
    "        self.norm1 = nn.LayerNorm(hidden1)\n",
    "        self.norm2 = nn.LayerNorm(hidden2)\n",
    "        self.norm3 = nn.LayerNorm(action_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.tanh(out)\n",
    "        return out\n",
    "\n",
    "    def add_parameter_noise(self, scalar=.1):\n",
    "        self.fc1.weight.data += torch.randn_like(self.fc1.weight.data) * scalar\n",
    "        self.fc2.weight.data += torch.randn_like(self.fc2.weight.data) * scalar\n",
    "        self.fc3.weight.data += torch.randn_like(self.fc3.weight.data) * scalar\n",
    "        \n",
    "    def noised_add_disturb(self, weight=.1):\n",
    "        for curr_param in self.parameters():\n",
    "            curr_param += weight * np.random.uniform(-1, 1) * curr_param\n",
    "        \n",
    "    def actor_add_noised(self, source_network, weight=.05):\n",
    "        for curr_param in self.parameters():\n",
    "            curr_param = curr_param + curr_param * weight\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden1=400, hidden2=300, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1+action_dim, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights(init_w)\n",
    "    \n",
    "    def init_weights(self, init_w):\n",
    "        self.fc1.weight.data = fanin_init(self.fc1.weight.data.size())\n",
    "        self.fc2.weight.data = fanin_init(self.fc2.weight.data.size())\n",
    "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        x, a = xs\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # debug()\n",
    "        out = self.fc2(torch.cat([out,a],1))\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def to_tensor(input_df):\n",
    "  return torch.tensor(input_df).to(DEVICE).float()\n",
    "\n",
    "class DDPG(object):\n",
    "  def __init__(self, actor_lr, critic_lr, state_dim, action_dim, mode='baseline',\n",
    "               hidden1=400, hidden2=300, diff_multiplier=1, noised_decay=.05,\n",
    "               top_k=10, max_memory=10000, depsilon=50000, discount=0.99,\n",
    "               tau=0.001, batch_size=100, ou_theta=0.15, ou_mu=0.0, ou_sigma=0.2, \n",
    "               param_noise_scalar=.05, param_noise_scalar_alpha=1.01, desired_distance=.7,\n",
    "               major_update_interval=500, noise_type='disturb_noise'):\n",
    "    # State & Action dimension\n",
    "    self.state_dim = state_dim\n",
    "    self.action_dim = action_dim\n",
    "    \n",
    "    # Mode settings\n",
    "    # baseline\n",
    "    # diff: diff_pca, diff_svd, diff_vae\n",
    "    # threshold: thres_vae, thres_vdbe\n",
    "    # multiplier: MUL150_PCA, MUL150_VAE, MUL64_PCA, MUL64_VAE\n",
    "    # Parameter noise: PARAM_NOISE\n",
    "    self.mode = mode\n",
    "    self.diff_multiplier = diff_multiplier\n",
    "    \n",
    "    # Param Noise\n",
    "    self.distances = []\n",
    "    self.desired_distance = desired_distance\n",
    "    self.param_noise_scalar = param_noise_scalar\n",
    "    self.param_noise_scalar_alpha = param_noise_scalar_alpha\n",
    "\n",
    "    # Init Actor\n",
    "    self.actor = Actor(state_dim, action_dim, mode, hidden1, hidden2)\n",
    "    self.actor_noised = Actor(state_dim, action_dim, mode, hidden1, hidden2)\n",
    "    self.actor_target = Actor(state_dim, action_dim, mode, hidden1, hidden2)\n",
    "    self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "    # Init Critic\n",
    "    self.critic = Critic(state_dim, action_dim, hidden1, hidden2)\n",
    "    self.critic_target = Critic(state_dim, action_dim, hidden1, hidden2)\n",
    "    self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    # Memory\n",
    "    self.buffer = ReplayBuffer(limit=max_memory)\n",
    "\n",
    "    # Exploration\n",
    "    self.epsilon = 1.0\n",
    "    self.depsilon = 1.0 / depsilon\n",
    "    self.random_process = OrnsteinUhlenbeckProcess(size=action_dim, theta=ou_theta, mu=ou_mu, sigma=ou_sigma)\n",
    "\n",
    "    # Reward related\n",
    "    self.top_k = top_k\n",
    "    \n",
    "    # Loss Function\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # Hyperparameters\n",
    "    self.discount = discount\n",
    "    self.tau = tau\n",
    "    self.batch_size = batch_size\n",
    "    self.noised_decay = noised_decay\n",
    "    self.major_update_interval = major_update_interval\n",
    "    self.noise_type = noise_type\n",
    "\n",
    "    # Network to cuda\n",
    "    self.actor.to(DEVICE)\n",
    "    self.actor_noised.to(DEVICE)\n",
    "    self.critic.to(DEVICE)\n",
    "    self.actor_target.to(DEVICE)\n",
    "    self.critic_target.to(DEVICE)\n",
    "    \n",
    "  def random_action(self):\n",
    "    action = np.random.uniform(-1.,1.,self.action_dim).astype('float32')\n",
    "    action_list = ITEM_PCA_DF.iloc[self.curr_stream_items].apply(lambda x: np.dot(x, action), axis=1).nlargest(self.top_k).index\n",
    "    self.action_list = action_list\n",
    "    return action\n",
    "    \n",
    "  def generate_latent_action(self, state, decay_epsilon=True, vdbe_epsilon=False):\n",
    "    action = self.actor(to_tensor(state))\n",
    "    '''\n",
    "    Param Noise\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "      self.actor_noised.load_state_dict(self.actor.state_dict().copy())\n",
    "      if self.noise_type == 'disturb_noise':\n",
    "        self.actor_noised.noised_add_disturb(self.param_noise_scalar)\n",
    "      elif self.noise_type == 'param_noise':\n",
    "        self.actor_noised.add_parameter_noise(self.param_noise_scalar)\n",
    "      action_noised = self.actor_noised(to_tensor(state))\n",
    "\n",
    "        # distance = torch.sqrt(torch.mean(torch.square(action - action_noised))).cpu().detach().numpy()\n",
    "        # if distance < self.desired_distance:\n",
    "        #     self.param_noise_scalar *= self.param_noise_scalar_alpha\n",
    "        # else:\n",
    "        #     self.param_noise_scalar /= self.param_noise_scalar_alpha\n",
    "        # action = action_noised\n",
    "\n",
    "      \n",
    "    '''\n",
    "    Add the diff vector to action\n",
    "    '''\n",
    "    current_diff = globals()[self.mode.upper()]\n",
    "    if state.name in current_diff.index:\n",
    "      # for mode = [diff_pca, diff_svd, diff_vae, MUL150_VAE, MUL64_VAE, ITEM_DIFF]\n",
    "      action += to_tensor(current_diff.loc[state.name]) * self.diff_multiplier * max(self.epsilon, 0)\n",
    "      action_noised += to_tensor(current_diff.loc[state.name]) * self.diff_multiplier * max(self.epsilon, 0)\n",
    "\n",
    "    # clamping\n",
    "    action = torch.clamp(action, -1., 1.).cpu().detach().numpy()\n",
    "    action_noised = torch.clamp(action_noised, -1., 1.).cpu().detach().numpy()\n",
    "    \n",
    "    action_list = ITEM_PCA_DF.iloc[self.curr_stream_items].apply(lambda x: np.dot(x, action), axis=1).nlargest(self.top_k).index\n",
    "    action_noised_list = ITEM_PCA_DF.iloc[self.curr_stream_items].apply(lambda x: np.dot(x, action_noised), axis=1).nlargest(self.top_k).index\n",
    "    self.interleaver = interleaver.Probabilistic([action_list, action_noised_list])\n",
    "    # self.actions equals to rank in interleaving doc\n",
    "    \n",
    "    self.action_list = self.interleaver.interleave()\n",
    "    \n",
    "    if decay_epsilon:\n",
    "      self.epsilon -= self.depsilon\n",
    "        \n",
    "    return action\n",
    "  \n",
    "  def get_actions_rewards(self):\n",
    "    true_list = USER_BOUGHT_DICT[self.curr_user]\n",
    "    hit_list, rewarded_items = [], set()\n",
    "    for a in self.action_list:\n",
    "      if a in true_list:\n",
    "        hit_list.append(1)\n",
    "        rewarded_items.add(a)\n",
    "      else: hit_list.append(0)\n",
    "    res = sum(hit_list)\n",
    "    self.clicks = np.where(np.array(hit_list) == 1)[0]\n",
    "    return res, rewarded_items\n",
    "    \n",
    "  def update_policy(self, major_update=True):\n",
    "    # Calculate Interleaving results\n",
    "    if len(self.clicks) > 0:\n",
    "      result = self.interleaver.evaluate(self.action_list, self.clicks)\n",
    "      # naive vs noised\n",
    "      if len(result) > 0 and result[0][1] == 0: # means noised won\n",
    "        self.actor.actor_add_noised(self.actor_noised, weight=self.noised_decay)\n",
    "        if self.noised_decay == .05:\n",
    "          self.noised_decay -= self.depsilon\n",
    "      \n",
    "    if not major_update: return 0.0, 0.0\n",
    "  \n",
    "    # Sample batch\n",
    "    state_batch, action_batch, reward_batch, \\\n",
    "    next_state_batch, ongoing_batch = self.buffer.get_batch(self.batch_size)\n",
    "        \n",
    "    # Prepare for the target q batch\n",
    "    next_q_values = self.critic_target([\n",
    "        to_tensor(next_state_batch).detach(),\n",
    "        self.actor_target(to_tensor(next_state_batch).detach()),\n",
    "    ])\n",
    "    \n",
    "    # 如果 ongoing 為 True(1) 時，乘以 next_q 會有值\n",
    "    # 但是 ongoing 為 False(0) 時，乘以 next_q 會等於 0\n",
    "    target_q_batch = to_tensor(reward_batch).view(next_q_values.shape) + \\\n",
    "                       self.discount*to_tensor(ongoing_batch).view(next_q_values.shape)*next_q_values.detach()\n",
    "\n",
    "    # Critic update\n",
    "    self.critic_optim.zero_grad()\n",
    "\n",
    "    q_batch = self.critic([ to_tensor(state_batch), to_tensor(action_batch) ])\n",
    "\n",
    "    value_loss = self.loss_fn(q_batch, target_q_batch)\n",
    "    value_loss.backward()\n",
    "    self.critic_optim.step()\n",
    "\n",
    "    # Actor update\n",
    "    self.actor_optim.zero_grad()\n",
    "\n",
    "    policy_loss = -self.critic([\n",
    "        to_tensor(state_batch),\n",
    "        self.actor(to_tensor(state_batch))\n",
    "    ])\n",
    "\n",
    "    policy_loss = policy_loss.mean()\n",
    "    policy_loss.backward()\n",
    "    self.actor_optim.step()\n",
    "\n",
    "    # Target update\n",
    "    self._soft_update(self.actor_target, self.actor)\n",
    "    self._soft_update(self.critic_target, self.critic)\n",
    "    return value_loss, policy_loss\n",
    "    \n",
    "  def _soft_update(self, target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, reward_records=[], avg_rewards=[], hit_ratios=[], regret_list=[], coverage_list=[], diversity_list=[], warmup=100, ep_len=4000):\n",
    "  agent.actor.train(True)\n",
    "  agent.critic.train(True)\n",
    "  batch_value_loss, batch_policy_loss = 0., 0.\n",
    "  step_value_loss, step_policy_loss = 0., 0.\n",
    "  step = 0\n",
    "  # self.epsilon.clear()\n",
    "\n",
    "  # ------------------- Episode (State) -------------------------------\n",
    "  for stream in tqdm(STREAM_LIST[:ep_len]):\n",
    "    # self.__user_episode_context()\n",
    "    users = STREAM_USER_DICT[stream]\n",
    "\n",
    "    agent.curr_stream = stream\n",
    "    agent.curr_stream_items = STREAM_ITEM_DICT[stream]\n",
    "    batch_value_loss, batch_policy_loss = 0., 0.\n",
    "    stream_reward = 0\n",
    "    false_positive_cnt = 0\n",
    "    diversity_prod_set = set()\n",
    "    coverage_rewarded_items = set()\n",
    "\n",
    "    # ----------------- Runs (User under stream) ---------------------\n",
    "    for i, user in enumerate(users):\n",
    "      # update step\n",
    "      step += 1\n",
    "      \n",
    "      # state & ongoing\n",
    "      agent.curr_user = user\n",
    "      state = USER_CONTEXT.loc[(user, stream)]\n",
    "      ongoing = i != len(users) - 1\n",
    "\n",
    "      # --------------- Actor net choose actions ----------------------\n",
    "      # make actor to choose action\n",
    "      # action = latent vector\n",
    "      if step <= warmup:\n",
    "        action = agent.random_action()\n",
    "      else:\n",
    "        action = agent.generate_latent_action(state)\n",
    "          \n",
    "      # --------------- Get next state & info to store ---------------\n",
    "      # Generate reward\n",
    "      reward, curr_rewarded_items = agent.get_actions_rewards()\n",
    "\n",
    "      # next_state: next user's state\n",
    "      next_user = users[i + 1] if i + 1 < len(users) else None\n",
    "      \n",
    "      # update metrics\n",
    "      reward_records.append(reward)\n",
    "      stream_reward += reward\n",
    "      false_positive_cnt += (10 - reward)\n",
    "      coverage_rewarded_items = coverage_rewarded_items | curr_rewarded_items\n",
    "      diversity_prod_set = diversity_prod_set | set(agent.action_list)\n",
    "      \n",
    "      # Remember only when reward > 0 or the memory is short\n",
    "      if reward > 0 or len(agent.buffer) < 10:\n",
    "        storing_state = (user, stream)\n",
    "        storing_next_state = (next_user, stream) if next_user else storing_state\n",
    "        agent.buffer.remember(storing_state, action, reward, storing_next_state, ongoing)\n",
    "\n",
    "      # --------------- Train on Actor and Critic --------------------\n",
    "      # store pre-training value for td_error\n",
    "      # old_Q = self.q_value()\n",
    "      if step > warmup :\n",
    "        major_update = step < 120000 or (step % agent.major_update_interval == 0)\n",
    "        step_value_loss, step_policy_loss = agent.update_policy(major_update)\n",
    "      # store post-training value for td_error\n",
    "      # new_Q = self.q_value()\n",
    "      batch_value_loss += step_value_loss\n",
    "      batch_policy_loss += step_policy_loss\n",
    "\n",
    "      # --------------- Update with TD error -------------------------\n",
    "      # self.epsilon.update_at_step(self.asid, [(new_Q - old_Q), self.learn_step_counter], 1/len(self.stream_items))\n",
    "\n",
    "    # --------------- Inspecting result at step -------------------------\n",
    "    ep_avg_reward = round(sum(reward_records)/step, 2)\n",
    "    avg_rewards.append(ep_avg_reward)\n",
    "    \n",
    "    ground_truth = STREAM_ORDER_CNT.iloc[stream]\n",
    "    # hit ratio\n",
    "    hr10 = stream_reward / ground_truth\n",
    "    hit_ratios.append(hr10)\n",
    "    # regret\n",
    "    regret = false_positive_cnt / len(users)\n",
    "    regret_list.append(regret)\n",
    "    # coverage\n",
    "    coverage = len(coverage_rewarded_items) / ground_truth\n",
    "    coverage_list.append(coverage)\n",
    "    # diversity\n",
    "    diversity = len(diversity_prod_set) / len(STREAM_ITEM_DICT[stream])\n",
    "    diversity_list.append(diversity)\n",
    "    \n",
    "#     print(f'Stream: {stream}, user_count: {len(users)}, avg_reward@stream: {round(stream_reward/len(users), 2)}, avg_reward: {ep_avg_reward}, \\\n",
    "# hr10: {round(hr10, 3)}, regret: {round(regret, 3)}, coverage: {round(coverage, 3)}, diversity: {round(diversity, 3)}, \\\n",
    "# batch_value_loss: {round(float(batch_value_loss)/len(users), 3)}')\n",
    "    \n",
    "    # if len(reward_records) >= 247581: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "actor_lr = 1.0e-3\n",
    "critic_lr = 1.0e-3\n",
    "state_dim = USER_CONTEXT.shape[1]\n",
    "action_dim = ITEM_PCA_DF.shape[1]\n",
    "hidden1 = 400\n",
    "hidden2 = 300\n",
    "diff_multiplier = 4.236938\n",
    "noised_decay = 0.5 # decay: 0.5 / not_decay: 1\n",
    "top_k = 10\n",
    "max_memory = 10000\n",
    "depsilon = 50000\n",
    "discount = 0.99\n",
    "tau = 0.001\n",
    "batch_size = 100\n",
    "ou_theta = 0.15\n",
    "ou_mu = 0.0\n",
    "ou_sigma = 0.2\n",
    "param_noise_scalar = 0.05\n",
    "param_noise_scalar_alpha = 1.01\n",
    "desired_distance = 0.7\n",
    "major_update_interval = 500 # major: 500 / not_major: 1\n",
    "noise_type = 'disturb_noise' # baseline: 'disturb_noise' / 'param_noise'\n",
    "\n",
    "mode = 'diff_pca'\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## AX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from ax.utils.notebook.plotting import render\n",
    "from ax.utils.tutorials.cnn_utils import train, evaluate\n",
    "from ax.service.managed_loop import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義目標函數\n",
    "def target_function(parameters):\n",
    "    mode = 'diff_pca'\n",
    "    diff_multiplier = parameters.get(\"diff_multiplier\", 2)\n",
    "    \n",
    "    actor_lr = 1.0e-3\n",
    "    critic_lr = 1.0e-3\n",
    "    state_dim = USER_CONTEXT.shape[1]\n",
    "    action_dim = ITEM_PCA_DF.shape[1]\n",
    "    hidden1 = 400\n",
    "    hidden2 = 300\n",
    "    noised_decay = 0.5 # decay: 0.5 / not_decay: 1\n",
    "    top_k = 10\n",
    "    max_memory = 10000\n",
    "    depsilon = 50000\n",
    "    discount = 0.99\n",
    "    tau = 0.001\n",
    "    batch_size = 100\n",
    "    ou_theta = 0.15\n",
    "    ou_mu = 0.0\n",
    "    ou_sigma = 0.2\n",
    "    param_noise_scalar = 0.05\n",
    "    param_noise_scalar_alpha = 1.01\n",
    "    desired_distance = 0.7\n",
    "    major_update_interval = 1 # major: 500 / not_major: 1\n",
    "    noise_type = 'disturb_noise' # baseline: 'disturb_noise' / 'param_noise'\n",
    "        \n",
    "    ddpg = DDPG(actor_lr, critic_lr, state_dim, action_dim, mode, hidden1, hidden2, diff_multiplier, noised_decay,\n",
    "                top_k, max_memory, depsilon, discount, tau, batch_size, ou_theta, ou_mu, ou_sigma, \n",
    "                param_noise_scalar, param_noise_scalar_alpha, desired_distance, major_update_interval, noise_type)\n",
    "    \n",
    "    total_rewards = []\n",
    "    avg_rewards = []\n",
    "    hit_ratios, regret_list, coverage_list, diversity_list = [], [], [], []\n",
    "    train(ddpg, total_rewards, avg_rewards, hit_ratios, regret_list, coverage_list, diversity_list, ep_len=300)\n",
    "\n",
    "    sum_rewards = sum(total_rewards)\n",
    "    avg_hit = sum(hit_ratios)/len(hit_ratios)\n",
    "\n",
    "    return sum_rewards, avg_hit\n",
    "\n",
    "parameters = [\n",
    "  {\"name\": \"diff_multiplier\", \"type\": \"range\", \"bounds\": [0.1, 5.0]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 08:06:17] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 2 decimal points.\n",
      "[INFO 06-28 08:06:17] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 5 will take longer to generate due to model-fitting.\n",
      "[INFO 06-28 08:06:17] ax.service.ax_client: Generated new trial 0 with parameters {'diff_multiplier': 3.54}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ed3becfb454df8a455bac4d38d9007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 08:40:50] ax.service.ax_client: Completed trial 0 with data: {'target_function': (22807, 0.83)}.\n",
      "[INFO 06-28 08:40:50] ax.service.ax_client: Generated new trial 1 with parameters {'diff_multiplier': 2.39}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c27738df82047b8aa479f4198c9f1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 09:15:25] ax.service.ax_client: Completed trial 1 with data: {'target_function': (23111, 0.83)}.\n",
      "[INFO 06-28 09:15:25] ax.service.ax_client: Generated new trial 2 with parameters {'diff_multiplier': 2.23}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb1e32e4c0a486bab4018fbaffde60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 09:50:42] ax.service.ax_client: Completed trial 2 with data: {'target_function': (24269, 0.86)}.\n",
      "[INFO 06-28 09:50:42] ax.service.ax_client: Generated new trial 3 with parameters {'diff_multiplier': 1.06}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6751f9a668c14fa5a80d809f6c716287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 10:26:25] ax.service.ax_client: Completed trial 3 with data: {'target_function': (25488, 0.86)}.\n",
      "[INFO 06-28 10:26:25] ax.service.ax_client: Generated new trial 4 with parameters {'diff_multiplier': 1.51}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3bd0d3b21e4970b518a646b533843c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 11:02:54] ax.service.ax_client: Completed trial 4 with data: {'target_function': (24085, 0.85)}.\n",
      "[INFO 06-28 11:02:55] ax.service.ax_client: Generated new trial 5 with parameters {'diff_multiplier': 0.74}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edadb72a9f9e40e3a4d261e61289f4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 11:41:07] ax.service.ax_client: Completed trial 5 with data: {'target_function': (24668, 0.85)}.\n",
      "[INFO 06-28 11:41:07] ax.service.ax_client: Generated new trial 6 with parameters {'diff_multiplier': 5.0}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2e7a724bde4348a3a4f030d183a1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 15:27:26] ax.service.ax_client: Completed trial 11 with data: {'target_function': (24466, 0.86)}.\n",
      "[INFO 06-28 15:27:27] ax.service.ax_client: Generated new trial 12 with parameters {'diff_multiplier': 2.14}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bc477ce9284678b22788f112eeef96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 16:04:30] ax.service.ax_client: Completed trial 12 with data: {'target_function': (22472, 0.83)}.\n",
      "[INFO 06-28 16:04:30] ax.service.ax_client: Generated new trial 13 with parameters {'diff_multiplier': 1.61}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a359c7436f648a9a222d0c60fdeb18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 16:40:55] ax.service.ax_client: Completed trial 13 with data: {'target_function': (24127, 0.85)}.\n",
      "[INFO 06-28 16:40:55] ax.service.ax_client: Generated new trial 14 with parameters {'diff_multiplier': 1.05}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b50d21483c4800888161ef76981f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 17:16:08] ax.service.ax_client: Completed trial 14 with data: {'target_function': (25118, 0.87)}.\n",
      "[INFO 06-28 17:16:08] ax.service.ax_client: Generated new trial 15 with parameters {'diff_multiplier': 1.04}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c788e460aa46485880337a4a7a0c3931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 17:51:03] ax.service.ax_client: Completed trial 15 with data: {'target_function': (24195, 0.85)}.\n",
      "[INFO 06-28 17:51:03] ax.service.ax_client: Generated new trial 16 with parameters {'diff_multiplier': 1.06}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8277a7a83647d8a7ce1cd0e274e184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-28 18:26:08] ax.service.ax_client: Completed trial 16 with data: {'target_function': (24630, 0.85)}.\n",
      "[INFO 06-28 18:26:08] ax.service.ax_client: Generated new trial 17 with parameters {'diff_multiplier': 1.06}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8789b861f82461c9937c22896da88b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "[INFO 06-29 01:23:48] ax.service.ax_client: Completed trial 28 with data: {'target_function': (24561, 0.86)}.\n",
      "[INFO 06-29 01:23:48] ax.service.ax_client: Generated new trial 29 with parameters {'diff_multiplier': 3.58}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1510ff8a28a344129ef2103ad904237f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-29 01:57:33] ax.service.ax_client: Completed trial 29 with data: {'target_function': (23569, 0.83)}.\n",
      "[INFO 06-29 01:57:33] ax.service.ax_client: Generated new trial 30 with parameters {'diff_multiplier': 1.06}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d9b888531447ecb69851dca51f8844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 06-29 02:30:55] ax.service.ax_client: Completed trial 30 with data: {'target_function': (23366, 0.83)}.\n",
      "[INFO 06-29 02:30:55] ax.service.ax_client: Generated new trial 31 with parameters {'diff_multiplier': 1.05}.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5938093f77404fa00639ffe1fa44aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ax.service.ax_client import AxClient\n",
    "\n",
    "ax_client = AxClient()\n",
    "ax_client.create_experiment(name='my_bayesianopt',\n",
    "                            parameters=parameters,\n",
    "                            objective_name='target_function',\n",
    "                            minimize=False)\n",
    "\n",
    "for _ in range(50):\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=target_function(parameters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_multiplier: 1.0493908821213564\n"
     ]
    }
   ],
   "source": [
    "best_parameters, metrics = ax_client.get_best_parameters()\n",
    "\n",
    "for k, v in best_parameters.items():\n",
    "  print(f'{k}: {v}')\n",
    "# print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/merge.py:643: UserWarning:\n",
      "\n",
      "merging between different levels can give an unintended result (2 levels on the left,1 on the right)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arm_name</th>\n",
       "      <th>target_function</th>\n",
       "      <th>trial_index</th>\n",
       "      <th>diff_multiplier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_0</td>\n",
       "      <td>22807</td>\n",
       "      <td>0</td>\n",
       "      <td>3.538337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_0</td>\n",
       "      <td>23902</td>\n",
       "      <td>10</td>\n",
       "      <td>0.671956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11_0</td>\n",
       "      <td>24466</td>\n",
       "      <td>11</td>\n",
       "      <td>0.806354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_0</td>\n",
       "      <td>23111</td>\n",
       "      <td>1</td>\n",
       "      <td>2.394134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12_0</td>\n",
       "      <td>22472</td>\n",
       "      <td>12</td>\n",
       "      <td>2.140057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13_0</td>\n",
       "      <td>24127</td>\n",
       "      <td>13</td>\n",
       "      <td>1.605374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14_0</td>\n",
       "      <td>25118</td>\n",
       "      <td>14</td>\n",
       "      <td>1.048804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15_0</td>\n",
       "      <td>24195</td>\n",
       "      <td>15</td>\n",
       "      <td>1.038859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16_0</td>\n",
       "      <td>24630</td>\n",
       "      <td>16</td>\n",
       "      <td>1.063608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17_0</td>\n",
       "      <td>24911</td>\n",
       "      <td>17</td>\n",
       "      <td>1.056256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18_0</td>\n",
       "      <td>23570</td>\n",
       "      <td>18</td>\n",
       "      <td>0.751249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19_0</td>\n",
       "      <td>23345</td>\n",
       "      <td>19</td>\n",
       "      <td>1.045874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20_0</td>\n",
       "      <td>25429</td>\n",
       "      <td>20</td>\n",
       "      <td>1.049615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>21_0</td>\n",
       "      <td>24843</td>\n",
       "      <td>21</td>\n",
       "      <td>1.059887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22_0</td>\n",
       "      <td>24597</td>\n",
       "      <td>22</td>\n",
       "      <td>1.050487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2_0</td>\n",
       "      <td>24269</td>\n",
       "      <td>2</td>\n",
       "      <td>2.231188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23_0</td>\n",
       "      <td>24544</td>\n",
       "      <td>23</td>\n",
       "      <td>1.062143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24_0</td>\n",
       "      <td>24542</td>\n",
       "      <td>24</td>\n",
       "      <td>0.745060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25_0</td>\n",
       "      <td>23623</td>\n",
       "      <td>25</td>\n",
       "      <td>4.674932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26_0</td>\n",
       "      <td>23798</td>\n",
       "      <td>26</td>\n",
       "      <td>1.044415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>27_0</td>\n",
       "      <td>24048</td>\n",
       "      <td>27</td>\n",
       "      <td>0.805470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>28_0</td>\n",
       "      <td>24561</td>\n",
       "      <td>28</td>\n",
       "      <td>1.051403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29_0</td>\n",
       "      <td>23569</td>\n",
       "      <td>29</td>\n",
       "      <td>3.578628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30_0</td>\n",
       "      <td>23366</td>\n",
       "      <td>30</td>\n",
       "      <td>1.059434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31_0</td>\n",
       "      <td>25129</td>\n",
       "      <td>31</td>\n",
       "      <td>1.049501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32_0</td>\n",
       "      <td>23090</td>\n",
       "      <td>32</td>\n",
       "      <td>1.048989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33_0</td>\n",
       "      <td>24397</td>\n",
       "      <td>33</td>\n",
       "      <td>1.061924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3_0</td>\n",
       "      <td>25488</td>\n",
       "      <td>3</td>\n",
       "      <td>1.059199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>34_0</td>\n",
       "      <td>22794</td>\n",
       "      <td>34</td>\n",
       "      <td>1.048692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35_0</td>\n",
       "      <td>25828</td>\n",
       "      <td>35</td>\n",
       "      <td>1.055283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>36_0</td>\n",
       "      <td>24475</td>\n",
       "      <td>36</td>\n",
       "      <td>1.054521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>37_0</td>\n",
       "      <td>25221</td>\n",
       "      <td>37</td>\n",
       "      <td>1.049686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>38_0</td>\n",
       "      <td>26031</td>\n",
       "      <td>38</td>\n",
       "      <td>1.049391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>39_0</td>\n",
       "      <td>24005</td>\n",
       "      <td>39</td>\n",
       "      <td>1.054346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40_0</td>\n",
       "      <td>23236</td>\n",
       "      <td>40</td>\n",
       "      <td>4.417897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>41_0</td>\n",
       "      <td>23711</td>\n",
       "      <td>41</td>\n",
       "      <td>3.629608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>42_0</td>\n",
       "      <td>23129</td>\n",
       "      <td>42</td>\n",
       "      <td>3.875543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>43_0</td>\n",
       "      <td>24373</td>\n",
       "      <td>43</td>\n",
       "      <td>1.049353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>44_0</td>\n",
       "      <td>23035</td>\n",
       "      <td>44</td>\n",
       "      <td>3.858536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4_0</td>\n",
       "      <td>24085</td>\n",
       "      <td>4</td>\n",
       "      <td>1.507743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>45_0</td>\n",
       "      <td>23581</td>\n",
       "      <td>45</td>\n",
       "      <td>1.056327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>46_0</td>\n",
       "      <td>21677</td>\n",
       "      <td>46</td>\n",
       "      <td>4.460129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>47_0</td>\n",
       "      <td>23201</td>\n",
       "      <td>47</td>\n",
       "      <td>1.048857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>48_0</td>\n",
       "      <td>22892</td>\n",
       "      <td>48</td>\n",
       "      <td>4.578142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>49_0</td>\n",
       "      <td>22909</td>\n",
       "      <td>49</td>\n",
       "      <td>4.897470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5_0</td>\n",
       "      <td>24668</td>\n",
       "      <td>5</td>\n",
       "      <td>0.743540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6_0</td>\n",
       "      <td>22541</td>\n",
       "      <td>6</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>7_0</td>\n",
       "      <td>22674</td>\n",
       "      <td>7</td>\n",
       "      <td>0.973839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>8_0</td>\n",
       "      <td>24435</td>\n",
       "      <td>8</td>\n",
       "      <td>1.095508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>9_0</td>\n",
       "      <td>25599</td>\n",
       "      <td>9</td>\n",
       "      <td>1.044888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arm_name target_function trial_index  diff_multiplier\n",
       "0       0_0           22807           0         3.538337\n",
       "1      10_0           23902          10         0.671956\n",
       "2      11_0           24466          11         0.806354\n",
       "3       1_0           23111           1         2.394134\n",
       "4      12_0           22472          12         2.140057\n",
       "5      13_0           24127          13         1.605374\n",
       "6      14_0           25118          14         1.048804\n",
       "7      15_0           24195          15         1.038859\n",
       "8      16_0           24630          16         1.063608\n",
       "9      17_0           24911          17         1.056256\n",
       "10     18_0           23570          18         0.751249\n",
       "11     19_0           23345          19         1.045874\n",
       "12     20_0           25429          20         1.049615\n",
       "13     21_0           24843          21         1.059887\n",
       "14     22_0           24597          22         1.050487\n",
       "15      2_0           24269           2         2.231188\n",
       "16     23_0           24544          23         1.062143\n",
       "17     24_0           24542          24         0.745060\n",
       "18     25_0           23623          25         4.674932\n",
       "19     26_0           23798          26         1.044415\n",
       "20     27_0           24048          27         0.805470\n",
       "21     28_0           24561          28         1.051403\n",
       "22     29_0           23569          29         3.578628\n",
       "23     30_0           23366          30         1.059434\n",
       "24     31_0           25129          31         1.049501\n",
       "25     32_0           23090          32         1.048989\n",
       "26     33_0           24397          33         1.061924\n",
       "27      3_0           25488           3         1.059199\n",
       "28     34_0           22794          34         1.048692\n",
       "29     35_0           25828          35         1.055283\n",
       "30     36_0           24475          36         1.054521\n",
       "31     37_0           25221          37         1.049686\n",
       "32     38_0           26031          38         1.049391\n",
       "33     39_0           24005          39         1.054346\n",
       "34     40_0           23236          40         4.417897\n",
       "35     41_0           23711          41         3.629608\n",
       "36     42_0           23129          42         3.875543\n",
       "37     43_0           24373          43         1.049353\n",
       "38     44_0           23035          44         3.858536\n",
       "39      4_0           24085           4         1.507743\n",
       "40     45_0           23581          45         1.056327\n",
       "41     46_0           21677          46         4.460129\n",
       "42     47_0           23201          47         1.048857\n",
       "43     48_0           22892          48         4.578142\n",
       "44     49_0           22909          49         4.897470\n",
       "45      5_0           24668           5         0.743540\n",
       "46      6_0           22541           6         5.000000\n",
       "47      7_0           22674           7         0.973839\n",
       "48      8_0           24435           8         1.095508\n",
       "49      9_0           25599           9         1.044888"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax_client.get_trials_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_optimization_trace())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d9a31dca0641c4b8ffd241458b018d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream: 0, user_count: 56, avg_reward@stream: 0.45, avg_reward: 0.45, hr10: 0.333, regret: 9.554, coverage: 0.2, diversity: 1.0, batch_value_loss: 0.0\n",
      "Stream: 1, user_count: 534, avg_reward@stream: 0.43, avg_reward: 0.43, hr10: 0.138, regret: 9.569, coverage: 0.027, diversity: 1.0, batch_value_loss: 0.048\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ddpg_agent = DDPG(actor_lr, critic_lr, state_dim, action_dim, mode, hidden1, hidden2, diff_multiplier, noised_decay,\n",
    "                  top_k, max_memory, depsilon, discount, tau, batch_size, ou_theta, ou_mu, ou_sigma, \n",
    "                  param_noise_scalar, param_noise_scalar_alpha, desired_distance, major_update_interval, noise_type)\n",
    "reward_records = []\n",
    "avg_rewards = []\n",
    "hit_ratios, regret_list, coverage_list, diversity_list = [], [], [], []\n",
    "train(ddpg_agent, reward_records, avg_rewards, hit_ratios, regret_list, coverage_list, diversity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'diff_pca'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Save Result & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_items = {'mode': f'interleaving_decay_ax_pca_major', \n",
    "              'reward_records': reward_records, \n",
    "              'avg_rewards': avg_rewards,\n",
    "              'hit_ratios': hit_ratios,\n",
    "              'regret_list': regret_list,\n",
    "              'coverage_list': coverage_list,\n",
    "              'diversity_list': diversity_list}\n",
    "\n",
    "with open(f'results/interleaving_decay_ax_pca_major.pkl', 'wb') as handle:\n",
    "    pickle.dump(diff_items, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "torch.save(ddpg_agent, f'models/interleaving_decay_ax_pca_major.pth')\n",
    "\n",
    "model = torch.load(f'models/interleaving_decay_ax_pca_major.pth')\n",
    "\n",
    "model.depsilon == ddpg_agent.depsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg1 = DDPG(actor_lr, critic_lr, state_dim, action_dim, mode, hidden1, hidden2, diff_multiplier,\n",
    "                  top_k, max_memory, depsilon, discount, tau, batch_size, ou_theta, ou_mu, ou_sigma, \n",
    "                  param_noise_scalar, param_noise_scalar_alpha, desired_distance)\n",
    "reward_record1 = []\n",
    "avg_reward1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bbc6b14e3f4c49bb35133e5308e3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream: 0, user_count: 56, sum_rewards: 25,avg_reward@stream: 0.45, avg_reward: 0.45, batch_value_loss: 0.0, batch_policy_loss: 0.0 | reward_len: 29\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 3 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "Stream: 1, user_count: 534, sum_rewards: 299,avg_reward@stream: 0.51, avg_reward: 0.51, batch_value_loss: 0.063, batch_policy_loss: -1.491 | reward_len: 237\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "5 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 4 [] 0\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(1, 0)] 1 (1, 0)\n",
      "interleave_add_noised\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "4 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "<class 'numpy.ndarray'> 1 [] 0\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "4 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "2 [(0, 1)] 1 (0, 1)\n",
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 43.1675 s\n",
       "File: <ipython-input-10-8a7078876512>\n",
       "Function: train at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def train(agent, reward_records=[], avg_rewards=[], warmup=100, ep_len=4000):\n",
       "     2         1      41698.0  41698.0      0.0    agent.actor.train(True)\n",
       "     3         1      22399.0  22399.0      0.0    agent.critic.train(True)\n",
       "     4         1        179.0    179.0      0.0    batch_value_loss, batch_policy_loss = 0., 0.\n",
       "     5         1        147.0    147.0      0.0    step_value_loss, step_policy_loss = 0., 0.\n",
       "     6         1        204.0    204.0      0.0    step = 0\n",
       "     7                                             # self.epsilon.clear()\n",
       "     8                                           \n",
       "     9                                             # ------------------- Episode (State) -------------------------------\n",
       "    10         3   27740407.0 9246802.3      0.1    for stream in tqdm(STREAM_LIST[:ep_len]):\n",
       "    11                                               # self.__user_episode_context()\n",
       "    12         3       5496.0   1832.0      0.0      users = STREAM_USER_DICT[stream]\n",
       "    13         3       2199.0    733.0      0.0      agent.curr_stream = stream\n",
       "    14         3       6537.0   2179.0      0.0      agent.curr_stream_items = STREAM_ITEM_DICT[stream]\n",
       "    15         3        841.0    280.3      0.0      stream_reward = 0\n",
       "    16         3    4481933.0 1493977.7      0.0      batch_value_loss, batch_policy_loss = 0., 0.\n",
       "    17                                           \n",
       "    18                                               # ----------------- Runs (User under stream) ---------------------\n",
       "    19       646     488485.0    756.2      0.0      for i, user in enumerate(users):\n",
       "    20                                                 # update step\n",
       "    21       646     232370.0    359.7      0.0        step += 1\n",
       "    22                                                 \n",
       "    23                                                 # state & ongoing\n",
       "    24       646     337389.0    522.3      0.0        agent.curr_user = user\n",
       "    25       646 1134756206.0 1756588.6      2.6        state = USER_CONTEXT.loc[(user, stream)]\n",
       "    26       646     585176.0    905.8      0.0        ongoing = i != len(users) - 1\n",
       "    27                                           \n",
       "    28                                                 # --------------- Actor net choose actions ----------------------\n",
       "    29                                                 # make actor to choose action\n",
       "    30                                                 # action = latent vector\n",
       "    31       546     156346.0    286.3      0.0        if step <= warmup:\n",
       "    32       100  285158208.0 2851582.1      0.7          action = agent.random_action()\n",
       "    33                                                 else:\n",
       "    34       546 4880625747.0 8938875.0     11.3          action = agent.generate_latent_action(state)\n",
       "    35                                                     \n",
       "    36                                                 # --------------- Get next state & info to store ---------------\n",
       "    37                                                 # Generate reward\n",
       "    38       646  144818056.0 224176.6      0.3        reward = agent.get_actions_rewards()\n",
       "    39                                           \n",
       "    40                                                 # next_state: next user's state\n",
       "    41       646     622840.0    964.1      0.0        next_user = users[i + 1] if i + 1 < len(users) else None\n",
       "    42                                                 # Remember only when reward > 0 or the memory is short\n",
       "    43       367    1910332.0   5205.3      0.0        if reward > 0 or len(agent.buffer) < 10:\n",
       "    44       279     149876.0    537.2      0.0          reward_records.append(reward)\n",
       "    45       279      77567.0    278.0      0.0          stream_reward += reward\n",
       "    46       279      95649.0    342.8      0.0          storing_state = (user, stream)\n",
       "    47       279     117784.0    422.2      0.0          storing_next_state = (next_user, stream) if next_user else storing_state\n",
       "    48       279    2839989.0  10179.2      0.0          agent.buffer.remember(storing_state, action, reward, storing_next_state, ongoing)\n",
       "    49                                           \n",
       "    50                                                 # --------------- Train on Actor and Critic --------------------\n",
       "    51                                                 # store pre-training value for td_error\n",
       "    52                                                 # old_Q = self.q_value()\n",
       "    53       546     159328.0    291.8      0.0        if step > warmup :\n",
       "    54       546 36670326134.0 67161769.5     84.9          step_value_loss, step_policy_loss = agent.update_policy()\n",
       "    55                                                 # store post-training value for td_error\n",
       "    56                                                 # new_Q = self.q_value()\n",
       "    57       646    6962559.0  10778.0      0.0        batch_value_loss += step_value_loss\n",
       "    58       646    4390372.0   6796.2      0.0        batch_policy_loss += step_policy_loss\n",
       "    59                                           \n",
       "    60                                                 # --------------- Update with TD error -------------------------\n",
       "    61                                                 # self.epsilon.update_at_step(self.asid, [(new_Q - old_Q), self.learn_step_counter], 1/len(self.stream_items))\n",
       "    62                                           \n",
       "    63                                                 # --------------- Inspecting result at step -------------------------\n",
       "    64         3      18709.0   6236.3      0.0      ep_avg_reward = round(sum(reward_records)/step, 2)\n",
       "    65         3       1379.0    459.7      0.0      avg_rewards.append(ep_avg_reward)\n",
       "    66         3       1003.0    334.3      0.0      print(f'Stream: {stream}, user_count: {len(users)}, \\\n",
       "    67                                           sum_rewards: {round(sum(reward_records), 2)},avg_reward@stream: {round(stream_reward/len(users), 2)}, avg_reward: {ep_avg_reward}, \\\n",
       "    68         3     320780.0 106926.7      0.0  batch_value_loss: {round(float(batch_value_loss)/len(users), 3)}, batch_policy_loss: {round(float(batch_policy_loss)/len(users), 3)} | reward_len: {len(reward_records)}')\n",
       "    69                                               \n",
       "    70         3       1612.0    537.3      0.0      if len(reward_records) >= 247581: break"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [(0, 1)] 1 (0, 1)\n",
      "3 [(0, 1)] 1 (0, 1)\n",
      "Stream: 2, user_count: 56, sum_rewards: 370,avg_reward@stream: 1.27, avg_reward: 0.57, batch_value_loss: 0.091, batch_policy_loss: -2.08 | reward_len: 279\n"
     ]
    }
   ],
   "source": [
    "%lprun -f train train(ddpg1, reward_record1, avg_reward1, ep_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg1.clicks.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0585661 s\n",
       "File: ../../interleaving/interleaving/probabilistic.py\n",
       "Function: compute_scores at line 138\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   138                                               @classmethod\n",
       "   139                                               def compute_scores(cls, ranking, clicks, tau=3.0, n=10**4):\n",
       "   140                                                   '''\n",
       "   141                                                   ranking: an instance of Ranking\n",
       "   142                                                   clicks: a list of indices clicked by a user\n",
       "   143                                           \n",
       "   144                                                   Return a list of scores of each ranker.\n",
       "   145                                                   '''\n",
       "   146         1        999.0    999.0      0.0          L = ranking\n",
       "   147         1       8790.0   8790.0      0.0          C = {ranking[index] for index in clicks}\n",
       "   148         1        755.0    755.0      0.0          if len(ranking.lists) == 2:\n",
       "   149                                                       # [Hofmann+, CIKM 2011] (Computationally expensive)\n",
       "   150         1       3118.0   3118.0      0.0              o = cls.ProbablisticScore({0: 0.0, 1: 0.0})\n",
       "   151         1        422.0    422.0      0.0              o.allocations = {}\n",
       "   152       500     126685.0    253.4      0.2              for i in range(50 * len(ranking)): # 原為 2 ^ 10\n",
       "   153       500     131053.0    262.1      0.2                  a = []\n",
       "   154      5000     854451.0    170.9      1.5                  for d in L:\n",
       "   155      5000    1309977.0    262.0      2.2                      a.append(i % 2)\n",
       "   156      5000     902684.0    180.5      1.5                      i //= 2\n",
       "   157       500     124087.0    248.2      0.2                  c = [0, 0]\n",
       "   158       500   26690036.0  53380.1     45.6                  R = [cls.Softmax(tau, R_j) for R_j in ranking.lists]\n",
       "   159       500     104315.0    208.6      0.2                  cum_p = 1.0\n",
       "   160      5000    1303713.0    260.7      2.2                  for j, d in zip(a, L):\n",
       "   161      5000    1143603.0    228.7      2.0                      j_alter = (j + 1) % 2\n",
       "   162      3500     783803.0    223.9      1.3                      if d in C:\n",
       "   163      1500     424570.0    283.0      0.7                          c[j] += 1\n",
       "   164      5000   12568464.0   2513.7     21.5                      cum_p *= R[j].delete(d)\n",
       "   165      5000   11080040.0   2216.0     18.9                      R[j_alter].delete(d)\n",
       "   166       254      86583.0    340.9      0.1                  if c[0] < c[1]:\n",
       "   167       246     212458.0    863.7      0.4                      o[1] += cum_p\n",
       "   168       254      76199.0    300.0      0.1                  elif c[1] < c[0]:\n",
       "   169       254     218208.0    859.1      0.4                      o[0] += cum_p\n",
       "   170       500     410994.0    822.0      0.7                  o.allocations[tuple(a)] = (c, cum_p)\n",
       "   171         1        142.0    142.0      0.0              return o\n",
       "   172                                                   if 2 < len(ranking.lists):\n",
       "   173                                                       # [Schuth+, SIGIR 2015]\n",
       "   174                                                       R = [cls.Softmax(tau, R_j) for R_j in ranking.lists]\n",
       "   175                                                       A_prime = [(np.zeros(len(R)), 0.0, [])]\n",
       "   176                                                       threshold = 1 / len(R) * n ** (1 / len(L))\n",
       "   177                                                       for d in L:\n",
       "   178                                                           # Break if no click\n",
       "   179                                                           # Stop when all the clicks are examined.\n",
       "   180                                                           if len(C) == 0:\n",
       "   181                                                               break\n",
       "   182                                                           d_in_C = d in C\n",
       "   183                                                           if d_in_C:\n",
       "   184                                                               C.remove(d)\n",
       "   185                                           \n",
       "   186                                                           # Compute the document probability\n",
       "   187                                                           # Only keep non-zero rankers\n",
       "   188                                                           P = np.zeros(len(R))\n",
       "   189                                                           R_non_zero = []\n",
       "   190                                                           for j, R_j in enumerate(R):\n",
       "   191                                                               P[j] = R_j.delete(d)\n",
       "   192                                                               if P[j] > 0.0:\n",
       "   193                                                                   R_non_zero.append((j, R_j))\n",
       "   194                                                           if len(R_non_zero) == 0:\n",
       "   195                                                               break\n",
       "   196                                           \n",
       "   197                                                           A, A_prime = A_prime, []\n",
       "   198                                                           is_pass = np.random.rand(len(A), len(R_non_zero)) <= threshold\n",
       "   199                                                           for i, (o, p, a) in enumerate(A):\n",
       "   200                                                               # Skip some assignments with certain probability\n",
       "   201                                                               R_used = [R_non_zero[k]\n",
       "   202                                                                   for k in range(len(R_non_zero)) if is_pass[i][k]]\n",
       "   203                                                               for j, R_j in R_used:\n",
       "   204                                                                   p_prime = p + np.log(P[j])\n",
       "   205                                                                   o_prime = np.copy(o)\n",
       "   206                                                                   if d_in_C:\n",
       "   207                                                                       o_prime[j] += 1\n",
       "   208                                                                   A_prime.append((o_prime, p_prime, a + [j]))\n",
       "   209                                           \n",
       "   210                                                       o = np.zeros(len(R))\n",
       "   211                                                       allocations = {}\n",
       "   212                                                       if len(A_prime) > 0:\n",
       "   213                                                           # Use logsumexp to avoid over-flow\n",
       "   214                                                           p_log_all = np.array([p_prime for _, p_prime, _ in A_prime])\n",
       "   215                                                           p_all = np.exp(p_log_all - special.logsumexp(p_log_all))\n",
       "   216                                                           for i, (o_prime, _, a) in enumerate(A_prime):\n",
       "   217                                                               p_prime = p_all[i]\n",
       "   218                                                               o += o_prime * p_prime\n",
       "   219                                                               allocations[tuple(a)] = (list(o_prime), p_prime)\n",
       "   220                                                       result = cls.ProbablisticScore({i: o[i] for i in range(len(R))})\n",
       "   221                                                       result.allocations = allocations\n",
       "   222                                                       return result\n",
       "   223                                                   else:\n",
       "   224                                                       raise ValueError('Invalid number of original lists')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f ddpg1.interleaver.compute_scores ddpg1.interleaver.compute_scores(ddpg1.action_list, ddpg1.clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0471625 s\n",
       "File: <ipython-input-120-a0bda88d93f5>\n",
       "Function: get_batch at line 53\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    53                                             def get_batch(self, batch_size=100):\n",
       "    54         1       3766.0   3766.0      0.0      assert len(self.states_buffer) == len(self.next_states_buffer) == len(self.actions_buffer) == \\\n",
       "    55         1        876.0    876.0      0.0             len(self.rewards_buffer) == len(self.ongoings_buffer)\n",
       "    56                                               # same for sample_and_split\n",
       "    57         1        198.0    198.0      0.0      states0_batch = []\n",
       "    58         1        145.0    145.0      0.0      states1_batch = []\n",
       "    59         1        131.0    131.0      0.0      actions_batch = []\n",
       "    60         1        131.0    131.0      0.0      rewards_batch = []\n",
       "    61         1        137.0    137.0      0.0      ongoings_batch = []\n",
       "    62         1      56276.0  56276.0      0.1      batch_indexs = np.random.randint(0, len(self.states_buffer), size=min(len(self.states_buffer), batch_size))\n",
       "    63       100      49796.0    498.0      0.1      for i in batch_indexs:\n",
       "    64       100   17109838.0 171098.4     36.3        states0_batch.append(USER_CONTEXT.xs(self.states_buffer[i]))\n",
       "    65       100   17117806.0 171178.1     36.3        states1_batch.append(USER_CONTEXT.xs(self.next_states_buffer[i]))\n",
       "    66       100     368350.0   3683.5      0.8        actions_batch.append(self.actions_buffer[i])\n",
       "    67       100     150959.0   1509.6      0.3        rewards_batch.append(self.rewards_buffer[i])\n",
       "    68       100     142503.0   1425.0      0.3        ongoings_batch.append(self.ongoings_buffer[i])\n",
       "    69         1        707.0    707.0      0.0      assert len(states0_batch) == len(states1_batch) == len(actions_batch) == \\\n",
       "    70         1        324.0    324.0      0.0             len(rewards_batch) == len(ongoings_batch)\n",
       "    71         1   12160521.0 12160521.0     25.8      return np.array(states0_batch), np.array(actions_batch), np.array(rewards_batch), np.array(states1_batch), np.array(ongoings_batch)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f ddpg1.buffer.get_batch ddpg1.buffer.get_batch(ddpg1.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 3, 5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5] # Ranking 1\n",
    "b = [4, 3, 5, 1, 2] # Ranking 2\n",
    "method = interleaving.Probabilistic([a, b])\n",
    "ranking = method.interleave()\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute\n",
      "{0: 1.505927205808695, 1: 0.49407279419130523}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks = np.uint32([1, 3]).astype(int).tolist()\n",
    "result = method.evaluate(ranking, clicks)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clicks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute\n",
      "{0: 1.505927205808695, 1: 0.49407279419130523}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks = [1, 3]\n",
    "result = interleaving.Probabilistic.evaluate(ranking, clicks)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set([1, 2, 3])\n",
    "b = set([1, 2, 4, 5, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a | b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
