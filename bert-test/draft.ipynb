{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4bc1ac-ccdb-435a-acc9-a6e4de0d322b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac40e4e-9722-42cb-9299-efd9ad40c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "# import from preprocessing\n",
    "import pandas as pd\n",
    "# from preprocessing import USER_STREAM_CONTEXT, ITEM_DF, ITEM_STREAM_DF, REAL_BOUGHT_DF, LAST_BOUGHT_STREAM, LB_ITEMS, USER_LIST, LB_CE, STREAM_LIST\n",
    "\n",
    "### Environment\n",
    "# Get full state\n",
    "\n",
    "# * 參考原論文: user _interest 初始值為 random vector\n",
    "# 1. 很多筆記錄裡用戶只在一場直播裡購買，如果 random 的話 accuracy 會下降\n",
    "# 2. 不可能用該期直播的相關紀錄作為 state → 在 user/state context 裡加上 RFML/streamer information\n",
    "# 3. user x all_stream → 在第一場直播都給予 random vector，對 accuracy 的影響就不會太高（？\n",
    "\n",
    "\n",
    "'''\n",
    "Generate series: whether elements in A existed in list B\n",
    "A, B: List\n",
    "return: pd.Series\n",
    "example:\n",
    "  A: [1, 2, 4, 5]\n",
    "  B: [1, 2, 3, 4, 5, 6, 7]\n",
    "  return: Series([1, 1, 0, 1, 1, 0, 0], index=[1, 2, 3, 4, 5, 6, 7])\n",
    "'''\n",
    "def gen_exist_series(A, B):\n",
    "  exist_list = [int(item in A) for item in B]\n",
    "  return pd.Series(exist_list, index=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2f160-d52b-4b7b-b0dc-e49512ac85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Comparison function for reward\n",
    "'''\n",
    "def r(a, b):\n",
    "  if a==1 and b==0: return 0 # -1 when the rule is to punish unrec-bought\n",
    "  else: return a & b\n",
    "\n",
    "def get_reward(user_phone, stream, action_ids):\n",
    "  items = action_ids.index\n",
    "  real_bought_ids = REAL_BOUGHT_DF.loc[(REAL_BOUGHT_DF['聯絡電話'] == user_phone) \n",
    "                                         & (REAL_BOUGHT_DF['場次'] == stream)]['item_id'].values\n",
    "  real_bought_ids_series = gen_exist_series(real_bought_ids, items)\n",
    "  \n",
    "  reward_list = [r(a, b) for a, b in zip(real_bought_ids_series.values, action_ids.values)]\n",
    "  return pd.Series(reward_list, index=items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcf0b8-b7ba-4b52-905f-0a46108d6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def calculate_interest_change(user_all_streams, stream_list, i):\n",
    "  if i < 0:\n",
    "    return 0\n",
    "  else:\n",
    "    former_stream = stream_list[i-1]\n",
    "    current_stream = stream_list[i]\n",
    "\n",
    "    test_ce = user_all_streams.loc[:, LB_CE]\n",
    "    ce = log_loss(test_ce.loc[former_stream], test_ce.loc[current_stream], labels=['prev', 'current'])\n",
    "    if ce < 0.01: ce = 0\n",
    "    else: ce = round(ce, 3)\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537081f-9ad0-4103-8c38-8029b70a9cf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-f435818a5078>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f435818a5078>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "INPUT_DF_COL__USR = USER_STREAM_CONTEXT.columns.to_list()\n",
    "INPUT_DF_COL = INPUT_DF_COL__USR + LB_ITEMS\n",
    "\n",
    "'''\n",
    "Convert state format to model input format\n",
    "'''\n",
    "def get_input(input_state):\n",
    "  # Slice items\n",
    "  items = input_state['cand_item']\n",
    "  input_state = input_state.drop('cand_item')\n",
    "  item_feat = ITEM_DF.loc[items]\n",
    "  \n",
    "  # Create new dataframe\n",
    "  stream_item_feat = pd.DataFrame(columns=INPUT_DF_COL, index=item_feat.index)\n",
    "  \n",
    "  # Fill in other context\n",
    "  # stream_item_feat.loc[:, INPUT_DF_COL__USR].assign(**input_state)\n",
    "  stream_item_feat = stream_item_feat.loc[:, INPUT_DF_COL__USR].assign(**input_state)\n",
    "  \n",
    "  # Fill in items\n",
    "  stream_item_feat[LB_ITEMS] = item_feat\n",
    "  return stream_item_feat.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1198c-ed82-4865-99a0-fd8f9c98769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_top10(model, input_state):\n",
    "  # Get all items\n",
    "  full_input = get_input(input_state).astype('float32')\n",
    "  \n",
    "  # 紀錄所有預測結果\n",
    "  predicts = model.predict(full_input)\n",
    "  full_input['predict'] = predicts\n",
    "  actions = full_input['predict'].nlargest(10).index #['i_item_id'].to_list()\n",
    "  actions = full_input.loc[actions, 'i_item_id'].values\n",
    "  return actions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b8de2-9a7c-4620-9565-fe45ea1a5391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b56889-4fc2-4f71-b1f3-319a8335a0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c4ee3-53ed-40d8-8de8-d2a7d01713c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190cb55-e81b-497e-b68a-bbfdcb56633a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fea4c7-36e2-4138-a8e3-eb3da01f34fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd581c2-ee92-4888-ab77-d8b017fb5c28",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3af22-bd13-4a1b-8157-ed62539232b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collecting Training Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, max_memory=100000, discount=.9):\n",
    "    \"\"\"\n",
    "    Setup\n",
    "    max_memory: the maximum number of experiences we want to store\n",
    "    memory: a list of experiences\n",
    "    discount: the discount factor for future experience\n",
    "    In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "    [...\n",
    "    [experience, game_over]\n",
    "    [experience, game_over]\n",
    "    ...]\n",
    "    \"\"\"\n",
    "    self.max_memory = max_memory\n",
    "    self.memory = list()\n",
    "    self.discount = discount\n",
    "\n",
    "  def remember(self, interest_score, states, game_over):\n",
    "    # Save a state to memory\n",
    "    self.memory.append([interest_score, states, game_over])\n",
    "    # We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "    if len(self.memory) > self.max_memory:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def get_batch(self, model, batch_size=10):\n",
    "    # How many experiences do we have?\n",
    "    len_memory = len(self.memory)\n",
    "\n",
    "    # Calculate the number of actions that can possibly be taken in the game.\n",
    "    # Actions: 0 = not recommend, 1 = recommend\n",
    "    num_actions = model.output_shape[-1]\n",
    "\n",
    "    # Dimensions of our observed states, ie, the input to our model.\n",
    "    # Memory:  [\n",
    "    #   [interest_score, [ [...state], action, reward, next_state_idx], game_over],\n",
    "    #   [interest_score, [ [...state], action, reward, nexr_state_idx], game_over],\n",
    "    #   ...\n",
    "    # ]\n",
    "    env_dim = len(INPUT_DF_COL)\n",
    "\n",
    "    inputs = pd.DataFrame(columns=INPUT_DF_COL)\n",
    "    targets = pd.DataFrame(columns=[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # We draw states to learn from randomly\n",
    "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):      \n",
    "\n",
    "      # Here we load one transition <s, a, r, s'> from memory\n",
    "      state_t, action_t, reward_t, state_tp1 = self.memory[idx][1]\n",
    "      # state_t = state_t.astype('float32')\n",
    "      game_over = self.memory[idx][2]\n",
    "\n",
    "      '''\n",
    "      修改倒入 state 的方式 input = (state - item) + item_feat\n",
    "      拆掉 model_predict 成 function\n",
    "      \n",
    "      here should be state_t * all_items\n",
    "      '''\n",
    "      state_t = get_input(state_t).astype('float32')\n",
    "      # puts state into input\n",
    "      inputs = pd.concat([inputs, state_t], axis=0)\n",
    "      # TODO: Modify input shape 0\n",
    "\n",
    "      # First we fill the target values with the predictions of the model.\n",
    "      # They will not be affected by training (since the training loss for them is 0)\n",
    "      # TODO\n",
    "      '''\n",
    "      每個 actions 都會被 predict 一個成績/reward\n",
    "      '''\n",
    "\n",
    "      # if the game ended, the reward is the final reward\n",
    "      if game_over:  # if game_over is True\n",
    "        state_t['reward'] = reward_t\n",
    "        targets = pd.concat([targets, reward_t], axis=0).astype('float32')\n",
    "      else:\n",
    "        state_t['reward'] = model.predict(state_t).flatten()\n",
    "        # 找到 action_t 們，指到 state_t 上去算 discount values\n",
    "        action_t = action_t[action_t == 1].index.to_list()\n",
    "        \n",
    "        state_tp1 = get_input(state_tp1)\n",
    "        Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "        # r + gamma * max Q(s',a')\n",
    "        # DataFrame apply\n",
    "        state_t.loc[action_t, 'reward'] = state_t.loc[action_t, 'reward'].apply(lambda x: x + self.discount * Q_sa).astype('float32')\n",
    "        targets = pd.concat([targets, state_t['reward']], axis=0).astype('float32')\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b3a5e-b7fc-49f0-867e-846a605762d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e33d6f-e895-40be-bed6-136c7fd68a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24239ddd-a839-4154-9bc1-86797d768a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a38aee2d-d9ba-4e13-863a-fe8ad4a0913c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Train & Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5709b2ed-10b1-4e5c-9e5d-9d223cf9462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocessing import USER_STREAM_CONTEXT, ITEM_DF, ITEM_STREAM_DF, REAL_BOUGHT_DF, LAST_BOUGHT_STREAM, LB_ITEMS, USER_LIST, LB_CE, STREAM_LIST\n",
    "# from replay import ReplayBuffer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6f911b-1f0b-4c9a-ab83-338449cf7bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:      /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:\\\n",
    "      {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b2f337-3ba6-4bca-8de1-514ccefeb20e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train DQN model\n",
    "* Input: `user_df` 253, `item_df` 231(BERT: 768), interact (?), `reward` 1\n",
    "* Output: recommend a list of items\n",
    "* Methods Needed\n",
    "    * Environment Function\n",
    "    * Choose Action\n",
    "    * Store Transition\n",
    "    * Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adba99bf-3034-444c-be67-a43efd5a273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Process\n",
    "\n",
    "def train(model, exp_replay, epochs, batch_size, num_episode=1000, verbose=1, reward_set='strict', hist=[], c_hist=[], rec_list=[]):\n",
    "  # total_actions = ITEM_DF.shape[0]\n",
    "  total_episodes = len(USER_LIST)\n",
    "  # Reset win counter\n",
    "  c_win_cnt = 0\n",
    "\n",
    "  for e in range(epochs):\n",
    "    rec_cnt = 0\n",
    "    win_cnt = 0\n",
    "    loss = 0.\n",
    "    # TODO/MAIN: Apply user preference changes as epsilon\n",
    "    # epsilon for exploration - dependent inversely on the training epoch\n",
    "    epsilon = 4 / ((e + 1) ** (1 / 2))\n",
    "\n",
    "    # handling episodes by assigning users from USER_LIST\n",
    "    # Each user represent an Episode\n",
    "    episodes = random.sample(range(total_episodes), num_episode)\n",
    "\n",
    "    print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "    # ------------------- Episode (User) -------------------------------\n",
    "    for user_episode in episodes:\n",
    "      # game_over = False\n",
    "      # get episode data by user phone number\n",
    "      user_phone = USER_LIST[user_episode]\n",
    "      user_all_streams = USER_STREAM_CONTEXT.xs(user_phone, level=\"聯絡電話\")\n",
    "      stream_list = user_all_streams.index\n",
    "      final_stream = LAST_BOUGHT_STREAM.loc[user_phone, '場次']\n",
    "      \n",
    "      # ----------------- Runs (User x All_Stream) ---------------------\n",
    "      for i, stream in enumerate(stream_list):          \n",
    "        game_over = stream == final_stream\n",
    "                \n",
    "        # Get full state: current_state = user_stream + item_stream\n",
    "        # 用上一場紀錄預測下一場直播會購買的商品\n",
    "        current_state = get_full_state(user_all_streams, stream_list, i)\n",
    "        stream_items = current_state['cand_item']\n",
    "        \n",
    "        # --------------- Explore/Exploit Section ----------------------\n",
    "        if np.random.rand() <= epsilon:\n",
    "          # Explore by randomly select 10/n items from candidate_items\n",
    "          # Get all items from the stream\n",
    "          sample_actions = random.sample(stream_items, 10) if len(stream_items) > 10 else stream_items\n",
    "          action_ids = gen_exist_series(sample_actions, stream_items)\n",
    "        else:\n",
    "          # Exploit by choosing action from the model's prediction\n",
    "          pred_actions = model_predict_top10(model, current_state)\n",
    "          action_ids = gen_exist_series(pred_actions, stream_items)\n",
    "\n",
    "        # --------------- Get next state & info to store ---------------\n",
    "        reward = get_reward(user_phone, stream, action_ids)\n",
    "        next_state = get_full_state(user_all_streams, stream_list, i+1) if not game_over else []\n",
    "\n",
    "        rec_cnt += 1\n",
    "        if sum(reward) > 0:\n",
    "          c_win_cnt += 1\n",
    "          win_cnt += 1\n",
    "\n",
    "        # --------------- Calculating Interest Changes -----------------\n",
    "        interest_score = calculate_interest_change(user_all_streams, stream_list, i)\n",
    "\n",
    "        # --------------- Store Experience -----------------------------\n",
    "        exp_replay.remember(interest_score,\n",
    "                            [current_state, action_ids, reward, next_state],\n",
    "                            game_over)\n",
    "        \n",
    "\n",
    "        # --------------- Load batch of experiences --------------------\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "        # train model on experiences\n",
    "        batch_loss = model.train_on_batch(inputs, targets)\n",
    "        loss += batch_loss\n",
    "            \n",
    "    if verbose > 0:\n",
    "      print(f'Epoch: {e}/{epochs} | Loss {loss} | Win count {win_cnt} | Rec count {rec_cnt} | Time {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "    \n",
    "    # Track win history to later check if our model is improving at the game over time.\n",
    "    hist.append(win_cnt)\n",
    "    c_hist.append(c_win_cnt)\n",
    "    rec_list.append(rec_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d28e144-e732-4036-9765-8386ca3fbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 2  # Number of experiences we use for training per batch\n",
    "EPOCH = 50\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd4b39-e518-4b08-8356-bf61ce94210b",
   "metadata": {},
   "source": [
    "### Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0f1af-977d-40b8-aad5-8e5771c55fca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 13:28:43\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)# Our model's architecture parameters\n",
    "input_size = 473 # The input shape for model - this comes from the output shape of the CNN Mobilenet\n",
    "\n",
    "# Setting up the model with keras.\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(HIDDEN_SIZE, input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dense(HIDDEN_SIZE, activation='tanh'))\n",
    "model.add(Dense(TOTAL_ACTIONS))\n",
    "model.compile(Adam(learning_rate=.000001), \"mse\")\n",
    "\n",
    "hist = []\n",
    "c_hist = []\n",
    "rec_list = []\n",
    "\n",
    "# Training the model\n",
    "train(model, \n",
    "      exp_replay, \n",
    "      epochs=EPOCH, \n",
    "      batch_size=BATCH_SIZE, \n",
    "      num_episode=NUM_EPISODE, \n",
    "      verbose=1, \n",
    "      reward_set='strict',\n",
    "      hist=hist,\n",
    "      c_hist=c_hist,\n",
    "      rec_list=rec_list)\n",
    "plt.plot(range(EPOCH), hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b60988-b8fb-4035-b19b-ee73fb6c2f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3395a-8494-49dc-bb82-6017a9f7befe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2118731-4890-4dbf-a335-907b022ab6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_hist[-1]/sum(rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648ec5c-71db-4971-a291-7ecd0ea730c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = np.load('../../data/jambo_iname2embedding.npy', allow_pickle=True)[None][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413c06d-533d-4ca0-b65c-edccb33376f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bert.keys())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4e81c1cd-61c5-4906-a012-c29afd9cf93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert['【商城】W08-76 休閒毯'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c8631-f5e7-4940-9594-6fb898575204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc7697-af94-4bdc-8785-f67d1d026dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37da343-9479-4e80-9b0b-c7a900865fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('9.txt', 'w') as f:\n",
    "  f.write(' hist: \\n')\n",
    "  f.write(', '.join([str(a) for a in hist]))\n",
    "  f.write('\\n c_hist: \\n')\n",
    "  f.write(', '.join([str(a) for a in c_hist]))\n",
    "  f.write('\\n rec_list: \\n')\n",
    "  f.write(', '.join([str(a) for a in rec_list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d005d-a0cf-4e2a-bed3-51a6f9da0f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d8ae2-5178-4fe6-8aaa-9364ee19e62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8c5f0-10ea-446b-9b78-997eb4062b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5d26d15f81c32561881a39c85bdcf5a2542c694b6b4a6d5e9280707d5ee0206"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
