{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Baseline with All Streamers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "ORIGIN_DATA = pd.read_pickle('../../data/new_txn.pkl')\n",
    "BERT = np.load('../../data/jambo_iname2embedding.npy', allow_pickle=True)[None][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試少量資料\n",
    "DATA = ORIGIN_DATA.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing & Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.9 s, sys: 1.05 s, total: 8.95 s\n",
      "Wall time: 8.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Make Stream Indexer\n",
    "'''\n",
    "DATA['stream'] = DATA['user_id'].astype(str) + DATA['場次'].astype(str)\n",
    "\n",
    "# Set stream_id in time order\n",
    "sorted_streams = DATA.sort_values(by=['下單日期'])['stream'].reset_index(drop=True).unique()\n",
    "stream_dict = { x: i for i, x in enumerate(sorted_streams) }\n",
    "\n",
    "DATA['stream'] = DATA['stream'].map(stream_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 s, sys: 814 ms, total: 16.4 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "THRESHOLDS\n",
    "1. Users bought threshold: could be 9/15/25/1(consider all)\n",
    "2. Stream buyer threshold: could be 2/25/73/209\n",
    "   -> 避免 Feature 即為正確答案\n",
    "'''\n",
    "buy_threshold = 9\n",
    "stream_threshold = 1\n",
    "\n",
    "user_count = DATA.groupby(['asid']).count()\n",
    "uid = user_count['下單日期'].loc[user_count['下單日期'] > buy_threshold].index\n",
    "DATA = DATA.loc[DATA['asid'].isin(uid)]\n",
    "\n",
    "data_groupby_stream = DATA.groupby(['stream'])\n",
    "g_s_count = data_groupby_stream.count()\n",
    "available_streams = g_s_count.loc[g_s_count['訂單編號'] > stream_threshold].index\n",
    "DATA = DATA.loc[DATA['stream'].isin(available_streams)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.4 s, sys: 140 ms, total: 32.6 s\n",
      "Wall time: 32.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "period & item_id\n",
    "'''\n",
    "DATA['period'] = DATA['下單日期'].astype(str).apply(lambda x: x[:7])\n",
    "DATA.rename(columns = {'商品id': 'item_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.08 s, sys: 1.11 s, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "One Hot Encoding\n",
    "* 處理 shipment, payment\n",
    "* 要先做 label encoding 才能做 one hot encoding\n",
    "'''\n",
    "le = LabelEncoder()\n",
    "\n",
    "DATA['shipment'] = le.fit_transform(DATA['運送方式'])\n",
    "DATA['payment'] = le.fit_transform(DATA['付款方式'])\n",
    "DATA['streamer_id'] = le.fit_transform(DATA['user_id'])\n",
    "DATA = pd.get_dummies(DATA,\n",
    "                      prefix=['shipment', 'payment', 'streamer'],\n",
    "                      columns=['shipment', 'payment', 'streamer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 1.97 s, total: 14.3 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "USER Features \n",
    "Generate USER_STREAM_CONTEXT\n",
    "'''\n",
    "data_groupby_user_stream = DATA.groupby(['asid', 'stream'])\n",
    "\n",
    "\n",
    "# Get group by sum\n",
    "USER_STREAM_CONTEXT = data_groupby_user_stream.sum()\n",
    "USER_STREAM_CONTEXT = USER_STREAM_CONTEXT[['shipment_0', 'shipment_1', 'shipment_2', 'shipment_3', \n",
    "                                           'shipment_4', 'shipment_5', 'shipment_6', 'payment_0', \n",
    "                                           'payment_1', 'payment_2', 'payment_3', 'payment_4', 'payment_5',\n",
    "                                           'payment_6', 'payment_7', 'payment_8', 'streamer_0', 'streamer_1',\n",
    "                                           'streamer_2', 'streamer_3', 'streamer_4', 'streamer_5', \n",
    "                                           'streamer_6', 'streamer_7', 'streamer_8']]\n",
    "\n",
    "# Get group by count & mean\n",
    "u_s_count = data_groupby_user_stream.count()\n",
    "u_s_avg = data_groupby_user_stream.mean()\n",
    "\n",
    "# Assign seleted values to USER_STREAM_CONTEXT\n",
    "USER_STREAM_CONTEXT[['channel_name', 'user_id', 'item_id', '訂單編號', '專屬折扣']] = u_s_count[['channel_name', 'user_id', 'item_id', '訂單編號', '專屬折扣']]\n",
    "USER_STREAM_CONTEXT[['運費', '總金額', '數量', '單價']] = u_s_avg[['運費', '總金額', '數量', '單價']]\n",
    "\n",
    "# Rename columns\n",
    "USER_STREAM_CONTEXT.rename(columns={'user_id': 'streamer_id',\n",
    "                                    'channel_name': 'channel_cnt',\n",
    "                                    '訂單編號': 'order_cnt',\n",
    "                                    'item_id': 'item_var_cnt',\n",
    "                                    '運費': 'shipment_fee',\n",
    "                                    '專屬折扣': 'discount',\n",
    "                                    '總金額': 'total_price',\n",
    "                                    '單價': 'avg_item_price',\n",
    "                                    '數量': 'avg_item_cnt'\n",
    "                                   },\n",
    "                           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2de142c9cd040c886652429932f667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/162189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 s, sys: 207 ms, total: 48.6 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "USER_STREAM_MDB: user bought history average embedding (by stream) through items.unique.index\n",
    "* BERT_INDEX: [item_name] -> item_idx\n",
    "* item_name_embedding: [item_idx] -> item_embedding\n",
    "* user_stream_buy_list: (buyer, stream) -> [items]\n",
    "'''\n",
    "# Append item embeddings according to items list's order & convert to np array\n",
    "item_names = DATA['item_name'].unique().tolist()\n",
    "ITEM_EMB_BY_IDX = []\n",
    "for iname in tqdm(item_names):\n",
    "  ITEM_EMB_BY_IDX.append(BERT[iname])\n",
    "ITEM_EMB_BY_IDX = np.array(ITEM_EMB_BY_IDX)\n",
    "\n",
    "# Map item name to item index\n",
    "BERT_INDEX = {name:i for i, name in enumerate(item_names)}\n",
    "\n",
    "# Item names grouped by asid & stream\n",
    "user_stream_buy_list = DATA.groupby(['asid', 'stream'])['item_name'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f7cc3696b34cfea3eff749a09e8a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Calculate user cumulated stream embedding\n",
    "* user_stream_order_dict: { User: [stream list in order] }\n",
    "* buy list: [asid, streams]\n",
    "* 需求：可以選擇要「給予權重」或「直接平均」\n",
    "'''\n",
    "user_stream_order = DATA[['asid', 'stream']].groupby(['asid'])['stream'].apply(set)\n",
    "USER_LIST = DATA['asid'].unique().tolist()\n",
    "# test asid: ['1000057940522534']\n",
    "# test streams: {3397, 3415, 3815, 3865, 4365, 4478, 4563, 5028}\n",
    "\n",
    "# AVG\n",
    "FINAL_EMB = {}\n",
    "for asid in tqdm(USER_LIST):\n",
    "  # find all the stream user attended and sort them in time order\n",
    "  streamlist = list(user_stream_order[asid])\n",
    "  streamlist.sort()\n",
    "  # find items bought in each stream\n",
    "  buylist = [user_stream_buy_list[asid, stream] for stream in streamlist]\n",
    "  # cumulated items bought in each stream\n",
    "  cumulative_buylist = [sum(buylist[:i], []) for i in range(1, len(buylist) + 1)]\n",
    "  # map { stream: cumulate_items } to a dictionary\n",
    "  cumulative_dict = dict(zip(streamlist, cumulative_buylist))\n",
    "  # calculate average embedding and put them into FINAL_EMB\n",
    "  # FOR EACH STREAM\n",
    "  for k in cumulative_dict.keys():\n",
    "    # get all items' name from stream\n",
    "    i_list = cumulative_dict[k]\n",
    "    # get items' BERT_INDEX for retrived items\n",
    "    all_buy_item_index = [BERT_INDEX[item] for item in i_list]\n",
    "    # get their embeddings and calculate mean\n",
    "    user_emb = ITEM_EMB_BY_IDX[all_buy_item_index].mean(axis=0)\n",
    "    # put them into the FINAL_EMB as previous set format\n",
    "    FINAL_EMB[(asid, k)] = user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9d4aeb8e5c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFINAL_EMB\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# store \n",
    "with open('cumulative_item_emb.pkl', 'wb') as handle:\n",
    "    pickle.dump(FINAL_EMB, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load\n",
    "with open('cumulative_item_emb.pkl', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.9 s, sys: 1.43 s, total: 53.3 s\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "ITEM Features\n",
    "1. Attach item embeddings to DATA\n",
    "2. Reset ITEM_DF's index\n",
    "'''\n",
    "LB_BERT = [f'bert_{x}' for x in range(768)] # constant\n",
    "\n",
    "# Generate ITEM_DF for DATA\n",
    "ITEM_FEAT = pd.DataFrame.from_dict(BERT, orient='index').astype('float32') # decrease processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ITEM_STREAM_DICT: 直播 id 對應到的商品列表\n",
    "REAL_BOUGHT_DF: 計算 reward 用\n",
    "LAST_BOUGHT_STREAM: 作為 game_over\n",
    "'''\n",
    "# Constant: all stream list\n",
    "STREAM_LIST = DATA.stream.unique()\n",
    "\n",
    "# [Reward]\n",
    "REAL_BOUGHT_DF = DATA.loc[:, ['asid', 'stream', 'item_id']]\n",
    "\n",
    "# [Action]\n",
    "LAST_BOUGHT_STREAM = USER_STREAM_CONTEXT.reset_index().groupby('asid', as_index=False).last().loc[:, ['asid', 'stream']].set_index('asid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c69c55c17648038e920009d0dc0538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 2.18 s, total: 14.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "STREAM Features:\n",
    "* STREAM_FEAT: avg_price, avg_sum, count order #\n",
    "* STREAM_ITEM_EMB: [stream] -> avg item embedding\n",
    "'''\n",
    "data_gb_stream = DATA.groupby(['stream'])\n",
    "STREAM_FEAT = data_gb_stream.mean()[['單價', '總金額']]\n",
    "STREAM_FEAT[['count']] = data_gb_stream.count()[['item_name']]\n",
    "STREAM_FEAT.rename(columns={'單價':'avg_price', '總金額':'avg_sum'}, inplace=True)\n",
    "\n",
    "# Item names grouped by stream\n",
    "stream_item_list = data_gb_stream['item_name'].apply(list)\n",
    "\n",
    "STREAM_ITEM_EMB = {}\n",
    "for stream in tqdm(stream_item_list.index):\n",
    "  # Find each stream's items\n",
    "  item_list = stream_item_list[stream]\n",
    "  # Find these items' indeces\n",
    "  all_stream_item_index = [BERT_INDEX[item] for item in item_list]\n",
    "  # Use the indeces found above to locate BERT embeddings & calculate mean\n",
    "  stream_emb = ITEM_EMB_BY_IDX[all_stream_item_index].mean(axis=0)\n",
    "  # Put result into dict\n",
    "  STREAM_ITEM_EMB[stream] = stream_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.8 ms, sys: 36 ms, total: 71.7 ms\n",
      "Wall time: 70.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# store \n",
    "with open('stream_item_emb.pkl', 'wb') as handle:\n",
    "    pickle.dump(STREAM_ITEM_EMB, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_STREAM_CONTEXT.to_pickle('user_stream_context.pkl')\n",
    "STREAM_FEAT.to_pickle('stream_feat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('item_emb_by_idx.pkl', ITEM_EMB_BY_IDX, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data to use(exported)\n",
    "1. cumulative_item_emb\n",
    "2. stream_item_emb\n",
    "3. user_stream_context\n",
    "4. stream_feat\n",
    "5. item_emb\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Train DQN model\n",
    "* Input: `user_df` 253, `item_df` 231(BERT: 768), interact (?), `reward` 1\n",
    "* Output: recommend a list of items\n",
    "* Methods Needed\n",
    "    * Environment Function\n",
    "    * Choose Action\n",
    "    * Store Transition\n",
    "    * Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS_LIST = DATA['asid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stream</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000057940522534</th>\n",
       "      <td>6012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000149184139055</th>\n",
       "      <td>4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000250653732283</th>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000394583731592</th>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000409986993290</th>\n",
       "      <td>3397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  stream\n",
       "asid                    \n",
       "1000057940522534    6012\n",
       "1000149184139055    4962\n",
       "1000250653732283     906\n",
       "1000394583731592     527\n",
       "1000409986993290    3397"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAST_BOUGHT_STREAM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_STREAM_CONTEXTg Process\n",
    "\n",
    "def train(model, exp_replay, epochs, batch_size, num_episode=1000, verbose=1, reward_set='strict', hist=[], c_hist=[], rec_list=[]):\n",
    "  # total_actions = ITEM_DF.shape[0]\n",
    "  # total_episodes = len(USER_LIST)\n",
    "  # Reset win counter\n",
    "  c_win_cnt = 0\n",
    "\n",
    "  for e in range(epochs):\n",
    "    rec_cnt = 0\n",
    "    win_cnt = 0\n",
    "    loss = 0.\n",
    "    # TODO/MAIN: Apply user preference changes as epsilon\n",
    "    # epsilon for exploration - dependent inversely on the training epoch\n",
    "    epsilon = 4 / ((e + 1) ** (1 / 2))\n",
    "\n",
    "    # handling episodes by assigning users from USER_LIST\n",
    "    # Each user represent an Episode\n",
    "    episodes = np.random.choice(USER_LIST, num_episode, replace=False)\n",
    "\n",
    "    print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "    # ------------------- Episode (User) -------------------------------\n",
    "    for asid in episodes:\n",
    "      # get [episode data, stream list, final stream] by asid\n",
    "      user_all_streams = USER_STREAM_CONTEXT.xs(asid, level=\"asid\")\n",
    "      stream_list = user_all_streams.index\n",
    "      final_stream = LAST_BOUGHT_STREAM.loc[asid, 'stream']\n",
    "      \n",
    "      # ----------------- Runs (User x All_Stream) ---------------------\n",
    "      for i, stream in enumerate(stream_list):          \n",
    "        game_over = stream == final_stream\n",
    "                \n",
    "        # Get full state: current_state = user_stream + item_stream\n",
    "        # 用上一場紀錄預測下一場直播會購買的商品\n",
    "        current_state = get_full_state(user_all_streams, stream_list, i)\n",
    "        stream_items = STREAM_ITEM_DICT[stream]\n",
    "        \n",
    "        # --------------- Explore/Exploit Section ----------------------\n",
    "        if np.random.rand() <= epsilon:\n",
    "          # Explore by randomly select 10/n items from candidate_items\n",
    "          # Get all items from the stream\n",
    "          sample_actions = random.sample(stream_items, 10) if len(stream_items) > 10 else stream_items\n",
    "          action_ids = gen_exist_series(sample_actions, stream_items)\n",
    "        else:\n",
    "          # Exploit by choosing action from the model's prediction\n",
    "          pred_actions = model_predict_top10(model, current_state)\n",
    "          action_ids = gen_exist_series(pred_actions, stream_items)\n",
    "\n",
    "        # --------------- Get next state & info to store ---------------\n",
    "        reward = get_reward(asid, stream, action_ids)\n",
    "        next_state = get_full_state(user_all_streams, stream_list, i+1) if not game_over else []\n",
    "\n",
    "        rec_cnt += 1\n",
    "        if sum(reward) > 0:\n",
    "          c_win_cnt += 1\n",
    "          win_cnt += 1\n",
    "\n",
    "        # --------------- Calculating Interest Changes -----------------\n",
    "        interest_score = calculate_interest_change(user_all_streams, stream_list, i)\n",
    "\n",
    "        # --------------- Store Experience -----------------------------\n",
    "        exp_replay.remember(interest_score,\n",
    "                            [current_state, action_ids, reward, next_state],\n",
    "                            game_over)\n",
    "        \n",
    "\n",
    "        # --------------- Load batch of experiences --------------------\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "        # train model on experiences\n",
    "        batch_loss = model.train_on_batch(inputs, targets)\n",
    "        loss += batch_loss\n",
    "            \n",
    "    if verbose > 0:\n",
    "      print(f'Epoch: {e}/{epochs} | Loss {loss} | Win count {win_cnt} | Rec count {rec_cnt} | Time {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "    \n",
    "    # Track win history to later check if our model is improving at the game over time.\n",
    "    hist.append(win_cnt)\n",
    "    c_hist.append(c_win_cnt)\n",
    "    rec_list.append(rec_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 2  # Number of experiences we use for training per batch\n",
    "EPOCH = 50\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 13:28:43\n"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)# Our model's architecture parameters\n",
    "input_size = 473 # The input shape for model - this comes from the output shape of the CNN Mobilenet\n",
    "\n",
    "# Setting up the model with keras.\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(HIDDEN_SIZE, input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dense(HIDDEN_SIZE, activation='tanh'))\n",
    "model.add(Dense(TOTAL_ACTIONS))\n",
    "model.compile(Adam(learning_rate=.000001), \"mse\")\n",
    "\n",
    "hist = []\n",
    "c_hist = []\n",
    "rec_list = []\n",
    "\n",
    "# Training the model\n",
    "train(model, \n",
    "      exp_replay, \n",
    "      epochs=EPOCH, \n",
    "      batch_size=BATCH_SIZE, \n",
    "      num_episode=NUM_EPISODE, \n",
    "      verbose=1, \n",
    "      reward_set='strict',\n",
    "      hist=hist,\n",
    "      c_hist=c_hist,\n",
    "      rec_list=rec_list)\n",
    "plt.plot(range(EPOCH), hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
