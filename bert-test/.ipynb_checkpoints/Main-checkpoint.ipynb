{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_REPS = pd.read_pickle('final_context.pkl')\n",
    "STREAM_ITEM_DICT = pd.read_pickle('stream_item_dict.pkl')\n",
    "BERT_BY_IDX_DF = pd.read_pickle('bert_by_idx_pca.pkl')\n",
    "BOUGHT_DICT = pd.read_pickle('bought_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1397141, 181), 7701, (162189, 160), 79207)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_REPS.shape, len(STREAM_ITEM_DICT), BERT_BY_IDX_DF.shape, len(BOUGHT_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_LIST = CONTEXT_REPS.index.get_level_values('asid').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "retrieve full state -> should be exported to pickle\n",
    "'''\n",
    "USER_ALL_STREAM_INIT = CONTEXT_REPS.describe().loc['50%']\n",
    "\n",
    "def get_full_state(asid, user_all_streams, stream_list, i):\n",
    "  # Get full state: current_state = user_stream + item_stream\n",
    "  # 第一次參加直播/cold start\n",
    "  # CE paper: user_interest part init with random vector\n",
    "  # TODO init with random values\n",
    "  #      Cold start problem\n",
    "  \n",
    "  #! 前一個另外處理，其他人直接黏在一起\n",
    "  user_part = USER_ALL_STREAM_INIT.copy() if (i - 1) == -1 else user_all_streams.loc[stream_list[(i - 1)]]\n",
    "  return user_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LB_ITEMS = [f'i{x}' for x in range(160)]\n",
    "INPUT_DF_COL__USR = CONTEXT_REPS.columns.to_list()\n",
    "INPUT_DF_COL = INPUT_DF_COL__USR + LB_ITEMS\n",
    "\n",
    "'''\n",
    "Convert state format to model input format\n",
    "'''\n",
    "def get_input(input_state):\n",
    "  # Get item feats\n",
    "  item_list = STREAM_ITEM_DICT[input_state.name[1]]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list]\n",
    "\n",
    "  # Create new df\n",
    "  stream_item_feat = pd.DataFrame(columns=INPUT_DF_COL)\n",
    "\n",
    "  # Fill in other context\n",
    "  stream_item_feat = stream_item_feat.append([input_state]*len(item_list),ignore_index=True)\n",
    "\n",
    "  # stream_item_feat\n",
    "  stream_item_feat[LB_ITEMS] = item_feat.reset_index(drop=True)\n",
    "  return stream_item_feat.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate series: whether elements in A existed in list B\n",
    "A, B: List\n",
    "return: pd.Series\n",
    "example:\n",
    "  A: [1, 2, 4, 5]\n",
    "  B: [1, 2, 3, 4, 5, 6, 7]\n",
    "  return: Series([1, 1, 0, 1, 1, 0, 0], index=[1, 2, 3, 4, 5, 6, 7])\n",
    "'''\n",
    "def gen_exist_series(A, B):\n",
    "  exist_list = [int(item in A) for item in B]\n",
    "  return pd.Series(exist_list, index=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Comparison function for reward\n",
    "！考慮「所有」歷史購買紀錄！！！！！！\n",
    "'''\n",
    "def r(a, b):\n",
    "  # if a==1 and b==0: return 0 # -1 when the rule is to punish unrec-bought\n",
    "  # else: \n",
    "  return a & b\n",
    "\n",
    "def get_reward(asid, stream, action_ids):\n",
    "  items = action_ids.index # 不確定 action_id 裡面是 series 還是 list\n",
    "  real_bought_ids = BOUGHT_DICT[asid]\n",
    "  real_bought_ids_series = gen_exist_series(real_bought_ids, items)\n",
    "  \n",
    "  reward_list = [r(a, b) for a, b in zip(real_bought_ids_series.values, action_ids.values)]\n",
    "  return pd.Series(reward_list, index=items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_top10(model, input_state):\n",
    "  # Get all items\n",
    "  full_input = get_input(input_state).astype('float32')\n",
    "  \n",
    "  # 紀錄所有預測結果\n",
    "  predicts = model.predict(full_input)\n",
    "  full_input['predict'] = predicts\n",
    "  actions = full_input['predict'].nlargest(10).index #['i_item_id'].to_list()\n",
    "  actions = full_input.loc[actions, 'i_item_id'].values\n",
    "  return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collecting Training Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, max_memory=100000, discount=.9):\n",
    "    \"\"\"\n",
    "    Setup\n",
    "    max_memory: the maximum number of experiences we want to store\n",
    "    memory: a list of experiences\n",
    "    discount: the discount factor for future experience\n",
    "    In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "    [...\n",
    "    [experience, game_over]\n",
    "    [experience, game_over]\n",
    "    ...]\n",
    "    \"\"\"\n",
    "    self.max_memory = max_memory\n",
    "    self.memory = list()\n",
    "    self.discount = discount\n",
    "\n",
    "  def remember(self, interest_score, states, game_over):\n",
    "    # Save a state to memory\n",
    "    self.memory.append([interest_score, states, game_over])\n",
    "    # We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "    if len(self.memory) > self.max_memory:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def get_batch(self, model, batch_size=10):\n",
    "    # How many experiences do we have?\n",
    "    len_memory = len(self.memory)\n",
    "\n",
    "    # Calculate the number of actions that can possibly be taken in the game.\n",
    "    # Actions: 0 = not recommend, 1 = recommend\n",
    "    num_actions = model.output_shape[-1]\n",
    "\n",
    "    # Dimensions of our observed states, ie, the input to our model.\n",
    "    # Memory:  [\n",
    "    #   [interest_score, [ [...state], action, reward, next_state_idx], game_over],\n",
    "    #   [interest_score, [ [...state], action, reward, nexr_state_idx], game_over],\n",
    "    #   ...\n",
    "    # ]\n",
    "    env_dim = len(INPUT_DF_COL)\n",
    "\n",
    "    inputs = pd.DataFrame(columns=INPUT_DF_COL)\n",
    "    targets = pd.DataFrame(columns=[0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # We draw states to learn from randomly\n",
    "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):      \n",
    "\n",
    "      # Here we load one transition <s, a, r, s'> from memory\n",
    "      state_t, action_t, reward_t, state_tp1 = self.memory[idx][1]\n",
    "      # state_t = state_t.astype('float32')\n",
    "      game_over = self.memory[idx][2]\n",
    "\n",
    "      '''\n",
    "      修改倒入 state 的方式 input = (state - item) + item_feat\n",
    "      拆掉 model_predict 成 function\n",
    "      \n",
    "      here should be state_t * all_items\n",
    "      '''\n",
    "      state_t = get_input(state_t).astype('float32')\n",
    "      # puts state into input\n",
    "      inputs = pd.concat([inputs, state_t], axis=0)\n",
    "      # TODO: Modify input shape 0\n",
    "\n",
    "      # First we fill the target values with the predictions of the model.\n",
    "      # They will not be affected by training (since the training loss for them is 0)\n",
    "      # TODO\n",
    "      '''\n",
    "      每個 actions 都會被 predict 一個成績/reward\n",
    "      '''\n",
    "\n",
    "      # if the game ended, the reward is the final reward\n",
    "      if game_over:  # if game_over is True\n",
    "        state_t['reward'] = reward_t\n",
    "        targets = pd.concat([targets, reward_t], axis=0).astype('float32')\n",
    "      else:\n",
    "        state_t['reward'] = model.predict(state_t).flatten()\n",
    "        # 找到 action_t 們，指到 state_t 上去算 discount values\n",
    "        action_t = action_t[action_t == 1].index.to_list()\n",
    "        \n",
    "        state_tp1 = get_input(state_tp1)\n",
    "        Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "        # r + gamma * max Q(s',a')\n",
    "        # DataFrame apply\n",
    "        state_t.loc[action_t, 'reward'] = state_t.loc[action_t, 'reward'].apply(lambda x: x + self.discount * Q_sa).astype('float32')\n",
    "        targets = pd.concat([targets, state_t['reward']], axis=0).astype('float32')\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, exp_replay, epochs, batch_size, num_episode=1000, verbose=1, reward_set='strict', hist=[], c_hist=[], rec_list=[]):\n",
    "  # total_actions = ITEM_DF.shape[0]\n",
    "  # total_episodes = len(USER_LIST)\n",
    "  # Reset win counter\n",
    "  c_win_cnt = 0\n",
    "\n",
    "  for e in range(epochs):\n",
    "    rec_cnt = 0\n",
    "    win_cnt = 0\n",
    "    loss = 0.\n",
    "    # TODO/MAIN: Apply user preference changes as epsilon\n",
    "    # epsilon for exploration - dependent inversely on the training epoch\n",
    "    epsilon = 4 / ((e + 1) ** (1 / 2))\n",
    "\n",
    "    # handling episodes by assigning users from USER_LIST\n",
    "    # Each user represent an Episode\n",
    "    episodes = np.random.choice(USER_LIST, num_episode, replace=False)\n",
    "\n",
    "    print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "    # ------------------- Episode (User) -------------------------------\n",
    "    for asid in episodes:\n",
    "      # get [episode data, stream list, final stream] by asid\n",
    "      user_all_streams = CONTEXT_REPS.xs(asid, level=\"asid\")\n",
    "      stream_list = user_all_streams.index\n",
    "      final_stream = max(stream_list)\n",
    "      \n",
    "      # ----------------- Runs (User x All_Stream) ---------------------\n",
    "      for i, stream in enumerate(stream_list):          \n",
    "        game_over = stream == final_stream\n",
    "                \n",
    "        # Get full state: current_state = user_stream + item_stream\n",
    "        # 用上一場紀錄預測下一場直播會購買的商品\n",
    "        current_state = get_full_state(asid, user_all_streams, stream_list, i)\n",
    "        stream_items = STREAM_ITEM_DICT[stream]\n",
    "        \n",
    "        # --------------- Explore/Exploit Section ----------------------\n",
    "        if np.random.rand() <= epsilon:\n",
    "          # Explore by randomly select 10/n items from candidate_items\n",
    "          # Get all items from the stream\n",
    "          sample_actions = random.sample(stream_items, 10) if len(stream_items) > 10 else stream_items\n",
    "          action_ids = gen_exist_series(sample_actions, stream_items)\n",
    "        else:\n",
    "          # Exploit by choosing action from the model's prediction\n",
    "          pred_actions = model_predict_top10(model, current_state)\n",
    "          action_ids = gen_exist_series(pred_actions, stream_items)\n",
    "\n",
    "        # --------------- Get next state & info to store ---------------\n",
    "        reward = get_reward(asid, stream, action_ids)\n",
    "        next_state = get_full_state(asid, user_all_streams, stream_list, i+1) if not game_over else []\n",
    "\n",
    "        rec_cnt += 1\n",
    "        if sum(reward) > 0:\n",
    "          c_win_cnt += 1\n",
    "          win_cnt += 1\n",
    "\n",
    "        # --------------- Calculating Interest Changes -----------------\n",
    "        interest_score = calculate_interest_change(user_all_streams, stream_list, i)\n",
    "\n",
    "        # --------------- Store Experience -----------------------------\n",
    "        exp_replay.remember(interest_score,\n",
    "                            [current_state, action_ids, reward, next_state],\n",
    "                            game_over)\n",
    "        \n",
    "\n",
    "        # --------------- Load batch of experiences --------------------\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "        # train model on experiences\n",
    "        batch_loss = model.train_on_batch(inputs, targets)\n",
    "        loss += batch_loss\n",
    "        \n",
    "        break\n",
    "            \n",
    "    if verbose > 0:\n",
    "      print(f'Epoch: {e}/{epochs} | Loss {loss} | Win count {win_cnt} | Rec count {rec_cnt} | Time {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "    \n",
    "    # Track win history to later check if our model is improving at the game over time.\n",
    "    hist.append(win_cnt)\n",
    "    c_hist.append(c_win_cnt)\n",
    "    rec_list.append(rec_cnt)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 2  # Number of experiences we use for training per batch\n",
    "EPOCH = 50\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 15:27:30\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'interest_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-922a957b5109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mc_hist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_hist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       rec_list=rec_list)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-3e18803d3aa2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, exp_replay, epochs, batch_size, num_episode, verbose, reward_set, hist, c_hist, rec_list)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# --------------- Store Experience -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         exp_replay.remember(interest_score,\n\u001b[0m\u001b[1;32m     61\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                             game_over)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interest_score' is not defined"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)# Our model's architecture parameters\n",
    "input_size = 473 # The input shape for model - this comes from the output shape of the CNN Mobilenet\n",
    "\n",
    "# Setting up the model with keras.\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(HIDDEN_SIZE, input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dense(HIDDEN_SIZE, activation='tanh'))\n",
    "model.add(Dense(TOTAL_ACTIONS))\n",
    "model.compile(Adam(learning_rate=.000001), \"mse\")\n",
    "\n",
    "hist = []\n",
    "c_hist = []\n",
    "rec_list = []\n",
    "\n",
    "# Training the model\n",
    "train(model, \n",
    "      exp_replay, \n",
    "      epochs=EPOCH, \n",
    "      batch_size=BATCH_SIZE, \n",
    "      num_episode=NUM_EPISODE, \n",
    "      verbose=1, \n",
    "      reward_set='strict',\n",
    "      hist=hist,\n",
    "      c_hist=c_hist,\n",
    "      rec_list=rec_list)\n",
    "plt.plot(range(EPOCH), hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
