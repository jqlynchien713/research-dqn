{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888 0926 A34-(8包)黃金亞麻粉(500g±)(包裝隨機)*8\n",
      "888 0926 A32-(6盒)公主派對。蚓激酶膠囊(30顆/盒)*6\n",
      "888 0926 A29-孟宗竹砧板(大)*1\n",
      "888 0926 A31-(10件組)公主派對x三郎拍賣。備長碳洗衣粉 (一盒)*4+Spearmint香香粒 200g (味道隨機)*4+cab's 魔力奈米陶瓷洗衣球*2\n",
      "888 0926 A27-(3罐)STONE CARE WAX 大理石護理蠟*3---結單缺貨刪單不通知\n",
      "叫賣 0926 A18-Kolin歌林 旗艦水洗電鬍刀 KSH-HCW11U(保固一年)*1--結單缺貨刪單不通知\n",
      "叫賣 0926 A25-歌林手持無線電動掃地機(KTC-MN35)*1\n",
      "888 0926 A28-自動開合玻璃油壺 300ML*1\n",
      "叫賣 0926 A32-飛狼SWAG運動口袋腰包(JW-591BK)(顏色隨機)*1\n",
      "叫賣 0926 A34-妙管家悶燒罐提袋組800ML(HKVC-1004)*1\n"
     ]
    }
   ],
   "source": [
    "# [Inference] Main Training\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import line_profiler\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Fix Random Seeds\n",
    "\n",
    "def same_seeds(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "  np.random.seed(seed)  \n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)\n",
    "\n",
    "## Prepare Data\n",
    "\n",
    "CONTEXT_REPS = pd.read_pickle('../../data/simple_context.pkl')\n",
    "STREAM_ITEM_DICT = pd.read_pickle('../../data/stream_item_dict.pkl')\n",
    "BERT_BY_IDX_DF = pd.read_pickle('../../data/bert_by_idx_pca.pkl')\n",
    "BOUGHT_DICT = pd.read_pickle('../../data/bought_dict.pkl')\n",
    "USER_ALL_STREAM_INIT = CONTEXT_REPS.describe().loc['50%']\n",
    "\n",
    "CONTEXT_REPS.shape, len(STREAM_ITEM_DICT), BERT_BY_IDX_DF.shape, len(BOUGHT_DICT)\n",
    "\n",
    "USER_LIST = CONTEXT_REPS.index.get_level_values('asid').tolist()\n",
    "\n",
    "LB_ITEMS = ['item_id'] + [f'i{x}' for x in range(160)]\n",
    "INPUT_DF_COL__USR = CONTEXT_REPS.columns.to_list()\n",
    "INPUT_DF_COL = INPUT_DF_COL__USR + LB_ITEMS\n",
    "\n",
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "Convert state format to model input format\n",
    "'''\n",
    "def get_input_tensor(input_state, current_stream, with_tensor=False):\n",
    "  # Get item feats\n",
    "  # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
    "  item_list = STREAM_ITEM_DICT[current_stream]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
    "\n",
    "  # Fill in other context\n",
    "  stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
    "  \n",
    "  # Merge with items\n",
    "  stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
    "  \n",
    "  # Convert to tensor\n",
    "  if with_tensor: \n",
    "    stream_item_feat_tensor = df_to_tensor(stream_item_feat)\n",
    "    return stream_item_feat_tensor, stream_item_feat\n",
    "  else:\n",
    "    return stream_item_feat\n",
    "\n",
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "\n",
    "Generate series: whether elements in A existed in list B\n",
    "A, B: List\n",
    "return: pd.Series\n",
    "example:\n",
    "  A: [1, 2, 4, 5]\n",
    "  B: [1, 2, 3, 4, 5, 6, 7]\n",
    "  return: Series([1, 1, 0, 1, 1, 0, 0], index=[1, 2, 3, 4, 5, 6, 7])\n",
    "'''\n",
    "def gen_exist_series(A, B):\n",
    "  return [int(item in A) for item in B]\n",
    "\n",
    "def df_to_tensor(input_df):\n",
    "  return torch.tensor(input_df.values).to(DEVICE).float()\n",
    "## Replay\n",
    "\n",
    "class ReplayBuffer:\n",
    "  def __init__(self, max_memory=100000, discount=.9, model_output_shape=1):\n",
    "    \"\"\"\n",
    "    Setup\n",
    "    max_memory: the maximum number of experiences we want to store\n",
    "    memory: a list of experiences\n",
    "    discount: the discount factor for future experience\n",
    "    In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "    [...\n",
    "    [experience, game_over]\n",
    "    [experience, game_over]\n",
    "    ...]\n",
    "    \"\"\"\n",
    "    self.max_memory = max_memory\n",
    "    self.memory = list()\n",
    "    self.discount = discount\n",
    "    self.model_output_shape = model_output_shape\n",
    "\n",
    "  def remember(self, states, game_over):\n",
    "    # Save a state to memory\n",
    "    self.memory.append([states, game_over])\n",
    "    # We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "    if len(self.memory) > self.max_memory:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def get_batch(self, eval_net, target_net, batch_size=10):\n",
    "    # How many experiences do we have?\n",
    "    len_memory = len(self.memory)\n",
    "\n",
    "    # Calculate the number of actions that can possibly be taken in the game.\n",
    "    # Actions: 0 = not recommend, 1 = recommend\n",
    "    num_actions = self.model_output_shape\n",
    "\n",
    "    # Dimensions of our observed states, ie, the input to our model.\n",
    "    # Memory:  [\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
    "    #   ...\n",
    "    # ]\n",
    "    env_dim = len(INPUT_DF_COL)\n",
    "\n",
    "    inputs = pd.DataFrame()\n",
    "    targets = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    # We draw states to learn from randomly\n",
    "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
    "      # Here we load one transition <s, a, r, s'> from memory\n",
    "      streams, state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "      current_stream, next_stream = streams\n",
    "      game_over = self.memory[idx][1]\n",
    "\n",
    "      '''\n",
    "      修改倒入 state 的方式 input = (state - item) + item_feat\n",
    "      拆掉 model_predict 成 function\n",
    "      \n",
    "      here should be state_t * all_items\n",
    "      '''\n",
    "      state_tensor, state_t = get_input_tensor(state_t, current_stream, with_tensor=True)\n",
    "      # puts state into input\n",
    "      inputs = pd.concat([inputs, state_t], axis=0)\n",
    "      \n",
    "      # use target_net to predict target for eval_net to learn\n",
    "      current_target = target_net(state_tensor).detach().view(len(reward_t), 1)\n",
    "\n",
    "      selected_ids = np.where(action_t > 0)[0]\n",
    "      reward_t = df_to_tensor(reward_t).view(len(reward_t), 1)\n",
    "      \n",
    "      \n",
    "      '''\n",
    "      每個 actions 都會被 predict 一個成績/reward\n",
    "      '''\n",
    "      # if the game ended, the reward is the final reward\n",
    "      if game_over:  # if game_over is True\n",
    "        current_target[selected_ids] = reward_t[selected_ids]\n",
    "      else:\n",
    "        state_tp1, _ = get_input_tensor(state_tp1, next_stream, with_tensor=True)\n",
    "        Q_sa = torch.max(target_net(state_tp1).detach())\n",
    "        \n",
    "        # r + gamma * max Q(s',a')\n",
    "        # current_target = reward_t + self.discount * Q_sa\n",
    "        current_target[selected_ids] = reward_t[selected_ids] + Q_sa * self.discount\n",
    "\n",
    "      targets = torch.cat((targets, current_target), 0)\n",
    "    return inputs, targets\n",
    "\n",
    "## Epsilon\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "\n",
    "class Epsilon(ABC):\n",
    "  @abstractmethod\n",
    "  def clear(self):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def get_epsilon(self, key):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_step(self, key, data):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_epoch(self, data):\n",
    "    pass\n",
    "  \n",
    "  # @abstractmethod\n",
    "  # def update_at_epsisode():\n",
    "  #   pass\n",
    "\n",
    "\n",
    "class Decay(Epsilon):\n",
    "  # Ref: Decay(0.5, 0.85)\n",
    "  '''\n",
    "  Epsilon Decay EE method with update/decay at epoch\n",
    "  '''\n",
    "  def __init__(self, initial, epoch_decay, step_decay):\n",
    "    self.initial = initial\n",
    "    self.epoch_decay, self.step_decay = epoch_decay, step_decay\n",
    "    self.epsilon = self.initial\n",
    "    \n",
    "  def clear(self):\n",
    "    self.epsilon = self.initial # should be 4 for origin setting\n",
    "    \n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon\n",
    "  \n",
    "  def update_at_step(self, key, data):\n",
    "    # origin setting\n",
    "    pass\n",
    "    # exponentially\n",
    "    # self.epsilon *= self.step_decay\n",
    "    \n",
    "  def update_at_epoch(self, data):\n",
    "    # origin settings\n",
    "    epoch = data\n",
    "    self.epsilon = 4 / ((epoch + 1) ** (1 / 2))\n",
    "    # exponentially\n",
    "    # self.epsilon *= self.epoch_decay\n",
    "\n",
    "\n",
    "class VDBE(Epsilon):\n",
    "  # VDBE(0.5, 0.01)\n",
    "  def __init__(self, initial, sigma):\n",
    "    self.initial = initial\n",
    "    self.sigma = sigma\n",
    "\n",
    "  def clear(self):\n",
    "    self.epsilon = defaultdict(lambda: self.initial)\n",
    "\n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon[key]\n",
    "  \n",
    "  def update_at_step(self, key, data, delta):\n",
    "    td_error = data\n",
    "    coeff = math.exp(-abs(td_error) / self.sigma)\n",
    "    f = (1.0 - coeff) / (1.0 + coeff)\n",
    "    self.epsilon[key] = delta * f + (1.0 - delta) * self.epsilon[key]\n",
    "  \n",
    "  def update_at_epoch(self, data):\n",
    "    pass\n",
    "\n",
    "## DQN\n",
    "\n",
    "class DQN(object):\n",
    "  def __init__(self, exp_replay, epsilon, num_episode, epochs, batch_size, lr, switch_param_threshold):\n",
    "    self.eval_net, self.target_net = Net(), Net()\n",
    "    self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "    self.exp_replay = exp_replay\n",
    "    self.epsilon = epsilon\n",
    "    self.num_episode = num_episode\n",
    "    self.epochs = epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.switch_param_threshold = switch_param_threshold\n",
    "    self.user_all_stream_init = USER_ALL_STREAM_INIT\n",
    "    self.hist = []\n",
    "    self.c_hist = []\n",
    "    self.rec_list = []\n",
    "    self.ep_score_list = []\n",
    "    self.learn_step_counter = 0\n",
    "\n",
    "  # Environment Methods\n",
    "  def __episodes(self):\n",
    "    # return USER_LIST[:self.num_episode]\n",
    "    return np.random.choice(USER_LIST, self.num_episode, replace=False)\n",
    "  \n",
    "  def __user_episode_context(self):\n",
    "    self.user_all_streams = CONTEXT_REPS.xs(self.asid, level=\"asid\")\n",
    "    self.stream_list = self.user_all_streams.index\n",
    "    self.final_stream = max(self.stream_list)\n",
    "  \n",
    "  def __full_state(self, i):\n",
    "    '''\n",
    "    retrieve full state -> should be exported to pickle\n",
    "    '''\n",
    "    if (i - 1) == -1:\n",
    "      user_part = self.user_all_stream_init.copy()\n",
    "      user_part.name = self.stream_list[i]\n",
    "    else:\n",
    "      user_part = self.user_all_streams.loc[self.stream_list[(i - 1)]]\n",
    "    return user_part\n",
    "\n",
    "  def reward(self):\n",
    "    '''\n",
    "    Comparison function for reward, 考慮「所有」歷史購買紀錄\n",
    "    '''\n",
    "    real_bought_ids = BOUGHT_DICT[self.asid]\n",
    "    real_bought_ids_series = gen_exist_series(real_bought_ids, self.stream_items)\n",
    "    \n",
    "    reward_list = [a & b for a, b in zip(real_bought_ids_series, self.action_ids)]\n",
    "    # Reward Count \n",
    "    self.rec_cnt += 1\n",
    "    if sum(reward_list) > 0:\n",
    "      self.c_win_cnt += 1\n",
    "      self.win_cnt += 1\n",
    "      self.ep_score += sum(reward_list)\n",
    "    # return list(map(lambda x: x * sum(reward_list), reward_list))\n",
    "    return pd.Series(list(map(lambda x: x * sum(reward_list), reward_list)), index=self.stream_items)\n",
    "\n",
    "  # Agent Methods\n",
    "  def __choose_actions(self):\n",
    "    if np.random.rand() <= self.epsilon.get_epsilon(f'{self.asid}-{self.current_stream}'):\n",
    "    # if len(self.exp_replay.memory) < 1:\n",
    "      # Explore by randomly select 10/n items from candidate_items\n",
    "      # Get all items from the stream\n",
    "      self.explore += 1\n",
    "      selected_actions = random.sample(self.stream_items, 10) if len(self.stream_items) > 10 else self.stream_items\n",
    "    else:\n",
    "      # Exploit by choosing action from the model's prediction\n",
    "      self.exploit += 1\n",
    "      selected_actions = self.__agent_predict()\n",
    "    x = pd.Series(0, index=self.stream_items)\n",
    "    x.loc[selected_actions] = 1\n",
    "    return x\n",
    "    \n",
    "  def q_value(self): \n",
    "    if type(self.epsilon) == Decay: return 0\n",
    "\n",
    "    predicts = self.eval_net(self.full_input).flatten()    \n",
    "    actions_idx = np.where(self.action_ids.values == 1)[0]\n",
    "    q_val = predicts[actions_idx].mean()\n",
    "    return q_val\n",
    "\n",
    "  def __agent_predict(self):\n",
    "    predicts = self.eval_net(self.full_input).flatten()\n",
    "    if len(predicts) > 10:\n",
    "      top10_idx = torch.topk(predicts, 10).indices.cpu()\n",
    "      actions = self.candidate_actions.iloc[top10_idx]['item_id'].values\n",
    "    else:\n",
    "      actions = self.candidate_actions['item_id'].values\n",
    "    return actions\n",
    "\n",
    "  def __train_agent_batch(self, inputs, targets):\n",
    "    self.optimizer.zero_grad()\n",
    "    outputs = self.eval_net(inputs)\n",
    "    loss = self.loss_fn(outputs, targets)\n",
    "    # Add CL Regularization Term\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "  \n",
    "  def inference(self, asid, state):\n",
    "    self.asid = asid\n",
    "    self.current_stream = state.name\n",
    "    self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
    "    self.full_input, self.candidate_actions = get_input_tensor(state, self.current_stream, with_tensor=True)\n",
    "    selected_actions = self.__agent_predict().astype(int)\n",
    "    # reward = self.reward()\n",
    "    # reward = reward.loc[reward > 0].index.tolist()\n",
    "    return selected_actions#, reward\n",
    "\n",
    "  # MAIN TRAIN\n",
    "  def train(self):\n",
    "    self.eval_net.to(DEVICE)\n",
    "    self.target_net.to(DEVICE)\n",
    "    self.c_win_cnt = 0\n",
    "    self.eval_net.train(True)\n",
    "    self.epsilon.clear()\n",
    "\n",
    "    for e in self.epochs:\n",
    "      self.rec_cnt = 0\n",
    "      self.win_cnt = 0\n",
    "      self.loss = 0.\n",
    "      self.ep_score = 0\n",
    "      self.explore = 0\n",
    "      self.exploit = 0\n",
    "    \n",
    "      print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "      # ------------------- Episode (User) -------------------------------\n",
    "      for asid in tqdm(self.__episodes()):\n",
    "        self.asid = asid\n",
    "        self.__user_episode_context()\n",
    "\n",
    "        # ----------------- Runs (User x All_Stream) ---------------------\n",
    "        for i, stream in enumerate(self.stream_list):\n",
    "          game_over = stream == self.final_stream\n",
    "          self.current_stream = stream\n",
    "          self.current_state = self.__full_state(i)\n",
    "          self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
    "          self.full_input, self.candidate_actions = get_input_tensor(self.current_state, self.current_stream, with_tensor=True)\n",
    "\n",
    "          # --------------- Explore/Exploit Section ----------------------\n",
    "          self.action_ids = self.__choose_actions()\n",
    "\n",
    "          # --------------- Get next state & info to store ---------------\n",
    "          reward = self.reward()\n",
    "          next_state = self.__full_state(i+1) if not game_over else []\n",
    "          next_stream = 0 if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
    "          self.exp_replay.remember([[stream, next_stream], self.current_state, self.action_ids, reward, next_state], game_over)\n",
    "          self.learn_step_counter += 1\n",
    "          if self.learn_step_counter % self.switch_param_threshold == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "\n",
    "\n",
    "          # --------------- Load batch of experiences --------------------\n",
    "          inputs, targets = self.exp_replay.get_batch(self.eval_net, self.target_net, batch_size=self.batch_size)\n",
    "          inputs = df_to_tensor(inputs)\n",
    "          # store pre-training value for td_error\n",
    "          old_Q = self.q_value()\n",
    "          batch_loss = self.__train_agent_batch(inputs, targets)\n",
    "          # store post-training value for td_error\n",
    "          new_Q = self.q_value()\n",
    "          self.loss += batch_loss\n",
    "\n",
    "          # --------------- Update with TD error -------------------------\n",
    "          self.epsilon.update_at_step(f'{self.asid}-{self.current_stream}', (new_Q - old_Q), len(self.stream_items))\n",
    "\n",
    "      # Track win history to later check if our model is improving at the game over time.\n",
    "      self.hist.append(self.win_cnt)\n",
    "      self.c_hist.append(self.c_win_cnt)\n",
    "      self.rec_list.append(self.rec_cnt)\n",
    "      self.ep_score_list.append(self.ep_score)\n",
    "\n",
    "      print(f'Epoch: {e}/{len(self.epochs)} | Loss {self.loss} | Epoch Hit Rate {self.win_cnt/self.rec_cnt} | \\\n",
    "              Cumulative Hit Rate {self.c_win_cnt/sum(self.rec_list)} | Explore {self.explore} | Exploit {self.exploit} | \\\n",
    "              Score {self.ep_score}')\n",
    "\n",
    "## Main Method\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 2  # Number of experiences we use for training per batch\n",
    "EPOCH = range(100)\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 1.0e-4\n",
    "SWITCH_PARAM_THRESHOLD = 100\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(380, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, 128)\n",
    "    self.fc4 = nn.Linear(128, 64)\n",
    "    self.fc5 = nn.Linear(64, 1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc4(x)\n",
    "    x = self.tanh(x)\n",
    "    x = self.fc5(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# ----------\n",
    "## Inference\n",
    "\n",
    "vdbe_ddqn = pd.read_pickle('../Models/vdbe_100_ddqn.pkl')\n",
    "\n",
    "user = '1000057940522534'\n",
    "\n",
    "context = CONTEXT_REPS.xs('1000057940522534', level='asid').iloc[-1]\n",
    "\n",
    "selected_items = vdbe_ddqn.inference(user, context)\n",
    "\n",
    "item_list = pd.read_pickle('../../data/index_to_item_list.pkl')\n",
    "\n",
    "for i in selected_items: print(item_list[i])\n",
    "\n",
    "# for i in rewards: print(item_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_REPS = pd.read_pickle('../../data/w_final_context.pkl')\n",
    "USER_LIST = CONTEXT_REPS.index.get_level_values('asid').unique().tolist()[:100]\n",
    "simple_context = CONTEXT_REPS[CONTEXT_REPS.index.get_level_values(\"asid\").isin(USER_LIST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_context.to_pickle('../../data/simple_context.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000057940522534\n",
      "1000149184139055\n",
      "1000250653732283\n",
      "1000394583731592\n",
      "1000409986993290\n",
      "1000423206992036\n",
      "1000553193805792\n",
      "1000589583850341\n",
      "1000633920390718\n",
      "1000819440430954\n",
      "1000957906997529\n",
      "1000963976758847\n",
      "1001080880276189\n",
      "1001324550069084\n",
      "100136929093933\n",
      "1001431227072878\n",
      "100148518746060\n",
      "100149508669531\n",
      "1001659963549176\n",
      "100167471891988\n",
      "1001757413527281\n",
      "100178218925871\n",
      "1001893563877122\n",
      "1001894990260384\n",
      "100195065450589\n",
      "100203608791887\n",
      "1002038786835392\n",
      "100212032319094\n",
      "1002210203484869\n",
      "1002269363468282\n",
      "1002312599964580\n",
      "1002338176767192\n",
      "100234279080967\n",
      "1002379003572798\n",
      "100241592368627\n",
      "1002522596786391\n",
      "100255168585222\n",
      "100261762545667\n",
      "1002683636869020\n",
      "100281751886037\n",
      "100282318494432\n",
      "1002830366844609\n",
      "100289405694390\n",
      "100300209255064\n",
      "100300478719313\n",
      "1003123449898961\n",
      "1003319080133058\n",
      "100333772275481\n",
      "100334462314361\n",
      "1003455763170860\n",
      "1003508910067984\n",
      "1003517779858218\n",
      "100353798789259\n",
      "1003692537041261\n",
      "100376219187419\n",
      "1003923996752370\n",
      "1003925263457178\n",
      "1003937330048114\n",
      "100394815713642\n",
      "100405662291810\n",
      "100408985683868\n",
      "100415871683576\n",
      "100417648845561\n",
      "100434472210980\n",
      "1004371873104504\n",
      "1004384703347194\n",
      "100442675510468\n",
      "1004501586563412\n",
      "1004506119890038\n",
      "1004576646975515\n",
      "1004582000338650\n",
      "1004590483233195\n",
      "1004726733194132\n",
      "1004811646752885\n",
      "100484055230918\n",
      "100488951649399\n",
      "1004935853213240\n",
      "100497435677218\n",
      "100498748585690\n",
      "100504065802153\n",
      "100507838841788\n",
      "1005088286489614\n",
      "1005120933197139\n",
      "1005282543157629\n",
      "1005316436473974\n",
      "1005331043257440\n",
      "1005378936570802\n",
      "100544505781471\n",
      "1005576513218314\n",
      "100564995455083\n",
      "100566869194136\n",
      "100568329264410\n",
      "1005706236554477\n",
      "1005735829826110\n",
      "100579125678617\n",
      "1005815463083130\n",
      "100586001795909\n",
      "1005911546905018\n",
      "100594925755639\n",
      "100601475215758\n"
     ]
    }
   ],
   "source": [
    "inf_result = {}\n",
    "for user in USER_LIST:\n",
    "  print(user)\n",
    "  context = CONTEXT_REPS.xs(user, level='asid').iloc[-1]\n",
    "  selected_items = vdbe_ddqn.inference(user, context)\n",
    "  # rec_list = [item_list[i] for i in selected_items]\n",
    "  inf_result[user] = selected_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inf_result_idx.pkl', 'wb') as f:\n",
    "  pickle.dump(inf_result, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
