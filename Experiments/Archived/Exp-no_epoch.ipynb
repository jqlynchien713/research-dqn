{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import line_profiler\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "  np.random.seed(seed)  \n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "CONTEXT_REPS = pd.read_pickle('../../data/CONTEXT_REPS_CLEAN_ASID.pkl')\n",
    "STREAM_ITEM_DICT = pd.read_pickle('../../data/stream_item_dict.pkl')\n",
    "BERT_BY_IDX_DF = pd.read_pickle('../../data/bert_by_idx_pca.pkl')\n",
    "BOUGHT_DICT = pd.read_pickle('../../data/bought_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1333083, 219), 7701, (162189, 160), 79207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_REPS.shape, len(STREAM_ITEM_DICT), BERT_BY_IDX_DF.shape, len(BOUGHT_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "USER_LIST = CONTEXT_REPS.index.get_level_values('asid').unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "LB_ITEMS = ['item_id'] + [f'i{x}' for x in range(160)]\n",
    "INPUT_DF_COL__USR = CONTEXT_REPS.columns.to_list()\n",
    "INPUT_DF_COL = INPUT_DF_COL__USR + LB_ITEMS\n",
    "\n",
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "Convert state format to model input format\n",
    "'''\n",
    "def get_input_tensor(input_state, current_stream, with_tensor=False):\n",
    "  # Get item feats\n",
    "  # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
    "  item_list = STREAM_ITEM_DICT[current_stream]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
    "\n",
    "  # Fill in other context\n",
    "  stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
    "  \n",
    "  # Merge with items\n",
    "  stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
    "  \n",
    "  # Convert to tensor\n",
    "  if with_tensor: \n",
    "    stream_item_feat_tensor = df_to_tensor(stream_item_feat)\n",
    "    return stream_item_feat_tensor, stream_item_feat\n",
    "  else:\n",
    "    return stream_item_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "\n",
    "Generate series: whether elements in A existed in list B\n",
    "A, B: List\n",
    "return: pd.Series\n",
    "example:\n",
    "  A: [1, 2, 4, 5]\n",
    "  B: [1, 2, 3, 4, 5, 6, 7]\n",
    "  return: Series([1, 1, 0, 1, 1, 0, 0], index=[1, 2, 3, 4, 5, 6, 7])\n",
    "'''\n",
    "def gen_exist_series(A, B):\n",
    "  return [int(item in A) for item in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_tensor(input_df):\n",
    "  return torch.tensor(input_df.values).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self, max_memory=100000, discount=.9, model_output_shape=1):\n",
    "    \"\"\"\n",
    "    Setup\n",
    "    max_memory: the maximum number of experiences we want to store\n",
    "    memory: a list of experiences\n",
    "    discount: the discount factor for future experience\n",
    "    In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "    [...\n",
    "    [experience, game_over]\n",
    "    [experience, game_over]\n",
    "    ...]\n",
    "    \"\"\"\n",
    "    self.max_memory = max_memory\n",
    "    self.memory = list()\n",
    "    self.discount = discount\n",
    "    self.model_output_shape = model_output_shape\n",
    "\n",
    "  def remember(self, states, game_over):\n",
    "    # Save a state to memory\n",
    "    self.memory.append([states, game_over])\n",
    "    # We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "    if len(self.memory) > self.max_memory:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def get_batch(self, eval_net, target_net, structure, batch_size=10):\n",
    "    # How many experiences do we have?\n",
    "    len_memory = len(self.memory)\n",
    "\n",
    "    # Calculate the number of actions that can possibly be taken in the game.\n",
    "    # Actions: 0 = not recommend, 1 = recommend\n",
    "    num_actions = self.model_output_shape\n",
    "\n",
    "    # Dimensions of our observed states, ie, the input to our model.\n",
    "    # Memory:  [\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
    "    #   ...\n",
    "    # ]\n",
    "    env_dim = len(INPUT_DF_COL)\n",
    "\n",
    "    inputs = pd.DataFrame()\n",
    "    targets = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    # We draw states to learn from randomly\n",
    "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
    "      # Here we load one transition <s, a, r, s'> from memory\n",
    "      streams, state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "      current_stream, next_stream = streams\n",
    "      game_over = self.memory[idx][1]\n",
    "\n",
    "      '''\n",
    "      修改倒入 state 的方式 input = (state - item) + item_feat\n",
    "      拆掉 model_predict 成 function\n",
    "\n",
    "      here should be state_t * all_items\n",
    "      '''\n",
    "      state_tensor, state_t = get_input_tensor(state_t, current_stream, with_tensor=True)\n",
    "      # puts state into input\n",
    "      inputs = pd.concat([inputs, state_t], axis=0)\n",
    "\n",
    "      reward_t = df_to_tensor(reward_t).view(len(reward_t), 1)\n",
    "\n",
    "      '''\n",
    "      每個 actions 都會被 predict 一個成績/reward\n",
    "      '''\n",
    "      # if the game ended, the reward is the final reward\n",
    "      if game_over:  # if game_over is True\n",
    "        current_target = reward_t\n",
    "      else:\n",
    "        state_tp1, _ = get_input_tensor(state_tp1, next_stream, with_tensor=True)\n",
    "        if target_net == None:\n",
    "          with torch.no_grad():\n",
    "            Q_sa = torch.max(eval_net(state_tp1))\n",
    "        elif structure == 'target':\n",
    "          with torch.no_grad():\n",
    "            Q_sa = torch.max(target_net(state_tp1))\n",
    "        elif structure == 'double':\n",
    "          with torch.no_grad():\n",
    "            _, selected_actions = eval_net(state_tp1).max(dim=0, keepdim=True)\n",
    "            Q_sa = target_net(state_tp1).gather(dim=0, index=selected_actions)\n",
    "          \n",
    "        # r + gamma * max Q(s',a')\n",
    "        # current_target = reward_t + self.discount * Q_sa\n",
    "        current_target = reward_t.add(Q_sa, alpha=self.discount)\n",
    "\n",
    "      targets = torch.cat((targets, current_target), 0)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "\n",
    "class Epsilon(ABC):\n",
    "  @abstractmethod\n",
    "  def clear(self):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def get_epsilon(self, key):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_step(self, key, data, delta):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_epoch(self, data):\n",
    "    pass\n",
    "  \n",
    "  # @abstractmethod\n",
    "  # def update_at_epsisode():\n",
    "  #   pass\n",
    "\n",
    "\n",
    "class Decay(Epsilon):\n",
    "  # Ref: Decay(0.5, 0.85)\n",
    "  '''\n",
    "  Epsilon Decay EE method with update/decay at epoch\n",
    "  '''\n",
    "  def __init__(self, initial, epoch_decay, step_decay=1.0):\n",
    "    self.initial = initial\n",
    "    self.epoch_decay, self.step_decay = epoch_decay, step_decay\n",
    "    self.epsilon = self.initial\n",
    "    \n",
    "  def clear(self):\n",
    "    self.epsilon = self.initial # should be 4 for origin setting\n",
    "    \n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon\n",
    "  \n",
    "  def update_at_step(self, key, data, delta):\n",
    "    # origin setting\n",
    "    pass\n",
    "    # exponentially\n",
    "    # self.epsilon *= self.step_decay\n",
    "    \n",
    "  def update_at_epoch(self, data):\n",
    "    # origin settings\n",
    "    epoch = data\n",
    "    self.epsilon = 4 / ((epoch + 1) ** (1 / 2))\n",
    "    # exponentially\n",
    "    # self.epsilon *= self.epoch_decay\n",
    "\n",
    "\n",
    "class VDBE(Epsilon):\n",
    "  # VDBE(0.5, 0.01)\n",
    "  def __init__(self, initial, sigma):\n",
    "    self.initial = initial\n",
    "    self.sigma = sigma\n",
    "\n",
    "  def clear(self):\n",
    "    self.epsilon = defaultdict(lambda: self.initial)\n",
    "\n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon[key]\n",
    "  \n",
    "  def update_at_step(self, key, data, delta):\n",
    "    td_error = data\n",
    "    coeff = math.exp(-abs(td_error) / self.sigma)\n",
    "    f = (1.0 - coeff) / (1.0 + coeff)\n",
    "    self.epsilon[key] = delta * f + (1.0 - delta) * self.epsilon[key]\n",
    "  \n",
    "  def update_at_epoch(self, data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "  def __init__(self, structure, exp_replay, epsilon, num_episode, epochs, batch_size, lr, switch_param_threshold, single_reward):\n",
    "    self.eval_net = Net()\n",
    "    self.target_net = Net() if not structure == 'vanilla' else None\n",
    "    self.structure = structure\n",
    "    self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "    self.exp_replay = exp_replay\n",
    "    self.epsilon = epsilon\n",
    "    self.num_episode = num_episode\n",
    "    self.epochs = epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.switch_param_threshold = switch_param_threshold\n",
    "    self.single_reward = single_reward\n",
    "    self.win_list = []\n",
    "    self.score_list = []\n",
    "    self.loss_list = []\n",
    "    self.learn_step_counter = 0\n",
    "\n",
    "  # Environment Methods\n",
    "  def __episodes(self):\n",
    "    # return USER_LIST[:self.num_episode]\n",
    "    return np.random.choice(USER_LIST, self.num_episode, replace=False)\n",
    "  \n",
    "  def __user_episode_context(self):\n",
    "    self.user_all_streams = CONTEXT_REPS.xs(self.asid, level=\"asid\")\n",
    "    self.stream_list = self.user_all_streams.index\n",
    "    self.final_stream = max(self.stream_list)\n",
    "    \n",
    "  def _print_status(self):\n",
    "    if self.rec_cnt % 100 == 0:\n",
    "      self.win_list.append(self.win_cnt)\n",
    "      self.score_list.append(self.score)\n",
    "      self.loss_list.append(self.loss)\n",
    "      print(f'[{len(self.win_list)}] win_cnt: {self.win_cnt} | score: {self.score} | asid: {USER_LIST.index(self.asid)} | loss: {self.loss:.2f} | explore: {self.explore} | exploit: {self.exploit}')\n",
    "      self.win_cnt = 0\n",
    "      self.score = 0\n",
    "      self.loss = 0\n",
    "      self.explore = 0\n",
    "      self.exploit = 0\n",
    "      \n",
    "  def reward(self):\n",
    "    '''\n",
    "    Comparison function for reward, 考慮「所有」歷史購買紀錄\n",
    "    '''\n",
    "    real_bought_ids = BOUGHT_DICT[self.asid]\n",
    "    real_bought_ids_series = gen_exist_series(real_bought_ids, self.stream_items)\n",
    "    \n",
    "    reward_list = [a & b for a, b in zip(real_bought_ids_series, self.action_ids)]\n",
    "\n",
    "    # Reward Count \n",
    "    self.rec_cnt += 1\n",
    "    if sum(reward_list) > 0:\n",
    "      self.win_cnt += 1\n",
    "      self.score += sum(reward_list)\n",
    "    self._print_status()\n",
    "    \n",
    "    if self.single_reward:\n",
    "      return pd.Series(reward_list, index=self.stream_items)\n",
    "    else:\n",
    "      return pd.Series(list(map(lambda x: x * sum(reward_list), reward_list)), index=self.stream_items)\n",
    "\n",
    "  # Agent Methods\n",
    "  def __choose_actions(self):\n",
    "    if np.random.rand() <= self.epsilon.get_epsilon(self.asid):\n",
    "    # if len(self.exp_replay.memory) < 1:\n",
    "      # Explore by randomly select 10/n items from candidate_items\n",
    "      # Get all items from the stream\n",
    "      self.explore += 1\n",
    "      selected_actions = random.sample(self.stream_items, 10) if len(self.stream_items) > 10 else self.stream_items\n",
    "    else:\n",
    "      # Exploit by choosing action from the model's prediction\n",
    "      self.exploit += 1\n",
    "      selected_actions = self.__agent_predict()\n",
    "    x = pd.Series(0, index=self.stream_items)\n",
    "    x.loc[selected_actions] = 1\n",
    "    return x\n",
    "    \n",
    "  def q_value(self): \n",
    "    if type(self.epsilon) == Decay: return 0\n",
    "    with torch.no_grad():\n",
    "      predicts = self.eval_net(self.full_input).flatten()    \n",
    "    actions_idx = np.where(self.action_ids.values == 1)[0]\n",
    "    q_val = predicts[actions_idx].mean()\n",
    "    return q_val\n",
    "\n",
    "  def __agent_predict(self):\n",
    "    with torch.no_grad():\n",
    "      predicts = self.eval_net(self.full_input).flatten()\n",
    "    if len(predicts) > 10:\n",
    "      top10_idx = torch.topk(predicts, 10).indices.cpu()\n",
    "      actions = self.candidate_actions.iloc[top10_idx]['item_id'].values\n",
    "    else:\n",
    "      actions = self.candidate_actions['item_id'].values\n",
    "    return actions\n",
    "\n",
    "  def __train_agent_batch(self, inputs, targets):\n",
    "    self.optimizer.zero_grad()\n",
    "    outputs = self.eval_net(inputs)\n",
    "    loss = self.loss_fn(outputs, targets)\n",
    "    # Add CL Regularization Term\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "  # MAIN TRAIN\n",
    "  def train(self):\n",
    "    self.eval_net.to(DEVICE)\n",
    "    if self.target_net:\n",
    "      self.target_net.to(DEVICE)\n",
    "    self.eval_net.train(True)\n",
    "    self.epsilon.clear()\n",
    "    self.loss = 0.\n",
    "    self.rec_cnt = 0\n",
    "    self.win_cnt = 0\n",
    "    self.explore = 0\n",
    "    self.exploit = 0\n",
    "    self.score = 0\n",
    "\n",
    "    # ------------------- Episode (User) -------------------------------\n",
    "    for asid in tqdm(USER_LIST):\n",
    "      self.asid = asid\n",
    "      self.__user_episode_context()\n",
    "\n",
    "      # ----------------- Runs (User x All_Stream) ---------------------\n",
    "      for i, stream in enumerate(self.stream_list):\n",
    "        game_over = stream == self.final_stream\n",
    "        self.current_stream = stream\n",
    "        self.current_state = self.user_all_streams.loc[stream]\n",
    "        self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
    "        self.full_input, self.candidate_actions = get_input_tensor(self.current_state, self.current_stream, with_tensor=True)\n",
    "\n",
    "        # --------------- Explore/Exploit Section ----------------------\n",
    "        self.action_ids = self.__choose_actions()\n",
    "\n",
    "        # --------------- Get next state & info to store ---------------\n",
    "        reward = self.reward()\n",
    "        next_state = self.user_all_streams.loc[self.stream_list[i + 1]] if not game_over else []\n",
    "        next_stream = None if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
    "        self.exp_replay.remember([[stream, next_stream], self.current_state, self.action_ids, reward, next_state], game_over)\n",
    "        self.learn_step_counter += 1\n",
    "        if self.target_net and (self.learn_step_counter % self.switch_param_threshold == 0):\n",
    "          self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "\n",
    "\n",
    "        # --------------- Load batch of experiences --------------------\n",
    "        inputs, targets = self.exp_replay.get_batch(self.eval_net, self.target_net, self.structure, batch_size=self.batch_size)\n",
    "        inputs = df_to_tensor(inputs)\n",
    "        # store pre-training value for td_error\n",
    "        old_Q = self.q_value()\n",
    "        batch_loss = self.__train_agent_batch(inputs, targets)\n",
    "        # store post-training value for td_error\n",
    "        new_Q = self.q_value()\n",
    "        self.loss += batch_loss\n",
    "\n",
    "        # --------------- Update with TD error -------------------------\n",
    "        self.epsilon.update_at_step(self.asid, (new_Q - old_Q), len(self.stream_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 50  # Number of experiences we use for training per batch\n",
    "EPOCH = range(100)\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 1.0e-4\n",
    "SWITCH_PARAM_THRESHOLD = 100\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(380, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, 128)\n",
    "    self.fc4 = nn.Linear(128, 64)\n",
    "    self.fc5 = nn.Linear(64, 1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc4(x)\n",
    "    x = self.tanh(x)\n",
    "    x = self.fc5(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp: Reward Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14672052ef674619b88a97641692c68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] win_cnt: 56 | score: 102 | asid: 5 | loss: 2.48 | explore: 45 | exploit: 55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ebb898a005e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mEPS_baseline_rsingle_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vanilla'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPISODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSWITCH_PARAM_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mEPS_baseline_rsingle_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-e15b241684e4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# --------------- Load batch of experiences --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# store pre-training value for td_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-34fb434da724>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, eval_net, target_net, structure, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mcurrent_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mstate_tp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_net\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0d98a1c59d51>\u001b[0m in \u001b[0;36mget_input_tensor\u001b[0;34m(input_state, current_stream, with_tensor)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# Fill in other context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mstream_item_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# Merge with items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         return _list_of_series_to_arrays(\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         )\n\u001b[1;32m    533\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_of_series_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0maligned_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     func = _get_take_nd_function(\n\u001b[0;32m-> 1735\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m     )\n\u001b[1;32m   1737\u001b[0m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m_get_take_nd_function\u001b[0;34m(ndim, arr_dtype, out_dtype, axis, mask_info)\u001b[0m\n\u001b[1;32m   1513\u001b[0m ):\n\u001b[1;32m   1514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_take_1d_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;31m# historically, void subclasses preserve their name, eg `record64`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "EPS_baseline_rsingle_x\n",
    "\n",
    "- VDBE(0.5, 0.01)\n",
    "- Decay(0.5, 0.85)\n",
    "'''\n",
    "\n",
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)\n",
    "epsilon = Decay(0.5, 0.85)\n",
    "EPS_baseline_rsingle_x = DQN('vanilla', exp_replay, epsilon, NUM_EPISODE, EPOCH, BATCH_SIZE, LR, SWITCH_PARAM_THRESHOLD, single_reward=True)\n",
    "EPS_baseline_rsingle_x.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Models/EPS_baseline_rsingle_x.pkl', 'wb') as file_pi:\n",
    "  pickle.dump(EPS_baseline_rsingle_x, file_pi, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_baseline_rsum_x = pd.read_pickle('../Models/EPS_baseline_rsum_x.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd100280b38>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(EPS_baseline_rsingle_x.score_list[:215], label='VDBE hit@10')\n",
    "# plt.plot(EPS_baseline_rsum_x.score_list[:215], label='VDBE hit@10')\n",
    "# plt.plot(baseline100, label='baseline hit@10')\n",
    "plt.plot(pd.Series(EPS_baseline_rsingle_x.score_list[:215]).rolling(110).mean(), label='single reward')\n",
    "plt.plot(pd.Series(EPS_baseline_rsum_x.score_list[:215]).rolling(110).mean(), label='sum reward')\n",
    "# plt.plot(pd.Series(baseline100).rolling(30).mean(), label='Baseline hit@10 ma 30')\n",
    "plt.xlabel('100 step')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score for every 100 steps')\n",
    "# plt.ylim([0.4, 0.6])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Experiment/vdbe_100_max_target.pkl', 'wb') as file_pi:\n",
    "  pickle.dump(res, file_pi, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5406706451835664"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.c_hist[-1]/sum(dqn.rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n請不要關掉這ㄍ分頁 乾蝦哈咪搭\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "請不要關掉這ㄍ分頁 乾蝦哈咪搭\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "with open('../Models/vdbe_100_target.pkl', 'wb') as file_pi:\n",
    "  pickle.dump(dqn, file_pi, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 0.9990258147229674\n"
     ]
    }
   ],
   "source": [
    "origin = 0.99\n",
    "for i in range(68334):\n",
    "  origin = 4 / ((i + 1) ** (1 / 4))\n",
    "  if origin < 1:\n",
    "    print(i, origin)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_REPS = pd.read_pickle('../../data/CONTEXT_REPS_CLEAN_ASID.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipment_0</th>\n",
       "      <th>shipment_1</th>\n",
       "      <th>shipment_2</th>\n",
       "      <th>shipment_3</th>\n",
       "      <th>shipment_4</th>\n",
       "      <th>shipment_5</th>\n",
       "      <th>shipment_6</th>\n",
       "      <th>payment_0</th>\n",
       "      <th>payment_1</th>\n",
       "      <th>payment_2</th>\n",
       "      <th>...</th>\n",
       "      <th>s62</th>\n",
       "      <th>s63</th>\n",
       "      <th>s64</th>\n",
       "      <th>s65</th>\n",
       "      <th>s66</th>\n",
       "      <th>s67</th>\n",
       "      <th>s68</th>\n",
       "      <th>s69</th>\n",
       "      <th>s70</th>\n",
       "      <th>s71</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132498</td>\n",
       "      <td>-0.052705</td>\n",
       "      <td>-0.059136</td>\n",
       "      <td>0.041836</td>\n",
       "      <td>0.064853</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>-0.002886</td>\n",
       "      <td>-0.051057</td>\n",
       "      <td>0.035917</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013412</td>\n",
       "      <td>0.063558</td>\n",
       "      <td>-0.048068</td>\n",
       "      <td>0.040219</td>\n",
       "      <td>0.045013</td>\n",
       "      <td>0.027529</td>\n",
       "      <td>0.072258</td>\n",
       "      <td>-0.095085</td>\n",
       "      <td>0.094784</td>\n",
       "      <td>0.060942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127052</td>\n",
       "      <td>-0.020923</td>\n",
       "      <td>0.040821</td>\n",
       "      <td>0.069121</td>\n",
       "      <td>0.059461</td>\n",
       "      <td>-0.001729</td>\n",
       "      <td>-0.009453</td>\n",
       "      <td>-0.144311</td>\n",
       "      <td>0.036030</td>\n",
       "      <td>-0.033306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139076</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.052675</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.069309</td>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>-0.084407</td>\n",
       "      <td>0.037224</td>\n",
       "      <td>0.093305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094447</td>\n",
       "      <td>-0.079715</td>\n",
       "      <td>0.027152</td>\n",
       "      <td>-0.070138</td>\n",
       "      <td>-0.017507</td>\n",
       "      <td>0.095076</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>-0.060667</td>\n",
       "      <td>-0.007425</td>\n",
       "      <td>0.063574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126453</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>-0.011018</td>\n",
       "      <td>-0.011735</td>\n",
       "      <td>0.072177</td>\n",
       "      <td>0.071742</td>\n",
       "      <td>0.038208</td>\n",
       "      <td>-0.139593</td>\n",
       "      <td>0.070405</td>\n",
       "      <td>0.017937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043089</td>\n",
       "      <td>0.014795</td>\n",
       "      <td>-0.006246</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.006497</td>\n",
       "      <td>0.025628</td>\n",
       "      <td>0.070256</td>\n",
       "      <td>-0.151250</td>\n",
       "      <td>0.118813</td>\n",
       "      <td>-0.030756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057637</td>\n",
       "      <td>-0.115033</td>\n",
       "      <td>-0.014697</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>-0.065821</td>\n",
       "      <td>0.031127</td>\n",
       "      <td>-0.029407</td>\n",
       "      <td>-0.041986</td>\n",
       "      <td>0.064447</td>\n",
       "      <td>0.038614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121436</td>\n",
       "      <td>-0.081162</td>\n",
       "      <td>0.012896</td>\n",
       "      <td>-0.006772</td>\n",
       "      <td>0.102165</td>\n",
       "      <td>0.081686</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>-0.083960</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>-0.007319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066127</td>\n",
       "      <td>-0.026321</td>\n",
       "      <td>-0.090138</td>\n",
       "      <td>-0.023947</td>\n",
       "      <td>-0.042660</td>\n",
       "      <td>0.058371</td>\n",
       "      <td>-0.053976</td>\n",
       "      <td>-0.064433</td>\n",
       "      <td>0.016361</td>\n",
       "      <td>-0.058150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069888</td>\n",
       "      <td>-0.010498</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>0.029052</td>\n",
       "      <td>-0.010239</td>\n",
       "      <td>0.041776</td>\n",
       "      <td>0.027888</td>\n",
       "      <td>-0.073415</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>-0.014582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129827</td>\n",
       "      <td>-0.066502</td>\n",
       "      <td>-0.068940</td>\n",
       "      <td>-0.067753</td>\n",
       "      <td>0.019688</td>\n",
       "      <td>0.063812</td>\n",
       "      <td>-0.024990</td>\n",
       "      <td>-0.096472</td>\n",
       "      <td>0.024010</td>\n",
       "      <td>0.020763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031262</td>\n",
       "      <td>-0.003622</td>\n",
       "      <td>-0.062835</td>\n",
       "      <td>-0.006238</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>-0.054488</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>-0.070517</td>\n",
       "      <td>-0.008876</td>\n",
       "      <td>-0.054611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034600</td>\n",
       "      <td>-0.017016</td>\n",
       "      <td>-0.011535</td>\n",
       "      <td>0.072153</td>\n",
       "      <td>-0.027633</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>-0.034807</td>\n",
       "      <td>0.011942</td>\n",
       "      <td>-0.042104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102788</td>\n",
       "      <td>-0.049090</td>\n",
       "      <td>-0.010447</td>\n",
       "      <td>0.085219</td>\n",
       "      <td>0.079645</td>\n",
       "      <td>0.081952</td>\n",
       "      <td>-0.012910</td>\n",
       "      <td>-0.016796</td>\n",
       "      <td>0.095188</td>\n",
       "      <td>-0.032967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2711</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031068</td>\n",
       "      <td>-0.048208</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.055133</td>\n",
       "      <td>0.088089</td>\n",
       "      <td>-0.006557</td>\n",
       "      <td>0.053668</td>\n",
       "      <td>-0.054465</td>\n",
       "      <td>0.047081</td>\n",
       "      <td>-0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048857</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>-0.002106</td>\n",
       "      <td>0.109374</td>\n",
       "      <td>0.064582</td>\n",
       "      <td>-0.066092</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>-0.063603</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>0.018016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041618</td>\n",
       "      <td>-0.033293</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>-0.118940</td>\n",
       "      <td>0.076775</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>-0.054030</td>\n",
       "      <td>-0.037968</td>\n",
       "      <td>-0.046052</td>\n",
       "      <td>0.026062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105968</td>\n",
       "      <td>-0.073180</td>\n",
       "      <td>-0.008095</td>\n",
       "      <td>-0.019889</td>\n",
       "      <td>0.045591</td>\n",
       "      <td>0.097201</td>\n",
       "      <td>0.077644</td>\n",
       "      <td>-0.072554</td>\n",
       "      <td>0.064139</td>\n",
       "      <td>0.018985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051650</td>\n",
       "      <td>0.016138</td>\n",
       "      <td>-0.081634</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.036047</td>\n",
       "      <td>-0.076927</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>0.031412</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>-0.059531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118906</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.038006</td>\n",
       "      <td>-0.033654</td>\n",
       "      <td>0.070978</td>\n",
       "      <td>0.077241</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>-0.062246</td>\n",
       "      <td>0.114088</td>\n",
       "      <td>0.072084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037251</td>\n",
       "      <td>0.007950</td>\n",
       "      <td>0.050301</td>\n",
       "      <td>0.073348</td>\n",
       "      <td>0.014395</td>\n",
       "      <td>-0.006544</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>-0.020904</td>\n",
       "      <td>0.046826</td>\n",
       "      <td>0.016414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021846</td>\n",
       "      <td>0.055454</td>\n",
       "      <td>-0.052896</td>\n",
       "      <td>-0.056571</td>\n",
       "      <td>0.027017</td>\n",
       "      <td>-0.056567</td>\n",
       "      <td>0.050297</td>\n",
       "      <td>-0.046061</td>\n",
       "      <td>0.111124</td>\n",
       "      <td>-0.038801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019337</td>\n",
       "      <td>-0.008645</td>\n",
       "      <td>-0.090672</td>\n",
       "      <td>-0.060802</td>\n",
       "      <td>0.097688</td>\n",
       "      <td>-0.024699</td>\n",
       "      <td>0.056864</td>\n",
       "      <td>0.072767</td>\n",
       "      <td>0.052020</td>\n",
       "      <td>0.045198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075855</td>\n",
       "      <td>0.072335</td>\n",
       "      <td>0.029784</td>\n",
       "      <td>-0.131294</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>-0.047460</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>-0.094352</td>\n",
       "      <td>-0.013668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065646</td>\n",
       "      <td>0.030532</td>\n",
       "      <td>-0.005461</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>-0.046433</td>\n",
       "      <td>-0.002795</td>\n",
       "      <td>0.038645</td>\n",
       "      <td>-0.019324</td>\n",
       "      <td>0.103891</td>\n",
       "      <td>0.053782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055488</td>\n",
       "      <td>-0.037748</td>\n",
       "      <td>0.024814</td>\n",
       "      <td>-0.012822</td>\n",
       "      <td>0.034961</td>\n",
       "      <td>-0.058659</td>\n",
       "      <td>-0.035921</td>\n",
       "      <td>-0.010057</td>\n",
       "      <td>0.007774</td>\n",
       "      <td>-0.019779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060068</td>\n",
       "      <td>0.025311</td>\n",
       "      <td>0.020046</td>\n",
       "      <td>-0.056578</td>\n",
       "      <td>0.102103</td>\n",
       "      <td>-0.046003</td>\n",
       "      <td>0.055276</td>\n",
       "      <td>-0.059223</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>0.003788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023211</td>\n",
       "      <td>0.050932</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.016905</td>\n",
       "      <td>0.031677</td>\n",
       "      <td>-0.032469</td>\n",
       "      <td>-0.038902</td>\n",
       "      <td>0.028886</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>0.013845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016673</td>\n",
       "      <td>0.026203</td>\n",
       "      <td>0.052309</td>\n",
       "      <td>-0.002054</td>\n",
       "      <td>0.033831</td>\n",
       "      <td>0.007159</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>-0.023932</td>\n",
       "      <td>-0.032117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041947</td>\n",
       "      <td>-0.003612</td>\n",
       "      <td>0.013755</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>-0.119486</td>\n",
       "      <td>0.014275</td>\n",
       "      <td>0.063669</td>\n",
       "      <td>-0.068627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054756</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>0.037002</td>\n",
       "      <td>0.010550</td>\n",
       "      <td>0.025145</td>\n",
       "      <td>-0.077136</td>\n",
       "      <td>-0.005064</td>\n",
       "      <td>-0.025120</td>\n",
       "      <td>0.145761</td>\n",
       "      <td>0.054094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012714</td>\n",
       "      <td>-0.066666</td>\n",
       "      <td>-0.051571</td>\n",
       "      <td>0.042622</td>\n",
       "      <td>0.021236</td>\n",
       "      <td>0.087924</td>\n",
       "      <td>-0.016008</td>\n",
       "      <td>-0.013221</td>\n",
       "      <td>0.034679</td>\n",
       "      <td>-0.003116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052274</td>\n",
       "      <td>-0.015634</td>\n",
       "      <td>0.025075</td>\n",
       "      <td>0.047349</td>\n",
       "      <td>-0.013891</td>\n",
       "      <td>0.017031</td>\n",
       "      <td>0.035921</td>\n",
       "      <td>-0.000741</td>\n",
       "      <td>0.027946</td>\n",
       "      <td>0.006445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4583</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126370</td>\n",
       "      <td>-0.032085</td>\n",
       "      <td>0.100587</td>\n",
       "      <td>0.054125</td>\n",
       "      <td>0.047933</td>\n",
       "      <td>0.045383</td>\n",
       "      <td>0.096770</td>\n",
       "      <td>-0.056938</td>\n",
       "      <td>0.086937</td>\n",
       "      <td>0.016043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072825</td>\n",
       "      <td>-0.074017</td>\n",
       "      <td>-0.113991</td>\n",
       "      <td>0.014532</td>\n",
       "      <td>-0.009711</td>\n",
       "      <td>0.072430</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.043775</td>\n",
       "      <td>0.074102</td>\n",
       "      <td>-0.027172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089663</td>\n",
       "      <td>0.071181</td>\n",
       "      <td>0.026253</td>\n",
       "      <td>0.093594</td>\n",
       "      <td>0.107234</td>\n",
       "      <td>0.071475</td>\n",
       "      <td>0.033695</td>\n",
       "      <td>-0.011588</td>\n",
       "      <td>0.025617</td>\n",
       "      <td>0.021216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4872</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>-0.081436</td>\n",
       "      <td>-0.124314</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>-0.005669</td>\n",
       "      <td>-0.054075</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.087492</td>\n",
       "      <td>-0.021448</td>\n",
       "      <td>0.017235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038339</td>\n",
       "      <td>-0.096655</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>0.076628</td>\n",
       "      <td>-0.030555</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>-0.037153</td>\n",
       "      <td>-0.007623</td>\n",
       "      <td>0.046954</td>\n",
       "      <td>0.007124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5162</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>-0.101694</td>\n",
       "      <td>-0.004092</td>\n",
       "      <td>0.057638</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>-0.190448</td>\n",
       "      <td>0.098920</td>\n",
       "      <td>0.057716</td>\n",
       "      <td>-0.070508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5163</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018872</td>\n",
       "      <td>0.041743</td>\n",
       "      <td>0.020902</td>\n",
       "      <td>-0.035777</td>\n",
       "      <td>0.110242</td>\n",
       "      <td>-0.059897</td>\n",
       "      <td>0.037792</td>\n",
       "      <td>-0.007310</td>\n",
       "      <td>0.045669</td>\n",
       "      <td>0.017153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064789</td>\n",
       "      <td>-0.063090</td>\n",
       "      <td>-0.014706</td>\n",
       "      <td>0.102986</td>\n",
       "      <td>-0.023453</td>\n",
       "      <td>0.038916</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.106118</td>\n",
       "      <td>0.073957</td>\n",
       "      <td>-0.123016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030392</td>\n",
       "      <td>0.056825</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.031254</td>\n",
       "      <td>0.154370</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>-0.087815</td>\n",
       "      <td>-0.044427</td>\n",
       "      <td>-0.053208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108903</td>\n",
       "      <td>-0.004241</td>\n",
       "      <td>0.014057</td>\n",
       "      <td>0.071285</td>\n",
       "      <td>-0.023773</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>0.029487</td>\n",
       "      <td>-0.028742</td>\n",
       "      <td>0.032717</td>\n",
       "      <td>0.009895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5289</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056316</td>\n",
       "      <td>0.042210</td>\n",
       "      <td>-0.021544</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.042698</td>\n",
       "      <td>-0.150478</td>\n",
       "      <td>-0.135634</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.106180</td>\n",
       "      <td>0.063239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055257</td>\n",
       "      <td>0.087241</td>\n",
       "      <td>-0.077434</td>\n",
       "      <td>0.054634</td>\n",
       "      <td>0.060940</td>\n",
       "      <td>-0.029755</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.040686</td>\n",
       "      <td>-0.047657</td>\n",
       "      <td>0.038726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5438</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055906</td>\n",
       "      <td>0.007237</td>\n",
       "      <td>-0.054178</td>\n",
       "      <td>-0.044025</td>\n",
       "      <td>0.069747</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>-0.001784</td>\n",
       "      <td>-0.042071</td>\n",
       "      <td>0.044289</td>\n",
       "      <td>0.011804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097811</td>\n",
       "      <td>-0.029467</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.054064</td>\n",
       "      <td>-0.087808</td>\n",
       "      <td>0.083927</td>\n",
       "      <td>-0.016585</td>\n",
       "      <td>0.023215</td>\n",
       "      <td>0.020191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052661</td>\n",
       "      <td>-0.010479</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.081952</td>\n",
       "      <td>0.036594</td>\n",
       "      <td>0.053331</td>\n",
       "      <td>0.054781</td>\n",
       "      <td>-0.005305</td>\n",
       "      <td>-0.064634</td>\n",
       "      <td>0.061770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014729</td>\n",
       "      <td>-0.016360</td>\n",
       "      <td>-0.061178</td>\n",
       "      <td>-0.058050</td>\n",
       "      <td>0.063393</td>\n",
       "      <td>-0.011449</td>\n",
       "      <td>-0.052955</td>\n",
       "      <td>-0.007177</td>\n",
       "      <td>0.022947</td>\n",
       "      <td>-0.047732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127247</td>\n",
       "      <td>-0.020577</td>\n",
       "      <td>-0.043027</td>\n",
       "      <td>0.025737</td>\n",
       "      <td>0.077444</td>\n",
       "      <td>-0.103440</td>\n",
       "      <td>0.025439</td>\n",
       "      <td>0.081876</td>\n",
       "      <td>0.103826</td>\n",
       "      <td>0.039167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5677</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060116</td>\n",
       "      <td>0.062517</td>\n",
       "      <td>-0.050902</td>\n",
       "      <td>0.141311</td>\n",
       "      <td>0.072671</td>\n",
       "      <td>0.082617</td>\n",
       "      <td>-0.065164</td>\n",
       "      <td>-0.015678</td>\n",
       "      <td>-0.036398</td>\n",
       "      <td>-0.055054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5738</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035454</td>\n",
       "      <td>-0.023045</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>0.050375</td>\n",
       "      <td>0.044121</td>\n",
       "      <td>-0.051482</td>\n",
       "      <td>-0.005897</td>\n",
       "      <td>-0.004090</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.006042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062297</td>\n",
       "      <td>0.062208</td>\n",
       "      <td>-0.103403</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.133625</td>\n",
       "      <td>0.040745</td>\n",
       "      <td>0.073201</td>\n",
       "      <td>-0.061358</td>\n",
       "      <td>0.023729</td>\n",
       "      <td>-0.013348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        shipment_0  shipment_1  shipment_2  shipment_3  shipment_4  \\\n",
       "stream                                                               \n",
       "9              0.0         0.0         0.0         1.0         0.0   \n",
       "170            0.0         0.0         3.0         0.0         0.0   \n",
       "259            0.0         0.0         0.0         1.0         0.0   \n",
       "530            0.0         0.0         0.0         3.0         0.0   \n",
       "854            0.0         0.0         0.0         5.0         0.0   \n",
       "1116           0.0         0.0         0.0         1.0         0.0   \n",
       "1249           0.0         0.0         0.0         6.0         0.0   \n",
       "1741           0.0         0.0         0.0         6.0         0.0   \n",
       "1885           0.0         0.0         0.0         6.0         0.0   \n",
       "1944           0.0         0.0         0.0         2.0         0.0   \n",
       "2216           0.0         0.0         0.0         4.0         0.0   \n",
       "2513           0.0         0.0         0.0         3.0         0.0   \n",
       "2568           0.0         0.0         0.0         4.0         0.0   \n",
       "2639           0.0         0.0         0.0         2.0         0.0   \n",
       "2655           0.0         0.0         0.0         3.0         0.0   \n",
       "2711           0.0         0.0         0.0         2.0         0.0   \n",
       "2861           0.0         0.0         0.0         3.0         0.0   \n",
       "2922           0.0         0.0         0.0         4.0         0.0   \n",
       "3017           0.0         0.0         0.0         2.0         0.0   \n",
       "3281           0.0         0.0         0.0         3.0         0.0   \n",
       "3309           0.0         0.0         0.0         2.0         0.0   \n",
       "3385           0.0         0.0         0.0         1.0         0.0   \n",
       "3544           0.0         0.0         0.0         1.0         0.0   \n",
       "3620           0.0         0.0         0.0         2.0         0.0   \n",
       "3673           0.0         0.0         0.0         2.0         0.0   \n",
       "3780           0.0         0.0         0.0         7.0         0.0   \n",
       "3848           0.0         0.0         0.0         1.0         0.0   \n",
       "3877           0.0         0.0         0.0         1.0         0.0   \n",
       "3979           0.0         0.0         0.0         4.0         0.0   \n",
       "4106           0.0         0.0         0.0         4.0         0.0   \n",
       "4247           0.0         0.0         0.0         4.0         0.0   \n",
       "4353           0.0         0.0         0.0         4.0         0.0   \n",
       "4456           0.0         0.0         0.0         5.0         0.0   \n",
       "4519           0.0         0.0         0.0         2.0         0.0   \n",
       "4583           0.0         0.0         0.0         3.0         0.0   \n",
       "4776           0.0         0.0         0.0         0.0         0.0   \n",
       "4824           0.0         0.0         0.0         2.0         0.0   \n",
       "4872           0.0         0.0         0.0         1.0         0.0   \n",
       "4988           0.0         0.0         0.0         2.0         0.0   \n",
       "5162           0.0         0.0         0.0         6.0         0.0   \n",
       "5163           0.0         0.0         2.0         0.0         0.0   \n",
       "5178           0.0         0.0         0.0         5.0         0.0   \n",
       "5212           0.0         0.0         0.0         2.0         0.0   \n",
       "5258           0.0         0.0         0.0         1.0         0.0   \n",
       "5289           0.0         0.0         0.0         7.0         0.0   \n",
       "5310           0.0         0.0         2.0         0.0         0.0   \n",
       "5438           0.0         0.0         0.0         3.0         0.0   \n",
       "5473           0.0         0.0         0.0         6.0         0.0   \n",
       "5506           0.0         0.0         0.0         2.0         0.0   \n",
       "5586           0.0         0.0         0.0         2.0         0.0   \n",
       "5630           0.0         0.0         0.0         8.0         0.0   \n",
       "5677           0.0         0.0         0.0         2.0         0.0   \n",
       "5738           0.0         0.0         0.0         5.0         0.0   \n",
       "5799           0.0         0.0         0.0         4.0         0.0   \n",
       "\n",
       "        shipment_5  shipment_6  payment_0  payment_1  payment_2  ...  \\\n",
       "stream                                                           ...   \n",
       "9              0.0         0.0        0.0        0.0        0.0  ...   \n",
       "170            0.0         0.0        0.0        0.0        0.0  ...   \n",
       "259            0.0         0.0        0.0        0.0        0.0  ...   \n",
       "530            0.0         0.0        0.0        0.0        0.0  ...   \n",
       "854            0.0         0.0        0.0        0.0        0.0  ...   \n",
       "1116           0.0         0.0        0.0        1.0        0.0  ...   \n",
       "1249           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "1741           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "1885           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "1944           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "2216           0.0         0.0        0.0        2.0        0.0  ...   \n",
       "2513           0.0         0.0        0.0        1.0        0.0  ...   \n",
       "2568           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "2639           0.0         0.0        0.0        2.0        0.0  ...   \n",
       "2655           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "2711           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "2861           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "2922           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "3017           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "3281           0.0         0.0        0.0        3.0        0.0  ...   \n",
       "3309           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "3385           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "3544           0.0         0.0        0.0        0.0        1.0  ...   \n",
       "3620           0.0         0.0        0.0        0.0        2.0  ...   \n",
       "3673           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "3780           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "3848           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "3877           0.0         0.0        0.0        1.0        0.0  ...   \n",
       "3979           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4106           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4247           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4353           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4456           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4519           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4583           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4776           0.0         2.0        0.0        2.0        0.0  ...   \n",
       "4824           0.0         3.0        0.0        3.0        0.0  ...   \n",
       "4872           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "4988           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5162           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5163           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5178           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5212           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5258           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5289           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5310           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5438           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5473           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5506           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5586           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5630           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5677           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5738           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "5799           0.0         0.0        0.0        0.0        0.0  ...   \n",
       "\n",
       "             s62       s63       s64       s65       s66       s67       s68  \\\n",
       "stream                                                                         \n",
       "9      -0.132498 -0.052705 -0.059136  0.041836  0.064853  0.021252 -0.002886   \n",
       "170     0.013412  0.063558 -0.048068  0.040219  0.045013  0.027529  0.072258   \n",
       "259    -0.127052 -0.020923  0.040821  0.069121  0.059461 -0.001729 -0.009453   \n",
       "530    -0.139076  0.005469  0.052675  0.042398  0.069309  0.016650  0.002554   \n",
       "854    -0.094447 -0.079715  0.027152 -0.070138 -0.017507  0.095076  0.009913   \n",
       "1116   -0.126453  0.003925 -0.011018 -0.011735  0.072177  0.071742  0.038208   \n",
       "1249   -0.043089  0.014795 -0.006246  0.022013  0.006497  0.025628  0.070256   \n",
       "1741   -0.057637 -0.115033 -0.014697  0.006761 -0.065821  0.031127 -0.029407   \n",
       "1885   -0.121436 -0.081162  0.012896 -0.006772  0.102165  0.081686  0.059365   \n",
       "1944   -0.066127 -0.026321 -0.090138 -0.023947 -0.042660  0.058371 -0.053976   \n",
       "2216   -0.069888 -0.010498  0.009593  0.029052 -0.010239  0.041776  0.027888   \n",
       "2513   -0.129827 -0.066502 -0.068940 -0.067753  0.019688  0.063812 -0.024990   \n",
       "2568   -0.031262 -0.003622 -0.062835 -0.006238  0.032529 -0.054488  0.028125   \n",
       "2639   -0.034600 -0.017016 -0.011535  0.072153 -0.027633  0.009783  0.001389   \n",
       "2655   -0.102788 -0.049090 -0.010447  0.085219  0.079645  0.081952 -0.012910   \n",
       "2711    0.031068 -0.048208  0.000263  0.055133  0.088089 -0.006557  0.053668   \n",
       "2861   -0.048857  0.012578 -0.002106  0.109374  0.064582 -0.066092  0.005174   \n",
       "2922   -0.041618 -0.033293  0.044229 -0.118940  0.076775  0.007362 -0.054030   \n",
       "3017   -0.105968 -0.073180 -0.008095 -0.019889  0.045591  0.097201  0.077644   \n",
       "3281   -0.051650  0.016138 -0.081634  0.003746  0.036047 -0.076927 -0.001444   \n",
       "3309   -0.118906  0.004228  0.038006 -0.033654  0.070978  0.077241  0.022986   \n",
       "3385    0.037251  0.007950  0.050301  0.073348  0.014395 -0.006544  0.000478   \n",
       "3544    0.021846  0.055454 -0.052896 -0.056571  0.027017 -0.056567  0.050297   \n",
       "3620   -0.019337 -0.008645 -0.090672 -0.060802  0.097688 -0.024699  0.056864   \n",
       "3673   -0.075855  0.072335  0.029784 -0.131294  0.013778 -0.047460  0.014797   \n",
       "3780   -0.065646  0.030532 -0.005461  0.002382 -0.046433 -0.002795  0.038645   \n",
       "3848    0.055488 -0.037748  0.024814 -0.012822  0.034961 -0.058659 -0.035921   \n",
       "3877   -0.060068  0.025311  0.020046 -0.056578  0.102103 -0.046003  0.055276   \n",
       "3979   -0.023211  0.050932  0.002857  0.016905  0.031677 -0.032469 -0.038902   \n",
       "4106    0.016673  0.026203  0.052309 -0.002054  0.033831  0.007159 -0.000595   \n",
       "4247   -0.041947 -0.003612  0.013755  0.008952  0.011776  0.003189 -0.119486   \n",
       "4353    0.054756  0.009007  0.037002  0.010550  0.025145 -0.077136 -0.005064   \n",
       "4456   -0.012714 -0.066666 -0.051571  0.042622  0.021236  0.087924 -0.016008   \n",
       "4519   -0.052274 -0.015634  0.025075  0.047349 -0.013891  0.017031  0.035921   \n",
       "4583   -0.126370 -0.032085  0.100587  0.054125  0.047933  0.045383  0.096770   \n",
       "4776   -0.072825 -0.074017 -0.113991  0.014532 -0.009711  0.072430  0.001757   \n",
       "4824   -0.089663  0.071181  0.026253  0.093594  0.107234  0.071475  0.033695   \n",
       "4872    0.012251 -0.081436 -0.124314  0.002920 -0.005669 -0.054075  0.012906   \n",
       "4988   -0.038339 -0.096655  0.005064  0.076628 -0.030555  0.007653 -0.037153   \n",
       "5162   -0.026926  0.030952 -0.101694 -0.004092  0.057638  0.011523 -0.190448   \n",
       "5163    0.018872  0.041743  0.020902 -0.035777  0.110242 -0.059897  0.037792   \n",
       "5178    0.064789 -0.063090 -0.014706  0.102986 -0.023453  0.038916  0.014512   \n",
       "5212    0.030392  0.056825  0.015451  0.031254  0.154370  0.001493  0.014820   \n",
       "5258   -0.108903 -0.004241  0.014057  0.071285 -0.023773  0.021305  0.029487   \n",
       "5289   -0.056316  0.042210 -0.021544  0.005804  0.042698 -0.150478 -0.135634   \n",
       "5310   -0.055257  0.087241 -0.077434  0.054634  0.060940 -0.029755  0.003816   \n",
       "5438    0.055906  0.007237 -0.054178 -0.044025  0.069747  0.002263 -0.001784   \n",
       "5473    0.097811 -0.029467  0.005274  0.000545  0.054064 -0.087808  0.083927   \n",
       "5506   -0.052661 -0.010479  0.023250  0.081952  0.036594  0.053331  0.054781   \n",
       "5586   -0.014729 -0.016360 -0.061178 -0.058050  0.063393 -0.011449 -0.052955   \n",
       "5630    0.127247 -0.020577 -0.043027  0.025737  0.077444 -0.103440  0.025439   \n",
       "5677   -0.060116  0.062517 -0.050902  0.141311  0.072671  0.082617 -0.065164   \n",
       "5738    0.035454 -0.023045  0.011908  0.050375  0.044121 -0.051482 -0.005897   \n",
       "5799   -0.062297  0.062208 -0.103403  0.014851  0.133625  0.040745  0.073201   \n",
       "\n",
       "             s69       s70       s71  \n",
       "stream                                \n",
       "9      -0.051057  0.035917  0.002849  \n",
       "170    -0.095085  0.094784  0.060942  \n",
       "259    -0.144311  0.036030 -0.033306  \n",
       "530    -0.084407  0.037224  0.093305  \n",
       "854    -0.060667 -0.007425  0.063574  \n",
       "1116   -0.139593  0.070405  0.017937  \n",
       "1249   -0.151250  0.118813 -0.030756  \n",
       "1741   -0.041986  0.064447  0.038614  \n",
       "1885   -0.083960  0.012134 -0.007319  \n",
       "1944   -0.064433  0.016361 -0.058150  \n",
       "2216   -0.073415  0.002894 -0.014582  \n",
       "2513   -0.096472  0.024010  0.020763  \n",
       "2568   -0.070517 -0.008876 -0.054611  \n",
       "2639   -0.034807  0.011942 -0.042104  \n",
       "2655   -0.016796  0.095188 -0.032967  \n",
       "2711   -0.054465  0.047081 -0.000724  \n",
       "2861   -0.063603  0.016695  0.018016  \n",
       "2922   -0.037968 -0.046052  0.026062  \n",
       "3017   -0.072554  0.064139  0.018985  \n",
       "3281    0.031412  0.141242 -0.059531  \n",
       "3309   -0.062246  0.114088  0.072084  \n",
       "3385   -0.020904  0.046826  0.016414  \n",
       "3544   -0.046061  0.111124 -0.038801  \n",
       "3620    0.072767  0.052020  0.045198  \n",
       "3673    0.003006 -0.094352 -0.013668  \n",
       "3780   -0.019324  0.103891  0.053782  \n",
       "3848   -0.010057  0.007774 -0.019779  \n",
       "3877   -0.059223  0.039880  0.003788  \n",
       "3979    0.028886  0.022478  0.013845  \n",
       "4106   -0.000343 -0.023932 -0.032117  \n",
       "4247    0.014275  0.063669 -0.068627  \n",
       "4353   -0.025120  0.145761  0.054094  \n",
       "4456   -0.013221  0.034679 -0.003116  \n",
       "4519   -0.000741  0.027946  0.006445  \n",
       "4583   -0.056938  0.086937  0.016043  \n",
       "4776    0.043775  0.074102 -0.027172  \n",
       "4824   -0.011588  0.025617  0.021216  \n",
       "4872    0.087492 -0.021448  0.017235  \n",
       "4988   -0.007623  0.046954  0.007124  \n",
       "5162    0.098920  0.057716 -0.070508  \n",
       "5163   -0.007310  0.045669  0.017153  \n",
       "5178    0.106118  0.073957 -0.123016  \n",
       "5212   -0.087815 -0.044427 -0.053208  \n",
       "5258   -0.028742  0.032717  0.009895  \n",
       "5289    0.001405  0.106180  0.063239  \n",
       "5310    0.040686 -0.047657  0.038726  \n",
       "5438   -0.042071  0.044289  0.011804  \n",
       "5473   -0.016585  0.023215  0.020191  \n",
       "5506   -0.005305 -0.064634  0.061770  \n",
       "5586   -0.007177  0.022947 -0.047732  \n",
       "5630    0.081876  0.103826  0.039167  \n",
       "5677   -0.015678 -0.036398 -0.055054  \n",
       "5738   -0.004090  0.004913  0.006042  \n",
       "5799   -0.061358  0.023729 -0.013348  \n",
       "\n",
       "[54 rows x 219 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_REPS.xs('100195065450589', level='asid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100195065450589'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_LIST[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
