{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Target Network] Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import line_profiler\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "  np.random.seed(seed)  \n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "CONTEXT_REPS = pd.read_pickle('../../data/w_final_context.pkl')\n",
    "STREAM_ITEM_DICT = pd.read_pickle('../../data/stream_item_dict.pkl')\n",
    "BERT_BY_IDX_DF = pd.read_pickle('../../data/bert_by_idx_pca.pkl')\n",
    "BOUGHT_DICT = pd.read_pickle('../../data/bought_dict.pkl')\n",
    "USER_ALL_STREAM_INIT = CONTEXT_REPS.describe().loc['50%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1397141, 219), 7701, (162189, 160), 79207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_REPS.shape, len(STREAM_ITEM_DICT), BERT_BY_IDX_DF.shape, len(BOUGHT_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "USER_LIST = CONTEXT_REPS.index.get_level_values('asid').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "LB_ITEMS = ['item_id'] + [f'i{x}' for x in range(160)]\n",
    "INPUT_DF_COL__USR = CONTEXT_REPS.columns.to_list()\n",
    "INPUT_DF_COL = INPUT_DF_COL__USR + LB_ITEMS\n",
    "\n",
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "Convert state format to model input format\n",
    "'''\n",
    "def get_input_tensor(input_state, current_stream, with_tensor=False):\n",
    "  # Get item feats\n",
    "  # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
    "  item_list = STREAM_ITEM_DICT[current_stream]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
    "\n",
    "  # Fill in other context\n",
    "  stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
    "  \n",
    "  # Merge with items\n",
    "  stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
    "  \n",
    "  # Convert to tensor\n",
    "  if with_tensor: \n",
    "    stream_item_feat_tensor = df_to_tensor(stream_item_feat)\n",
    "    return stream_item_feat_tensor, stream_item_feat\n",
    "  else:\n",
    "    return stream_item_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "\n",
    "Generate series: whether elements in A existed in list B\n",
    "A, B: List\n",
    "return: pd.Series\n",
    "example:\n",
    "  A: [1, 2, 4, 5]\n",
    "  B: [1, 2, 3, 4, 5, 6, 7]\n",
    "  return: Series([1, 1, 0, 1, 1, 0, 0], index=[1, 2, 3, 4, 5, 6, 7])\n",
    "'''\n",
    "def gen_exist_series(A, B):\n",
    "  return [int(item in A) for item in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_tensor(input_df):\n",
    "  return torch.tensor(input_df.values).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self, max_memory=100000, discount=.9, model_output_shape=1):\n",
    "    \"\"\"\n",
    "    Setup\n",
    "    max_memory: the maximum number of experiences we want to store\n",
    "    memory: a list of experiences\n",
    "    discount: the discount factor for future experience\n",
    "    In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "    [...\n",
    "    [experience, game_over]\n",
    "    [experience, game_over]\n",
    "    ...]\n",
    "    \"\"\"\n",
    "    self.max_memory = max_memory\n",
    "    self.memory = list()\n",
    "    self.discount = discount\n",
    "    self.model_output_shape = model_output_shape\n",
    "\n",
    "  def remember(self, states, game_over):\n",
    "    # Save a state to memory\n",
    "    self.memory.append([states, game_over])\n",
    "    # We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "    if len(self.memory) > self.max_memory:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def get_batch(self, eval_net, target_net, batch_size=10):\n",
    "    # How many experiences do we have?\n",
    "    len_memory = len(self.memory)\n",
    "\n",
    "    # Calculate the number of actions that can possibly be taken in the game.\n",
    "    # Actions: 0 = not recommend, 1 = recommend\n",
    "    num_actions = self.model_output_shape\n",
    "\n",
    "    # Dimensions of our observed states, ie, the input to our model.\n",
    "    # Memory:  [\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
    "    #   ...\n",
    "    # ]\n",
    "    env_dim = len(INPUT_DF_COL)\n",
    "\n",
    "    inputs = pd.DataFrame()\n",
    "    targets = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    # We draw states to learn from randomly\n",
    "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
    "      # Here we load one transition <s, a, r, s'> from memory\n",
    "      streams, state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "      current_stream, next_stream = streams\n",
    "      game_over = self.memory[idx][1]\n",
    "\n",
    "      '''\n",
    "      修改倒入 state 的方式 input = (state - item) + item_feat\n",
    "      拆掉 model_predict 成 function\n",
    "      \n",
    "      here should be state_t * all_items\n",
    "      '''\n",
    "      state_tensor, state_t = get_input_tensor(state_t, current_stream, with_tensor=True)\n",
    "      # puts state into input\n",
    "      inputs = pd.concat([inputs, state_t], axis=0)\n",
    "      \n",
    "      # use target_net to predict target for eval_net to learn\n",
    "      # current_target = target_net(state_tensor).detach().view(len(reward_t), 1)\n",
    "\n",
    "      # selected_ids = np.where(action_t > 0)[0]\n",
    "      reward_t = df_to_tensor(reward_t).view(len(reward_t), 1)\n",
    "      \n",
    "      \n",
    "      '''\n",
    "      每個 actions 都會被 predict 一個成績/reward\n",
    "      '''\n",
    "      # if the game ended, the reward is the final reward\n",
    "      if game_over:  # if game_over is True\n",
    "        current_target = reward_t\n",
    "      else:\n",
    "        state_tp1, _ = get_input_tensor(state_tp1, next_stream, with_tensor=True)\n",
    "        if target_net == None:\n",
    "          Q_sa = torch.max(eval_net(state_tp1))\n",
    "        else:\n",
    "          Q_sa = torch.max(target_net(state_tp1).detach())\n",
    "        \n",
    "        # r + gamma * max Q(s',a')\n",
    "        # current_target = reward_t + self.discount * Q_sa\n",
    "        current_target = reward_t.add(Q_sa, alpha=self.discount)\n",
    "\n",
    "      targets = torch.cat((targets, current_target), 0)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "\n",
    "class Epsilon(ABC):\n",
    "  @abstractmethod\n",
    "  def clear(self):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def get_epsilon(self, key):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_step(self, key, data, delta):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_epoch(self, data):\n",
    "    pass\n",
    "  \n",
    "  # @abstractmethod\n",
    "  # def update_at_epsisode():\n",
    "  #   pass\n",
    "\n",
    "\n",
    "class Decay(Epsilon):\n",
    "  # Ref: Decay(0.5, 0.85)\n",
    "  '''\n",
    "  Epsilon Decay EE method with update/decay at epoch\n",
    "  '''\n",
    "  def __init__(self, initial, epoch_decay, step_decay=1.0):\n",
    "    self.initial = initial\n",
    "    self.epoch_decay, self.step_decay = epoch_decay, step_decay\n",
    "    self.epsilon = self.initial\n",
    "    \n",
    "  def clear(self):\n",
    "    self.epsilon = self.initial # should be 4 for origin setting\n",
    "    \n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon\n",
    "  \n",
    "  def update_at_step(self, key, data, delta):\n",
    "    # origin setting\n",
    "    pass\n",
    "    # exponentially\n",
    "    # self.epsilon *= self.step_decay\n",
    "    \n",
    "  def update_at_epoch(self, data):\n",
    "    # origin settings\n",
    "    epoch = data\n",
    "    self.epsilon = 4 / ((epoch + 1) ** (1 / 2))\n",
    "    # exponentially\n",
    "    # self.epsilon *= self.epoch_decay\n",
    "\n",
    "\n",
    "class VDBE(Epsilon):\n",
    "  # VDBE(0.5, 0.01)\n",
    "  def __init__(self, initial, sigma):\n",
    "    self.initial = initial\n",
    "    self.sigma = sigma\n",
    "\n",
    "  def clear(self):\n",
    "    self.epsilon = defaultdict(lambda: self.initial)\n",
    "\n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon[key]\n",
    "  \n",
    "  def update_at_step(self, key, data, delta):\n",
    "    td_error = data\n",
    "    coeff = math.exp(-abs(td_error) / self.sigma)\n",
    "    f = (1.0 - coeff) / (1.0 + coeff)\n",
    "    self.epsilon[key] = delta * f + (1.0 - delta) * self.epsilon[key]\n",
    "  \n",
    "  def update_at_epoch(self, data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "  def __init__(self, target_net, exp_replay, epsilon, num_episode, epochs, batch_size, lr, switch_param_threshold, single_reward):\n",
    "    self.eval_net = Net()\n",
    "    self.target_net = Net() if target_net else None\n",
    "    self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "    self.exp_replay = exp_replay\n",
    "    self.epsilon = epsilon\n",
    "    self.num_episode = num_episode\n",
    "    self.epochs = epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.switch_param_threshold = switch_param_threshold\n",
    "    self.single_reward = single_reward\n",
    "    self.user_all_stream_init = USER_ALL_STREAM_INIT\n",
    "    self.hist = []\n",
    "    self.c_hist = []\n",
    "    self.rec_list = []\n",
    "    self.ep_score_list = []\n",
    "    self.learn_step_counter = 0\n",
    "\n",
    "  # Environment Methods\n",
    "  def __episodes(self):\n",
    "    # return USER_LIST[:self.num_episode]\n",
    "    return np.random.choice(USER_LIST, self.num_episode, replace=False)\n",
    "  \n",
    "  def __user_episode_context(self):\n",
    "    self.user_all_streams = CONTEXT_REPS.xs(self.asid, level=\"asid\")\n",
    "    self.stream_list = self.user_all_streams.index\n",
    "    self.final_stream = max(self.stream_list)\n",
    "  \n",
    "  def __full_state(self, i):\n",
    "    '''\n",
    "    retrieve full state -> should be exported to pickle\n",
    "    '''\n",
    "    if (i - 1) == -1:\n",
    "      user_part = self.user_all_stream_init.copy()\n",
    "      user_part.name = self.stream_list[i]\n",
    "    else:\n",
    "      user_part = self.user_all_streams.loc[self.stream_list[(i - 1)]]\n",
    "    return user_part\n",
    "\n",
    "  def reward(self):\n",
    "    '''\n",
    "    Comparison function for reward, 考慮「所有」歷史購買紀錄\n",
    "    '''\n",
    "    real_bought_ids = BOUGHT_DICT[self.asid]\n",
    "    real_bought_ids_series = gen_exist_series(real_bought_ids, self.stream_items)\n",
    "    \n",
    "    reward_list = [a & b for a, b in zip(real_bought_ids_series, self.action_ids)]\n",
    "    # Reward Count \n",
    "    self.rec_cnt += 1\n",
    "    if sum(reward_list) > 0:\n",
    "      self.c_win_cnt += 1\n",
    "      self.win_cnt += 1\n",
    "      self.ep_score += sum(reward_list)\n",
    "    # return list(map(lambda x: x * sum(reward_list), reward_list))\n",
    "    if self.single_reward:\n",
    "      return pd.Series(reward_list, index=self.stream_items)\n",
    "    else:\n",
    "      return pd.Series(list(map(lambda x: x * sum(reward_list), reward_list)), index=self.stream_items)\n",
    "\n",
    "  # Agent Methods\n",
    "  def __choose_actions(self):\n",
    "    if np.random.rand() <= self.epsilon.get_epsilon(self.asid):\n",
    "    # if len(self.exp_replay.memory) < 1:\n",
    "      # Explore by randomly select 10/n items from candidate_items\n",
    "      # Get all items from the stream\n",
    "      self.explore += 1\n",
    "      selected_actions = random.sample(self.stream_items, 10) if len(self.stream_items) > 10 else self.stream_items\n",
    "    else:\n",
    "      # Exploit by choosing action from the model's prediction\n",
    "      self.exploit += 1\n",
    "      selected_actions = self.__agent_predict()\n",
    "    x = pd.Series(0, index=self.stream_items)\n",
    "    x.loc[selected_actions] = 1\n",
    "    return x\n",
    "    \n",
    "  def q_value(self): \n",
    "    if type(self.epsilon) == Decay: return 0\n",
    "\n",
    "    predicts = self.eval_net(self.full_input).flatten()    \n",
    "    actions_idx = np.where(self.action_ids.values == 1)[0]\n",
    "    q_val = predicts[actions_idx].mean()\n",
    "    return q_val\n",
    "\n",
    "  def __agent_predict(self):\n",
    "    predicts = self.eval_net(self.full_input).flatten()\n",
    "    if len(predicts) > 10:\n",
    "      top10_idx = torch.topk(predicts, 10).indices.cpu()\n",
    "      actions = self.candidate_actions.iloc[top10_idx]['item_id'].values\n",
    "    else:\n",
    "      actions = self.candidate_actions['item_id'].values\n",
    "    return actions\n",
    "\n",
    "  def __train_agent_batch(self, inputs, targets):\n",
    "    self.optimizer.zero_grad()\n",
    "    outputs = self.eval_net(inputs)\n",
    "    loss = self.loss_fn(outputs, targets)\n",
    "    # Add CL Regularization Term\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "  # MAIN TRAIN\n",
    "  def train(self):\n",
    "    self.eval_net.to(DEVICE)\n",
    "    if self.target_net:\n",
    "      self.target_net.to(DEVICE)\n",
    "    self.c_win_cnt = 0\n",
    "    self.eval_net.train(True)\n",
    "    self.epsilon.clear()\n",
    "\n",
    "    for e in self.epochs:\n",
    "      self.rec_cnt = 0\n",
    "      self.win_cnt = 0\n",
    "      self.loss = 0.\n",
    "      self.ep_score = 0\n",
    "      self.explore = 0\n",
    "      self.exploit = 0\n",
    "\n",
    "      print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "      # ------------------- Episode (User) -------------------------------\n",
    "      for asid in tqdm(self.__episodes()):\n",
    "        self.asid = asid\n",
    "        self.__user_episode_context()\n",
    "\n",
    "        # ----------------- Runs (User x All_Stream) ---------------------\n",
    "        for i, stream in enumerate(self.stream_list):\n",
    "          game_over = stream == self.final_stream\n",
    "          self.current_stream = stream\n",
    "          self.current_state = self.__full_state(i)\n",
    "          self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
    "          self.full_input, self.candidate_actions = get_input_tensor(self.current_state, self.current_stream, with_tensor=True)\n",
    "\n",
    "          # --------------- Explore/Exploit Section ----------------------\n",
    "          self.action_ids = self.__choose_actions()\n",
    "\n",
    "          # --------------- Get next state & info to store ---------------\n",
    "          reward = self.reward()\n",
    "          next_state = self.__full_state(i+1) if not game_over else []\n",
    "          next_stream = 0 if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
    "          self.exp_replay.remember([[stream, next_stream], self.current_state, self.action_ids, reward, next_state], game_over)\n",
    "          self.learn_step_counter += 1\n",
    "          if self.target_net and (self.learn_step_counter % self.switch_param_threshold == 0):\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "\n",
    "\n",
    "          # --------------- Load batch of experiences --------------------\n",
    "          inputs, targets = self.exp_replay.get_batch(self.eval_net, self.target_net, batch_size=self.batch_size)\n",
    "          inputs = df_to_tensor(inputs)\n",
    "          # store pre-training value for td_error\n",
    "          old_Q = self.q_value()\n",
    "          batch_loss = self.__train_agent_batch(inputs, targets)\n",
    "          # store post-training value for td_error\n",
    "          new_Q = self.q_value()\n",
    "          self.loss += batch_loss\n",
    "\n",
    "          # --------------- Update with TD error -------------------------\n",
    "          self.epsilon.update_at_step(self.asid, (new_Q - old_Q), len(self.stream_items))\n",
    "\n",
    "      # Track win history to later check if our model is improving at the game over time.\n",
    "      self.hist.append(self.win_cnt)\n",
    "      self.c_hist.append(self.c_win_cnt)\n",
    "      self.rec_list.append(self.rec_cnt)\n",
    "      self.ep_score_list.append(self.ep_score)\n",
    "\n",
    "      print(f'Epoch: {e}/{len(self.epochs)} | Loss {self.loss} | Epoch Hit Rate {self.win_cnt/self.rec_cnt} | \\\n",
    "              Cumulative Hit Rate {self.c_win_cnt/sum(self.rec_list)} | Explore {self.explore} | Exploit {self.exploit} | \\\n",
    "              Score {self.ep_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 2  # Number of experiences we use for training per batch\n",
    "EPOCH = range(100)\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 1.0e-4\n",
    "SWITCH_PARAM_THRESHOLD = 100\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(380, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, 128)\n",
    "    self.fc4 = nn.Linear(128, 64)\n",
    "    self.fc5 = nn.Linear(64, 1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc4(x)\n",
    "    x = self.tanh(x)\n",
    "    x = self.fc5(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp: Reward Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 17:21:57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7856ae3c96c24698a439461c8760e30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 | Loss 3309.6643291336295 | Epoch Hit Rate 0.5628177196804648 |               Cumulative Hit Rate 0.5628177196804648 | Explore 2005 | Exploit 2126 |               Score 3521\n",
      "Epoch 1 started.   Time: 17:27:10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a3941b1e7241749c95d5f0f8983313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 | Loss 20198.32041412592 | Epoch Hit Rate 0.522938623682579 |               Cumulative Hit Rate 0.5453309773005302 | Explore 1611 | Exploit 1615 |               Score 2549\n",
      "Epoch 2 started.   Time: 17:31:08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d05da75caac4e1aa70f644758233610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 | Loss 38197.964174985886 | Epoch Hit Rate 0.5242312423124231 |               Cumulative Hit Rate 0.5378217475048153 | Explore 2056 | Exploit 2009 |               Score 2996\n",
      "Epoch 3 started.   Time: 17:36:08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fba02e55284e0ba4b3bab59d120fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 | Loss 104211.96636009216 | Epoch Hit Rate 0.5483528161530287 |               Cumulative Hit Rate 0.5408941526632356 | Explore 2402 | Exploit 2303 |               Score 3895\n",
      "Epoch 4 started.   Time: 17:41:36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1993ba2afe24450b07d479c1045af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 | Loss 99734.3571228981 | Epoch Hit Rate 0.5426179604261796 |               Cumulative Hit Rate 0.5412327470227715 | Explore 1906 | Exploit 2036 |               Score 3242\n",
      "Epoch 5 started.   Time: 17:46:16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89063dbc3e7a4f7cb4ad9c6e6f57a434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 | Loss 154970.21319770813 | Epoch Hit Rate 0.5362966272057181 |               Cumulative Hit Rate 0.5403324370569543 | Explore 2244 | Exploit 2233 |               Score 3447\n",
      "Epoch 6 started.   Time: 17:51:40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b389a907231b4d399a934576b50fbd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 | Loss 235429.96506404877 | Epoch Hit Rate 0.5482491856677525 |               Cumulative Hit Rate 0.5416525222350465 | Explore 2419 | Exploit 2493 |               Score 3929\n",
      "Epoch 7 started.   Time: 17:57:23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7b95f55b084e0ca1133096c84d177e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100 | Loss 341833.42418670654 | Epoch Hit Rate 0.5372417107160019 |               Cumulative Hit Rate 0.5411064842355741 | Explore 2098 | Exploit 2064 |               Score 3349\n",
      "Epoch 8 started.   Time: 18:02:45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b3f7f92775438790c9a3524ca93c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/100 | Loss 473710.02977085114 | Epoch Hit Rate 0.5251363990646921 |               Cumulative Hit Rate 0.5389915359207267 | Explore 2595 | Exploit 2537 |               Score 3838\n",
      "Epoch 9 started.   Time: 18:08:34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996a21a25b1d4adfb9a199e2a1d71fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/100 | Loss 451517.9931049347 | Epoch Hit Rate 0.5286217525319242 |               Cumulative Hit Rate 0.5379036356077055 | Explore 2248 | Exploit 2294 |               Score 3441\n",
      "Epoch 10 started.   Time: 18:13:47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccac165fbd24f6e8c9c681005aed50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100 | Loss 678265.9563064575 | Epoch Hit Rate 0.5384159589176377 |               Cumulative Hit Rate 0.5379572760923962 | Explore 2510 | Exploit 2553 |               Score 4058\n",
      "Epoch 11 started.   Time: 18:19:28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa62e5cbc0e4f0fa2c6a3f3eb411a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/100 | Loss 1081355.945842743 | Epoch Hit Rate 0.5506018724921979 |               Cumulative Hit Rate 0.539030713623375 | Explore 2276 | Exploit 2210 |               Score 3820\n",
      "Epoch 12 started.   Time: 18:25:30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bffad6a12c04f67b3955379f2216d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/100 | Loss 1104074.0092849731 | Epoch Hit Rate 0.5441878221034548 |               Cumulative Hit Rate 0.5394958851279226 | Explore 2627 | Exploit 2612 |               Score 4201\n",
      "Epoch 13 started.   Time: 18:32:21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a21c9695bb04c9a99c4b83a391b9325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/100 | Loss 1322308.0160446167 | Epoch Hit Rate 0.5199254253087858 |               Cumulative Hit Rate 0.5381495198242829 | Explore 2142 | Exploit 2149 |               Score 3288\n",
      "Epoch 14 started.   Time: 18:38:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83199e08a4854638ba2a52c2353e7d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/100 | Loss 1766063.5102920532 | Epoch Hit Rate 0.5291914518616435 |               Cumulative Hit Rate 0.5375418460066954 | Explore 2274 | Exploit 2265 |               Score 3580\n",
      "Epoch 15 started.   Time: 18:43:26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfb783e0c034f9d83b233d798a38a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/100 | Loss 2087157.8594970703 | Epoch Hit Rate 0.5570411904248437 |               Cumulative Hit Rate 0.5388055738025689 | Explore 2321 | Exploit 2316 |               Score 3938\n",
      "Epoch 16 started.   Time: 18:49:30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef6efce92fb4ba5be109024d1eb1b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/100 | Loss 2506751.873199463 | Epoch Hit Rate 0.5640580966832863 |               Cumulative Hit Rate 0.5403350752343689 | Explore 2238 | Exploit 2375 |               Score 3861\n",
      "Epoch 17 started.   Time: 18:55:20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6006f614411f4b3ab2df0ec3906a44ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/100 | Loss 2866988.8650512695 | Epoch Hit Rate 0.558612891066717 |               Cumulative Hit Rate 0.5415255069475131 | Explore 2601 | Exploit 2705 |               Score 4462\n",
      "Epoch 18 started.   Time: 19:01:51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5e5ef8996d401fbd973e474d79952d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/100 | Loss 3494480.682647705 | Epoch Hit Rate 0.5507524815882164 |               Cumulative Hit Rate 0.5421825478258887 | Explore 3128 | Exploit 3118 |               Score 4895\n",
      "Epoch 19 started.   Time: 19:09:28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ce23a35d1a42498773c5ab9a0b936e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/100 | Loss 3373916.43611145 | Epoch Hit Rate 0.5423842540818609 |               Cumulative Hit Rate 0.5421923306394749 | Explore 2225 | Exploit 2246 |               Score 3564\n",
      "Epoch 20 started.   Time: 19:15:02\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6280b93bd9403ca964ef5f13325fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/100 | Loss 3793169.7850494385 | Epoch Hit Rate 0.5507749248207263 |               Cumulative Hit Rate 0.542576781199486 | Explore 2186 | Exploit 2137 |               Score 3612\n",
      "Epoch 21 started.   Time: 19:20:51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1cb58bb58944a9b434cd143f7472aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21/100 | Loss 4839313.985961914 | Epoch Hit Rate 0.5415338458653666 |               Cumulative Hit Rate 0.5425221669072378 | Explore 2624 | Exploit 2709 |               Score 4222\n",
      "Epoch 22 started.   Time: 19:28:01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8a41b8a18d4972b68cb640c9278a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22/100 | Loss 4444294.972900391 | Epoch Hit Rate 0.5187203791469195 |               Cumulative Hit Rate 0.5415751312923694 | Explore 2118 | Exploit 2102 |               Score 3230\n",
      "Epoch 23 started.   Time: 19:33:32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb7154994764f6498241e0667210653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23/100 | Loss 5264057.072418213 | Epoch Hit Rate 0.5491448588501957 |               Cumulative Hit Rate 0.5419063418504427 | Explore 2477 | Exploit 2376 |               Score 3793\n",
      "Epoch 24 started.   Time: 19:39:35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e500f1ab0e714f62b0fedd77766e7a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24/100 | Loss 5153670.085357666 | Epoch Hit Rate 0.536946736684171 |               Cumulative Hit Rate 0.5416788534659257 | Explore 2687 | Exploit 2645 |               Score 4314\n",
      "Epoch 25 started.   Time: 19:46:23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5bd781e7a742539b32fcd61a1e862e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/100 | Loss 2648743.2364959717 | Epoch Hit Rate 0.545414364640884 |               Cumulative Hit Rate 0.5418188141192836 | Explore 2256 | Exploit 2269 |               Score 3646\n",
      "Epoch 26 started.   Time: 19:51:54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e86b68d6da449be9dc6a61ae1e90cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26/100 | Loss 2487546.397201538 | Epoch Hit Rate 0.5295330958184886 |               Cumulative Hit Rate 0.5412992450675633 | Explore 2672 | Exploit 2661 |               Score 4019\n",
      "Epoch 27 started.   Time: 19:58:56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8ef08936ae46eb85c95708ac036947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27/100 | Loss 2231308.607788086 | Epoch Hit Rate 0.5248175182481751 |               Cumulative Hit Rate 0.5407790252968191 | Explore 2028 | Exploit 2082 |               Score 3234\n",
      "Epoch 28 started.   Time: 20:04:24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fac15697a14c01a3c90a4b651f902f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28/100 | Loss 2466088.762008667 | Epoch Hit Rate 0.5163781624500666 |               Cumulative Hit Rate 0.5400950966268315 | Explore 1869 | Exploit 1886 |               Score 2807\n",
      "Epoch 29 started.   Time: 20:09:33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcfa977722246e7bb74bdbe62818ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/100 | Loss 3000843.4983062744 | Epoch Hit Rate 0.5492240543161979 |               Cumulative Hit Rate 0.5403677232010312 | Explore 2026 | Exploit 2098 |               Score 3446\n",
      "Epoch 30 started.   Time: 20:15:11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9619114a44764371ae8fedbc27feafd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30/100 | Loss 3858905.3718414307 | Epoch Hit Rate 0.5297405189620759 |               Cumulative Hit Rate 0.5399956674563077 | Explore 2508 | Exploit 2502 |               Score 3798\n",
      "Epoch 31 started.   Time: 20:21:47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a66dba8f694db6918c9a5953eb19a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31/100 | Loss 3535336.296508789 | Epoch Hit Rate 0.5368657835541115 |               Cumulative Hit Rate 0.5399105394822711 | Explore 1978 | Exploit 2023 |               Score 3280\n",
      "Epoch 32 started.   Time: 20:27:19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fd4958523849109e606ff8d9110df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "EPS_baseline_rsingle_x\n",
    "\n",
    "- VDBE(0.5, 0.01)\n",
    "- Decay(0.5, 0.85)\n",
    "'''\n",
    "\n",
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)\n",
    "epsilon = Decay(0.5, 0.85)\n",
    "dqn = DQN(False, exp_replay, epsilon, NUM_EPISODE, EPOCH, BATCH_SIZE, LR, SWITCH_PARAM_THRESHOLD, single_reward=True)\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'step_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ca12f4a5af36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexp_replay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_MEMORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPISODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSWITCH_PARAM_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'step_decay'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "EPS_baseline_rsum_x\n",
    "'''\n",
    "\n",
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)\n",
    "epsilon = Decay(0.5, 0.85)\n",
    "dqn = DQN(False, exp_replay, epsilon, NUM_EPISODE, EPOCH, BATCH_SIZE, LR, SWITCH_PARAM_THRESHOLD, single_reward=False)\n",
    "dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline100 = pd.read_pickle('../Experiment/baseline-100ep-res.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "res = [a/b for a, b in zip(dqn.hist, dqn.rec_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc5c4b9e588>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(res, label='VDBE hit@10')\n",
    "# plt.plot(baseline100, label='baseline hit@10')\n",
    "# plt.plot(pd.Series(res).rolling(30).mean(), label='VDBE hit@10 ma 30')\n",
    "# plt.plot(pd.Series(baseline100).rolling(30).mean(), label='Baseline hit@10 ma 30')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Hit Ratio')\n",
    "plt.title('Hit@10 for BERT-baseline')\n",
    "# plt.ylim([0.4, 0.6])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Experiment/vdbe_100_max_target.pkl', 'wb') as file_pi:\n",
    "  pickle.dump(res, file_pi, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5406706451835664"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.c_hist[-1]/sum(dqn.rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n請不要關掉這ㄍ分頁 乾蝦哈咪搭\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "請不要關掉這ㄍ分頁 乾蝦哈咪搭\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "with open('../Models/vdbe_100_target.pkl', 'wb') as file_pi:\n",
    "  pickle.dump(dqn, file_pi, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test for correct memory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 15:36:33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac2426eb3654d91b2d68fcfa3d1c32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2 | Loss 84.24398557795212 | Epoch Hit Rate 0.6416040100250626 |               Cumulative Hit Rate 0.6416040100250626 | Explore 187 | Exploit 212 |               Score 395\n",
      "Epoch 1 started.   Time: 15:37:14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8eecd2f28f4a6da8340b5665a1a64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 51.9529 s\n",
       "File: <ipython-input-147-2ee99dcd434c>\n",
       "Function: train at line 100\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   100                                             def train(self):\n",
       "   101         1   14571425.0 14571425.0      0.0      self.eval_net.to(DEVICE)\n",
       "   102         1     475983.0 475983.0      0.0      self.target_net.to(DEVICE)\n",
       "   103         1        832.0    832.0      0.0      self.c_win_cnt = 0\n",
       "   104         1      38920.0  38920.0      0.0      self.eval_net.train(True)\n",
       "   105         1       3496.0   3496.0      0.0      self.epsilon.clear()\n",
       "   106         1        398.0    398.0      0.0      self.explore = 0\n",
       "   107         1        245.0    245.0      0.0      self.exploit = 0\n",
       "   108                                           \n",
       "   109         2       1204.0    602.0      0.0      for e in self.epochs:\n",
       "   110         2        624.0    312.0      0.0        self.rec_cnt = 0\n",
       "   111         2        430.0    215.0      0.0        self.win_cnt = 0\n",
       "   112         2        821.0    410.5      0.0        self.loss = 0.\n",
       "   113         2        475.0    237.5      0.0        self.ep_score = 0\n",
       "   114                                           \n",
       "   115         2     188434.0  94217.0      0.0        print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
       "   116                                                 # ------------------- Episode (User) -------------------------------\n",
       "   117        10  384940458.0 38494045.8      0.7        for asid in tqdm(self.__episodes()):\n",
       "   118        10       8690.0    869.0      0.0          self.asid = asid\n",
       "   119        10   25338205.0 2533820.5      0.0          self.__user_episode_context()\n",
       "   120                                           \n",
       "   121                                                   # ----------------- Runs (User x All_Stream) ---------------------\n",
       "   122       486     965731.0   1987.1      0.0          for i, stream in enumerate(self.stream_list):\n",
       "   123       486     305358.0    628.3      0.0            game_over = stream == self.final_stream\n",
       "   124       486     162835.0    335.1      0.0            self.current_stream = stream\n",
       "   125       486  196114211.0 403527.2      0.4            self.current_state = self.__full_state(i)\n",
       "   126       486     548310.0   1128.2      0.0            self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
       "   127       486 9399598953.0 19340738.6     18.1            self.full_input, self.candidate_actions = get_input_tensor(self.current_state, self.current_stream, with_tensor=True)\n",
       "   128                                           \n",
       "   129                                                     # --------------- Explore/Exploit Section ----------------------\n",
       "   130       486  552483687.0 1136797.7      1.1            self.action_ids = self.__choose_actions()\n",
       "   131                                           \n",
       "   132                                                     # --------------- Get next state & info to store ---------------\n",
       "   133       486 2728603826.0 5614411.2      5.3            reward = self.reward()\n",
       "   134       486  170807263.0 351455.3      0.3            next_state = self.__full_state(i+1) if not game_over else []\n",
       "   135       486    1890758.0   3890.4      0.0            next_stream = 0 if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
       "   136       486    1655747.0   3406.9      0.0            self.exp_replay.remember([[stream, next_stream], self.current_state, self.action_ids, reward, next_state], game_over)\n",
       "   137       486     293531.0    604.0      0.0            self.learn_step_counter += 1\n",
       "   138       482     315128.0    653.8      0.0            if self.learn_step_counter % self.switch_param_threshold == 0:\n",
       "   139         4    1123412.0 280853.0      0.0              self.target_net.load_state_dict(self.eval_net.state_dict())\n",
       "   140                                           \n",
       "   141                                           \n",
       "   142                                                     # --------------- Load batch of experiences --------------------\n",
       "   143       486 37011044863.0 76154413.3     71.2            inputs, targets = self.exp_replay.get_batch(self.eval_net, self.target_net, batch_size=self.batch_size)\n",
       "   144       486  110071097.0 226483.7      0.2            inputs = df_to_tensor(inputs)\n",
       "   145                                                     # store pre-training value for td_error\n",
       "   146       486  199400949.0 410290.0      0.4            old_Q = self.q_value()\n",
       "   147       486  908458194.0 1869255.5      1.7            batch_loss = self.__train_agent_batch(inputs, targets)\n",
       "   148                                                     # store post-training value for td_error\n",
       "   149       486  198883830.0 409226.0      0.4            new_Q = self.q_value()\n",
       "   150       486     582820.0   1199.2      0.0            self.loss += batch_loss\n",
       "   151                                           \n",
       "   152                                                     # --------------- Update with TD error -------------------------\n",
       "   153       486   43877127.0  90282.2      0.1            self.epsilon.update_at_step(f'{self.asid}-{self.current_stream}', (new_Q - old_Q), len(self.stream_items))\n",
       "   154                                           \n",
       "   155                                                 # Track win history to later check if our model is improving at the game over time.\n",
       "   156         2       2060.0   1030.0      0.0        self.hist.append(self.win_cnt)\n",
       "   157         2      13809.0   6904.5      0.0        self.c_hist.append(self.c_win_cnt)\n",
       "   158         2       1170.0    585.0      0.0        self.rec_list.append(self.rec_cnt)\n",
       "   159         2       1391.0    695.5      0.0        self.ep_score_list.append(self.ep_score)\n",
       "   160                                           \n",
       "   161         2        722.0    361.0      0.0        print(f'Epoch: {e}/{len(self.epochs)} | Loss {self.loss} | Epoch Hit Rate {self.win_cnt/self.rec_cnt} | \\\n",
       "   162                                                         Cumulative Hit Rate {self.c_win_cnt/sum(self.rec_list)} | Explore {self.explore} | Exploit {self.exploit} | \\\n",
       "   163         2     117748.0  58874.0      0.0                Score {self.ep_score}')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 | Loss 15.964139020594303 | Epoch Hit Rate 0.4367816091954023 |               Cumulative Hit Rate 0.6049382716049383 | Explore 228 | Exploit 258 |               Score 57\n",
      "CPU times: user 1min 7s, sys: 308 ms, total: 1min 8s\n",
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "CURRENT_RUNNING: correct q memory & reward * total score\n",
    "'''\n",
    "\n",
    "exp_replay2 = ReplayBuffer(max_memory=MAX_MEMORY)\n",
    "epsilon2 = VDBE(0.5, 0.01)\n",
    "dqn2 = DQN(exp_replay2, epsilon2, 5, range(2), BATCH_SIZE, LR, SWITCH_PARAM_THRESHOLD)\n",
    "%lprun -f dqn2.train  dqn2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.329238 s\n",
       "File: <ipython-input-191-cfe761152fdd>\n",
       "Function: get_batch at line 26\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    26                                             def get_batch(self, eval_net, target_net, batch_size=10):\n",
       "    27                                               # How many experiences do we have?\n",
       "    28         1        990.0    990.0      0.0      len_memory = len(self.memory)\n",
       "    29                                           \n",
       "    30                                               # Calculate the number of actions that can possibly be taken in the game.\n",
       "    31                                               # Actions: 0 = not recommend, 1 = recommend\n",
       "    32         1        329.0    329.0      0.0      num_actions = self.model_output_shape\n",
       "    33                                           \n",
       "    34                                               # Dimensions of our observed states, ie, the input to our model.\n",
       "    35                                               # Memory:  [\n",
       "    36                                               #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
       "    37                                               #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
       "    38                                               #   ...\n",
       "    39                                               # ]\n",
       "    40         1        347.0    347.0      0.0      env_dim = len(INPUT_DF_COL)\n",
       "    41                                           \n",
       "    42         1     398679.0 398679.0      0.1      inputs = pd.DataFrame()\n",
       "    43         1      81003.0  81003.0      0.0      targets = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
       "    44                                               \n",
       "    45                                               \n",
       "    46                                               # We draw states to learn from randomly\n",
       "    47         2      62566.0  31283.0      0.0      for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
       "    48                                                 # Here we load one transition <s, a, r, s'> from memory\n",
       "    49         2       8327.0   4163.5      0.0        streams, state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
       "    50         2        789.0    394.5      0.0        current_stream, next_stream = streams\n",
       "    51         2        668.0    334.0      0.0        game_over = self.memory[idx][1]\n",
       "    52                                           \n",
       "    53                                                 '''\n",
       "    54                                                 修改倒入 state 的方式 input = (state - item) + item_feat\n",
       "    55                                                 拆掉 model_predict 成 function\n",
       "    56                                                 \n",
       "    57                                                 here should be state_t * all_items\n",
       "    58                                                 '''\n",
       "    59         2  296838686.0 148419343.0     90.2        state_tensor, state_t = get_input_tensor(state_t, current_stream, with_tensor=True)\n",
       "    60                                                 # puts state into input\n",
       "    61         2    1608358.0 804179.0      0.5        inputs = pd.concat([inputs, state_t], axis=0)\n",
       "    62                                                 \n",
       "    63                                                 # use target_net to predict target for eval_net to learn\n",
       "    64         2     855204.0 427602.0      0.3        current_target = target_net(state_tensor).detach().view(len(reward_t), 1)\n",
       "    65                                           \n",
       "    66         2     534647.0 267323.5      0.2        selected_ids = np.where(action_t > 0)[0]\n",
       "    67         2     642437.0 321218.5      0.2        reward_t = torch.tensor(reward_t.values).to(DEVICE).view(len(reward_t), 1).float()\n",
       "    68                                                 \n",
       "    69                                                 '''\n",
       "    70                                                 每個 actions 都會被 predict 一個成績/reward\n",
       "    71                                                 '''\n",
       "    72                                                 # if the game ended, the reward is the final reward\n",
       "    73         2        702.0    351.0      0.0        if game_over:  # if game_over is True\n",
       "    74                                                   current_target[selected_ids] = reward_t[selected_ids]\n",
       "    75                                                 else:\n",
       "    76         2   27087044.0 13543522.0      8.2          state_tp1, _ = get_input_tensor(state_tp1, next_stream, with_tensor=True)\n",
       "    77         2     744343.0 372171.5      0.2          Q_sa = torch.max(target_net(state_tp1).detach())\n",
       "    78                                                   \n",
       "    79                                                   # r + gamma * max Q(s',a')\n",
       "    80                                                   # current_target = reward_t + self.discount * Q_sa\n",
       "    81         2     292185.0 146092.5      0.1          current_target[selected_ids] = reward_t[selected_ids] + Q_sa * self.discount\n",
       "    82                                           \n",
       "    83         2      80781.0  40390.5      0.0        targets = torch.cat((targets, current_target), 0)\n",
       "    84         1        281.0    281.0      0.0      return inputs, targets"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f exp_replay2.get_batch exp_replay2.get_batch(dqn2.eval_net, dqn2.target_net, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0283332 s\n",
       "File: <ipython-input-6-0d98a1c59d51>\n",
       "Function: get_input_tensor at line 9\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     9                                           def get_input_tensor(input_state, current_stream, with_tensor=False):\n",
       "    10                                             # Get item feats\n",
       "    11                                             # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
       "    12         1       2027.0   2027.0      0.0    item_list = STREAM_ITEM_DICT[current_stream]\n",
       "    13         1    2111140.0 2111140.0      7.5    item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
       "    14                                           \n",
       "    15                                             # Fill in other context\n",
       "    16         1   10931339.0 10931339.0     38.6    stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
       "    17                                             \n",
       "    18                                             # Merge with items\n",
       "    19         1    1233721.0 1233721.0      4.4    stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
       "    20                                             \n",
       "    21                                             # Convert to tensor\n",
       "    22         1        189.0    189.0      0.0    if with_tensor: \n",
       "    23         1   14054446.0 14054446.0     49.6      stream_item_feat_tensor = df_to_tensor(stream_item_feat)\n",
       "    24         1        300.0    300.0      0.0      return stream_item_feat_tensor, stream_item_feat\n",
       "    25                                             else:\n",
       "    26                                               return stream_item_feat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f get_input_tensor get_input_tensor(CONTEXT_REPS.iloc[0], CONTEXT_REPS.iloc[0].name[1], with_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "# %reload_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 10.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "exp_replay2 = ReplayBuffer(max_memory=MAX_MEMORY)\n",
    "epsilon2 = VDBE(0.5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.22 s, sys: 172 ms, total: 8.4 s\n",
      "Wall time: 8.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dqn2 = DQN(exp_replay2, epsilon2, 5, range(2), BATCH_SIZE, LR, SWITCH_PARAM_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Chasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 20:42:38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f255e0d2fb420cac37c173c8cf503d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2 | Loss 29.661170213483274 | Epoch Hit Rate 0.5357142857142857 |               Cumulative Hit Rate 0.5357142857142857 | Explore 35 | Exploit 49 | Score 73\n",
      "Epoch 1 started.   Time: 20:42:52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c825af29716543ce891dc53023fa6bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 | Loss 21.957408252041205 | Epoch Hit Rate 0.5808383233532934 |               Cumulative Hit Rate 0.5657370517928287 | Explore 126 | Exploit 125 | Score 160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 43.5841 s\n",
       "File: <ipython-input-106-35d26399f5ef>\n",
       "Function: train at line 102\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   102                                             def train(self):\n",
       "   103         1   14851547.0 14851547.0      0.0      self.eval_net.to(DEVICE)\n",
       "   104         1     470070.0 470070.0      0.0      self.target_net.to(DEVICE)\n",
       "   105         1        630.0    630.0      0.0      self.c_win_cnt = 0\n",
       "   106         1      41511.0  41511.0      0.0      self.eval_net.train(True)\n",
       "   107         1       3745.0   3745.0      0.0      self.epsilon.clear()\n",
       "   108         1        403.0    403.0      0.0      self.explore = 0\n",
       "   109         1        293.0    293.0      0.0      self.exploit = 0\n",
       "   110                                           \n",
       "   111         2       1402.0    701.0      0.0      for e in self.epochs:\n",
       "   112         2        774.0    387.0      0.0        self.rec_cnt = 0\n",
       "   113         2        580.0    290.0      0.0        self.win_cnt = 0\n",
       "   114         2       4359.0   2179.5      0.0        self.loss = 0.\n",
       "   115         2        583.0    291.5      0.0        self.ep_score = 0\n",
       "   116                                           \n",
       "   117         2     155680.0  77840.0      0.0        print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
       "   118                                                 # ------------------- Episode (User) -------------------------------\n",
       "   119        10  383932861.0 38393286.1      0.9        for asid in tqdm(self.__episodes()):\n",
       "   120        10       9415.0    941.5      0.0          self.asid = asid\n",
       "   121        10   25253138.0 2525313.8      0.1          self.__user_episode_context()\n",
       "   122                                           \n",
       "   123                                                   # ----------------- Runs (User x All_Stream) ---------------------\n",
       "   124       251     503727.0   2006.9      0.0          for i, stream in enumerate(self.stream_list):\n",
       "   125       251     148386.0    591.2      0.0            game_over = stream == self.final_stream\n",
       "   126       251      93040.0    370.7      0.0            self.current_stream = stream\n",
       "   127       251  103323504.0 411647.4      0.2            self.current_state = self.__full_state(i)\n",
       "   128       251     339517.0   1352.7      0.0            self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
       "   129       251 6397184711.0 25486791.7     14.7            self.full_input, self.candidate_actions = get_input_tensor(self.current_state, self.current_stream, with_tensor=True)\n",
       "   130                                           \n",
       "   131                                                     # --------------- Explore/Exploit Section ----------------------\n",
       "   132       251  282345412.0 1124882.1      0.6            self.action_ids = self.__choose_actions()\n",
       "   133                                           \n",
       "   134                                                     # --------------- Get next state & info to store ---------------\n",
       "   135       251 3187153854.0 12697824.1      7.3            reward = self.reward()\n",
       "   136       251   88312098.0 351841.0      0.2            next_state = self.__full_state(i+1) if not game_over else []\n",
       "   137       251     990561.0   3946.5      0.0            next_stream = 0 if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
       "   138       251     715486.0   2850.5      0.0            self.exp_replay.remember([[stream, next_stream], self.current_state, self.action_ids, reward, next_state], game_over)\n",
       "   139       251     153884.0    613.1      0.0            self.learn_step_counter += 1\n",
       "   140       249     157433.0    632.3      0.0            if self.learn_step_counter % self.switch_param_threshold == 0:\n",
       "   141         2     508734.0 254367.0      0.0              self.target_net.load_state_dict(self.eval_net.state_dict())\n",
       "   142                                           \n",
       "   143                                           \n",
       "   144                                                     # --------------- Load batch of experiences --------------------\n",
       "   145       251 32274151979.0 128582278.8     74.1            inputs, targets = self.exp_replay.get_batch(self.eval_net, self.target_net, batch_size=self.batch_size)\n",
       "   146       251   85096773.0 339031.0      0.2            inputs = df_to_tensor(inputs)\n",
       "   147                                                     # store pre-training value for td_error\n",
       "   148       251  119542070.0 476263.2      0.3            old_Q = self.q_value()\n",
       "   149       251  479630606.0 1910878.9      1.1            batch_loss = self.__train_agent_batch(inputs, targets)\n",
       "   150                                                     # store post-training value for td_error\n",
       "   151       251  115672801.0 460847.8      0.3            new_Q = self.q_value()\n",
       "   152       251     332667.0   1325.4      0.0            self.loss += batch_loss\n",
       "   153                                           \n",
       "   154                                                     # --------------- Update with TD error -------------------------\n",
       "   155       251   22880243.0  91156.3      0.1            self.epsilon.update_at_step(f'{self.asid}-{self.current_stream}', (new_Q - old_Q), len(self.stream_items))\n",
       "   156                                           \n",
       "   157                                                 # Track win history to later check if our model is improving at the game over time.\n",
       "   158         2       2611.0   1305.5      0.0        self.hist.append(self.win_cnt)\n",
       "   159         2       1514.0    757.0      0.0        self.c_hist.append(self.c_win_cnt)\n",
       "   160         2       1108.0    554.0      0.0        self.rec_list.append(self.rec_cnt)\n",
       "   161         2        871.0    435.5      0.0        self.ep_score_list.append(self.ep_score)\n",
       "   162                                           \n",
       "   163         2        720.0    360.0      0.0        print(f'Epoch: {e}/{len(self.epochs)} | Loss {self.loss} | Epoch Hit Rate {self.win_cnt/self.rec_cnt} | \\\n",
       "   164         2     114889.0  57444.5      0.0                Cumulative Hit Rate {self.c_win_cnt/sum(self.rec_list)} | Explore {self.explore} | Exploit {self.exploit} | Score {self.ep_score}')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f dqn2.train dqn2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### old profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0906188 s\n",
       "File: <ipython-input-103-790fe20007f1>\n",
       "Function: get_batch at line 26\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    26                                             def get_batch(self, eval_net, target_net, batch_size=10):\n",
       "    27                                               # How many experiences do we have?\n",
       "    28         1       1053.0   1053.0      0.0      len_memory = len(self.memory)\n",
       "    29                                           \n",
       "    30                                               # Calculate the number of actions that can possibly be taken in the game.\n",
       "    31                                               # Actions: 0 = not recommend, 1 = recommend\n",
       "    32         1        399.0    399.0      0.0      num_actions = self.model_output_shape\n",
       "    33                                           \n",
       "    34                                               # Dimensions of our observed states, ie, the input to our model.\n",
       "    35                                               # Memory:  [\n",
       "    36                                               #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
       "    37                                               #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
       "    38                                               #   ...\n",
       "    39                                               # ]\n",
       "    40         1        398.0    398.0      0.0      env_dim = len(INPUT_DF_COL)\n",
       "    41                                           \n",
       "    42         1   22609847.0 22609847.0     25.0      inputs = pd.DataFrame(columns=INPUT_DF_COL)\n",
       "    43         1     143000.0 143000.0      0.2      targets = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
       "    44                                               \n",
       "    45                                               \n",
       "    46                                               # We draw states to learn from randomly\n",
       "    47         2      41981.0  20990.5      0.0      for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
       "    48                                                 # Here we load one transition <s, a, r, s'> from memory\n",
       "    49         2      11134.0   5567.0      0.0        streams, state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
       "    50         2        730.0    365.0      0.0        current_stream, next_stream = streams\n",
       "    51         2        661.0    330.5      0.0        game_over = self.memory[idx][1]\n",
       "    52                                           \n",
       "    53                                                 '''\n",
       "    54                                                 修改倒入 state 的方式 input = (state - item) + item_feat\n",
       "    55                                                 拆掉 model_predict 成 function\n",
       "    56                                                 \n",
       "    57                                                 here should be state_t * all_items\n",
       "    58                                                 '''\n",
       "    59         2   29837404.0 14918702.0     32.9        state_t = get_input_tensor(state_t, current_stream)\n",
       "    60                                                 # puts state into input\n",
       "    61         2    1536230.0 768115.0      1.7        inputs = pd.concat([inputs, state_t], axis=0)\n",
       "    62                                           \n",
       "    63                                                 '''\n",
       "    64                                                 每個 actions 都會被 predict 一個成績/reward\n",
       "    65                                                 '''\n",
       "    66                                                 # if the game ended, the reward is the final reward\n",
       "    67         2        529.0    264.5      0.0        if game_over:  # if game_over is True\n",
       "    68                                                   current_target = torch.tensor(reward_t).view(len(reward_t), 1).float().to(DEVICE)\n",
       "    69                                                 else:  \n",
       "    70         2   35463888.0 17731944.0     39.1          state_tp1, _ = get_input_tensor(state_tp1, current_stream, with_tensor=True)\n",
       "    71         2     732215.0 366107.5      0.8          Q_sa = target_net(state_tp1).detach()\n",
       "    72         2      91830.0  45915.0      0.1          reward_tensor = torch.tensor(reward_t).view(len(reward_t), 1).float().to(DEVICE)\n",
       "    73                                           \n",
       "    74                                                   # r + gamma * max Q(s',a')\n",
       "    75                                                   # current_target = reward_t + self.discount * Q_sa\n",
       "    76         2      69892.0  34946.0      0.1          current_target = torch.add(reward_tensor, torch.mul(Q_sa, self.discount))\n",
       "    77         2      77339.0  38669.5      0.1        targets = torch.cat((targets, current_target), 0)\n",
       "    78         1        256.0    256.0      0.0      return inputs, targets"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f exp_replay2.get_batch exp_replay2.get_batch(dqn2.eval_net, dqn2.target_net, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0201811 s\n",
       "File: <ipython-input-102-0d98a1c59d51>\n",
       "Function: get_input_tensor at line 9\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     9                                           def get_input_tensor(input_state, current_stream, with_tensor=False):\n",
       "    10                                             # Get item feats\n",
       "    11                                             # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
       "    12         1       2116.0   2116.0      0.0    item_list = STREAM_ITEM_DICT[current_stream]\n",
       "    13         1    2208028.0 2208028.0     10.9    item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
       "    14                                           \n",
       "    15                                             # Fill in other context\n",
       "    16         1   16447950.0 16447950.0     81.5    stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
       "    17                                             \n",
       "    18                                             # Merge with items\n",
       "    19         1    1158079.0 1158079.0      5.7    stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
       "    20                                             \n",
       "    21                                             # Convert to tensor\n",
       "    22         1        262.0    262.0      0.0    if with_tensor: \n",
       "    23         1     364396.0 364396.0      1.8      stream_item_feat_tensor = df_to_tensor(stream_item_feat)\n",
       "    24         1        273.0    273.0      0.0      return stream_item_feat_tensor, stream_item_feat\n",
       "    25                                             else:\n",
       "    26                                               return stream_item_feat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f get_input_tensor get_input_tensor(CONTEXT_REPS.iloc[0], CONTEXT_REPS.iloc[0].name[1], with_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleansing\n",
    "Original Version:\n",
    "```python\n",
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "\n",
    "Convert state format to model input format\n",
    "'''\n",
    "def get_input(input_state, current_stream):\n",
    "  # Get item feats\n",
    "  # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
    "  item_list = STREAM_ITEM_DICT[current_stream]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list]\n",
    "\n",
    "  # Create new df\n",
    "  stream_item_feat = pd.DataFrame(columns=INPUT_DF_COL)\n",
    "\n",
    "  # Fill in other context\n",
    "  stream_item_feat = stream_item_feat.append([input_state]*len(item_list),ignore_index=True)\n",
    "  \n",
    "  # stream_item_feat\n",
    "  stream_item_feat[LB_ITEMS] = item_feat.reset_index()\n",
    "  \n",
    "  return stream_item_feat.astype('float32')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "Convert state format to model input format\n",
    "\n",
    "LB_ITEMS = ['item_id'] + [f'i{x}' for x in range(160)]\n",
    "INPUT_DF_COL__USR = CONTEXT_REPS.columns.to_list()\n",
    "INPUT_DF_COL = INPUT_DF_COL__USR + LB_ITEMS\n",
    "'''\n",
    "input_state = CONTEXT_REPS.iloc[0]\n",
    "current_stream = CONTEXT_REPS.iloc[0].name[1]\n",
    "def new_get_input(input_state, current_stream):\n",
    "  # Get item feats\n",
    "  # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
    "  item_list = STREAM_ITEM_DICT[current_stream]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list]\n",
    "\n",
    "  for idx in input_state.index: item_feat[idx] = input_state[idx]\n",
    "  item_feat_ri = item_feat.reset_index().rename(columns={'index':'item_id'})\n",
    "  \n",
    "  return item_feat_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.107122 s\n",
       "File: <ipython-input-22-07c49d6a115a>\n",
       "Function: new_get_input at line 11\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    11                                           def new_get_input(input_state, current_stream):\n",
       "    12                                             # Get item feats\n",
       "    13                                             # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
       "    14         1       2309.0   2309.0      0.0    item_list = STREAM_ITEM_DICT[current_stream]\n",
       "    15         1     396509.0 396509.0      0.4    item_feat = BERT_BY_IDX_DF.loc[item_list]\n",
       "    16                                           \n",
       "    17       219  105267435.0 480673.2     98.3    for idx in input_state.index: item_feat[idx] = input_state[idx]\n",
       "    18         1    1455838.0 1455838.0      1.4    item_feat_ri = item_feat.reset_index().rename(columns={'index':'item_id'})\n",
       "    19                                             \n",
       "    20         1        133.0    133.0      0.0    return item_feat_ri"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f new_get_input new_get_input(CONTEXT_REPS.iloc[0], CONTEXT_REPS.iloc[0].name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_v2(input_state, current_stream):\n",
    "  # Get item feats\n",
    "  item_list = STREAM_ITEM_DICT[current_stream]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
    "\n",
    "  # Fill in other context\n",
    "  stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
    "  \n",
    "  # Merge with items\n",
    "  stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
    "  return stream_item_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0176343 s\n",
       "File: <ipython-input-84-4d017c41242b>\n",
       "Function: get_input_v2 at line 9\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     9                                           def get_input_v2(input_state, current_stream):\n",
       "    10                                             # Get item feats\n",
       "    11                                             # STREAM_ITEM_DICT: 要拿到對的 STREAM!!!\n",
       "    12         1       8785.0   8785.0      0.0    item_list = STREAM_ITEM_DICT[current_stream]\n",
       "    13         1    5560119.0 5560119.0     31.5    item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
       "    14                                           \n",
       "    15                                             # Fill in other context\n",
       "    16         1   10882756.0 10882756.0     61.7    stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
       "    17                                             \n",
       "    18                                             # Merge with items\n",
       "    19         1    1182536.0 1182536.0      6.7    stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
       "    20         1        130.0    130.0      0.0    return stream_item_feat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f get_input_v2 get_input_v2(CONTEXT_REPS.iloc[0], CONTEXT_REPS.iloc[0].name[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Chasing 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 µs, sys: 2.91 ms, total: 3.02 ms\n",
      "Wall time: 2.34 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_model3 = Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "exp_replay3 = ReplayBuffer(max_memory=MAX_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.32 s, sys: 177 ms, total: 8.5 s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dqn3 = DQN(baseline_model3, exp_replay3, 5, EPOCH, BATCH_SIZE, LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start Chasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 15:23:38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dac98cf24c4a009486a688ebb7ff89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 3.32292 s\n",
       "File: <ipython-input-87-1e430f3ba6d5>\n",
       "Function: train at line 93\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    93                                             def train(self):\n",
       "    94         1   16194974.0 16194974.0      0.5      self.model.to(DEVICE)\n",
       "    95         1        950.0    950.0      0.0      self.c_win_cnt = 0\n",
       "    96         1      40919.0  40919.0      0.0      self.model.train(True)\n",
       "    97                                           \n",
       "    98         1        738.0    738.0      0.0      for e in self.epochs:\n",
       "    99         1        361.0    361.0      0.0        self.rec_cnt = 0\n",
       "   100         1        335.0    335.0      0.0        self.win_cnt = 0\n",
       "   101         1        395.0    395.0      0.0        self.loss = 0.\n",
       "   102         1       1624.0   1624.0      0.0        self.epsilon = 4 / ((e + 1) ** (1 / 2))\n",
       "   103                                           \n",
       "   104         1      99669.0  99669.0      0.0        print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
       "   105                                                 # ------------------- Episode (User) -------------------------------\n",
       "   106         5   29169483.0 5833896.6      0.9        for asid in tqdm(self.__episodes()):\n",
       "   107         5       3671.0    734.2      0.0          self.asid = asid\n",
       "   108         5   13840796.0 2768159.2      0.4          self.__user_episode_context()\n",
       "   109                                                   \n",
       "   110                                                   # ----------------- Runs (User x All_Stream) ---------------------\n",
       "   111        40      74011.0   1850.3      0.0          for i, stream in enumerate(self.stream_list):\n",
       "   112        40      42038.0   1051.0      0.0            game_over = stream == self.final_stream\n",
       "   113        40      17114.0    427.9      0.0            self.current_stream = stream\n",
       "   114        40   14281161.0 357029.0      0.4            self.current_state = self.__full_state(i)\n",
       "   115        40      54286.0   1357.2      0.0            self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
       "   116                                                     \n",
       "   117                                                     # --------------- Explore/Exploit Section ----------------------\n",
       "   118        40   10822590.0 270564.8      0.3            self.action_ids = self.__choose_actions_eps()\n",
       "   119                                                     \n",
       "   120                                                     # --------------- Get next state & info to store ---------------\n",
       "   121        40   11733166.0 293329.2      0.4            reward = self.__reward()\n",
       "   122        40   13252099.0 331302.5      0.4            next_state = self.__full_state(i+1) if not game_over else []\n",
       "   123        40     155896.0   3897.4      0.0            next_stream = 0 if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
       "   124        40     110756.0   2768.9      0.0            self.exp_replay.remember([[stream, next_stream], self.current_state, self.action_ids, reward, next_state], game_over)\n",
       "   125                                                     \n",
       "   126                                                     # --------------- Load batch of experiences --------------------\n",
       "   127        40 3137809074.0 78445226.8     94.4            inputs, targets = self.exp_replay.get_batch(self.model, batch_size=self.batch_size)\n",
       "   128        40    5304262.0 132606.5      0.2            inputs, targets = df_to_tensor(inputs).to(DEVICE), df_to_tensor(targets).to(DEVICE)\n",
       "   129        40   69784301.0 1744607.5      2.1            batch_loss = self.__train_agent_batch(inputs, targets)\n",
       "   130        40      38686.0    967.1      0.0            self.loss += batch_loss      \n",
       "   131                                                     \n",
       "   132                                                 \n",
       "   133                                                 # Track win history to later check if our model is improving at the game over time.\n",
       "   134         1       1001.0   1001.0      0.0        self.hist.append(self.win_cnt)\n",
       "   135         1        693.0    693.0      0.0        self.c_hist.append(self.c_win_cnt)\n",
       "   136         1        586.0    586.0      0.0        self.rec_list.append(self.rec_cnt)\n",
       "   137                                           \n",
       "   138         1        295.0    295.0      0.0        print(f'Epoch: {e}/{len(self.epochs)} | Loss {self.loss} | Epoch Hit Rate {self.win_cnt/self.rec_cnt} | Cumulative Hit Rate {self.c_win_cnt/sum(self.rec_list)} |\\\n",
       "   139         1      83647.0  83647.0      0.0                Time {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
       "   140         1        357.0    357.0      0.0        break"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100 | Loss 2.4590116441249847 | Epoch Hit Rate 0.775 | Cumulative Hit Rate 0.775 |              Time 15:23:41\n"
     ]
    }
   ],
   "source": [
    "%lprun -f dqn3.train dqn3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0848023 s\n",
       "File: <ipython-input-86-f9f91d8b7747>\n",
       "Function: get_batch at line 26\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    26                                             def get_batch(self, model, batch_size=10):\n",
       "    27                                               # How many experiences do we have?\n",
       "    28         1       1013.0   1013.0      0.0      len_memory = len(self.memory)\n",
       "    29                                           \n",
       "    30                                               # Calculate the number of actions that can possibly be taken in the game.\n",
       "    31                                               # Actions: 0 = not recommend, 1 = recommend\n",
       "    32         1        741.0    741.0      0.0      num_actions = self.model_output_shape\n",
       "    33                                           \n",
       "    34                                               # Dimensions of our observed states, ie, the input to our model.\n",
       "    35                                               # Memory:  [\n",
       "    36                                               #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
       "    37                                               #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
       "    38                                               #   ...\n",
       "    39                                               # ]\n",
       "    40         1        512.0    512.0      0.0      env_dim = len(INPUT_DF_COL)\n",
       "    41                                           \n",
       "    42         1   23397393.0 23397393.0     27.6      inputs = pd.DataFrame(columns=INPUT_DF_COL)\n",
       "    43         1    1719888.0 1719888.0      2.0      targets = pd.DataFrame(columns=[0])\n",
       "    44                                               \n",
       "    45                                               \n",
       "    46                                               # We draw states to learn from randomly\n",
       "    47         2      40406.0  20203.0      0.0      for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
       "    48                                                 # Here we load one transition <s, a, r, s'> from memory\n",
       "    49         2      12107.0   6053.5      0.0        streams, state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
       "    50         2        582.0    291.0      0.0        current_stream, next_stream = streams\n",
       "    51         2        705.0    352.5      0.0        game_over = self.memory[idx][1]\n",
       "    52                                           \n",
       "    53                                                 '''\n",
       "    54                                                 修改倒入 state 的方式 input = (state - item) + item_feat\n",
       "    55                                                 拆掉 model_predict 成 function\n",
       "    56                                                 \n",
       "    57                                                 here should be state_t * all_items\n",
       "    58                                                 '''\n",
       "    59         2   24825667.0 12412833.5     29.3        state_t = get_input_v2(state_t, current_stream).astype('float32')\n",
       "    60                                                 # puts state into input\n",
       "    61         2    1505652.0 752826.0      1.8        inputs = pd.concat([inputs, state_t], axis=0)\n",
       "    62                                           \n",
       "    63                                                 '''\n",
       "    64                                                 每個 actions 都會被 predict 一個成績/reward\n",
       "    65                                                 '''\n",
       "    66                                                 # if the game ended, the reward is the final reward\n",
       "    67         2        491.0    245.5      0.0        if game_over:  # if game_over is True\n",
       "    68                                                   state_t['reward'] = reward_t\n",
       "    69                                                   targets = pd.concat([targets, reward_t], axis=0).astype('float32')\n",
       "    70                                                 else:\n",
       "    71         2    1833815.0 916907.5      2.2          state_t['reward'] = model(df_to_tensor(state_t).to(DEVICE)).detach().cpu().numpy()\n",
       "    72                                                   # 找到 action_t 們，指到 state_t 上去算 discount values\n",
       "    73         2     686727.0 343363.5      0.8          action_t = list(set(action_t[action_t == 1].index))\n",
       "    74                                                   \n",
       "    75         2   23827017.0 11913508.5     28.1          state_tp1 = get_input_v2(state_tp1, next_stream)\n",
       "    76         2    1203817.0 601908.5      1.4          Q_sa = np.max(model(df_to_tensor(state_tp1).to(DEVICE)).detach().cpu().numpy())\n",
       "    77                                                   # r + gamma * max Q(s',a')\n",
       "    78                                                   # DataFrame apply\n",
       "    79         2     195845.0  97922.5      0.2          state_t.loc[state_t['item_id'].isin(action_t)]['reward'] = state_t.loc[state_t['item_id'] \\\n",
       "    80         2    1266091.0 633045.5      1.5                                                                            .isin(action_t)]['reward'] \\\n",
       "    81         2     562086.0 281043.0      0.7                                                                            .apply(lambda x: x + self.discount * Q_sa) \\\n",
       "    82         2    1512456.0 756228.0      1.8                                                                            .astype('float32')\n",
       "    83         2    2209093.0 1104546.5      2.6          targets = pd.concat([targets, state_t['reward']], axis=0).astype('float32')\n",
       "    84         1        236.0    236.0      0.0      return inputs, targets"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f exp_replay3.get_batch exp_replay3.get_batch(baseline_model3, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per hit\n",
    "* exp/get_input:\n",
    "  * v1: `49609136.0`\n",
    "  * v2: `12412833.5`\n",
    "* train/get_batch:\n",
    "  * v1: `216278453.8`\n",
    "  * v2: `78445226.8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(85)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])\n",
    "if len(predicts) > 10:\n",
    "  ind = np.argpartition(predicts, -10)[-10:]\n",
    "  q_val = predicts[ind].sum()\n",
    "else:\n",
    "  q_val = predicts.sum()\n",
    "q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
