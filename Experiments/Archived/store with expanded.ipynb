{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember with Expanded Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data.sampler as sampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import line_profiler\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "  np.random.seed(seed)  \n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "CONTEXT_REPS = pd.read_pickle('../../data/CONTEXT_REPS_CLEAN.pkl')\n",
    "STREAM_ITEM_DICT = pd.read_pickle('../../data/stream_item_dict.pkl')\n",
    "BERT_BY_IDX_DF = pd.read_pickle('../../data/bert_by_idx_pca.pkl')\n",
    "BOUGHT_DICT = pd.read_pickle('../../data/bought_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1333083, 219), 7701, (162189, 160), 79207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_REPS.shape, len(STREAM_ITEM_DICT), BERT_BY_IDX_DF.shape, len(BOUGHT_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "USER_LIST = CONTEXT_REPS.index.get_level_values('asid').unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "LB_ITEMS = ['item_id'] + [f'i{x}' for x in range(160)]\n",
    "INPUT_DF_COL__USR = CONTEXT_REPS.columns.to_list()\n",
    "INPUT_DF_COL = INPUT_DF_COL__USR + LB_ITEMS\n",
    "\n",
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "Convert state format to model input format\n",
    "'''\n",
    "def get_input_tensor(input_state, current_stream, with_tensor=False):\n",
    "  # Get item feats\n",
    "  # STREAM_ITEM_DICT: 要拿到對的 STREAM\n",
    "  item_list = STREAM_ITEM_DICT[current_stream]\n",
    "  item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
    "\n",
    "  # Fill in other context\n",
    "  stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
    "  \n",
    "  # Merge with items\n",
    "  stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
    "  \n",
    "  # Convert to tensor\n",
    "  if with_tensor: \n",
    "    stream_item_feat_tensor = df_to_tensor(stream_item_feat)\n",
    "    return stream_item_feat_tensor, stream_item_feat\n",
    "  else:\n",
    "    return stream_item_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "METHOD FOR BOTH EXP_REPLAY & DQN\n",
    "\n",
    "Generate series: whether elements in A existed in list B\n",
    "A, B: List\n",
    "return: pd.Series\n",
    "example:\n",
    "  A: [1, 2, 4, 5]\n",
    "  B: [1, 2, 3, 4, 5, 6, 7]\n",
    "  return: Series([1, 1, 0, 1, 1, 0, 0], index=[1, 2, 3, 4, 5, 6, 7])\n",
    "'''\n",
    "def gen_exist_series(A, B):\n",
    "  return [int(item in A) for item in B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def df_to_tensor(input_df):\n",
    "  return torch.tensor(input_df.values).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self, max_memory=100000, discount=.9, model_output_shape=1):\n",
    "    \"\"\"\n",
    "    Setup\n",
    "    max_memory: the maximum number of experiences we want to store\n",
    "    memory: a list of experiences\n",
    "    discount: the discount factor for future experience\n",
    "    In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "    [...\n",
    "    [experience, game_over]\n",
    "    [experience, game_over]\n",
    "    ...]\n",
    "    \"\"\"\n",
    "    self.max_memory = max_memory\n",
    "    self.memory = list()\n",
    "    self.discount = discount\n",
    "    self.model_output_shape = model_output_shape\n",
    "    self.input_dim = 380\n",
    "    self.next_state_list = {}\n",
    "\n",
    "  def remember(self, states, game_over):\n",
    "    # Save a state to memory\n",
    "    current_state, reward, next_state, next_stream = states\n",
    "    for i in range(len(reward)):\n",
    "      self.memory.append([[current_state[i].view(1, self.input_dim), reward.iloc[i], next_state, next_stream], game_over])\n",
    "      \n",
    "    # We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "    if len(self.memory) > self.max_memory:\n",
    "      del self.memory[0]\n",
    "\n",
    "  def get_batch(self, eval_net, target_net, structure, batch_size=10):\n",
    "    # How many experiences do we have?\n",
    "    len_memory = len(self.memory)\n",
    "    # Dimensions of our observed states, ie, the input to our model.\n",
    "    # Memory:  [\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
    "    #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
    "    #   ...\n",
    "    # ]\n",
    "    env_dim = len(INPUT_DF_COL)\n",
    "\n",
    "    inputs = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
    "    targets = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    # We draw states to learn from randomly\n",
    "    for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
    "      # Here we load one transition <s, a, r, s'> from memory\n",
    "      state_t, reward, next_state, next_stream = self.memory[idx][0]\n",
    "      game_over = self.memory[idx][1]\n",
    "      \n",
    "      # puts state into input\n",
    "      inputs = torch.cat((inputs, state_t), dim=0)\n",
    "\n",
    "      # if the game ended, the reward is the final reward\n",
    "      if game_over:  # if game_over is True\n",
    "        current_target = torch.tensor([reward]).to(DEVICE).float()\n",
    "      else:\n",
    "        state_tp1, _ = get_input_tensor(next_state, next_stream, with_tensor=True)\n",
    "        \n",
    "        if target_net == None:\n",
    "          with torch.no_grad():\n",
    "            Q_sa = torch.max(eval_net(state_tp1))\n",
    "        elif structure == 'target':\n",
    "          with torch.no_grad():\n",
    "            Q_sa = torch.max(target_net(state_tp1))\n",
    "        elif structure == 'double':\n",
    "          with torch.no_grad():\n",
    "            _, selected_actions = eval_net(state_tp1).max(dim=0, keepdim=True)\n",
    "            Q_sa = target_net(state_tp1).gather(dim=0, index=selected_actions)          \n",
    "        # r + gamma * max Q(s',a')\n",
    "        current_target = torch.tensor([reward + self.discount * Q_sa]).to(DEVICE)\n",
    "      # concat targets\n",
    "      targets = torch.cat((targets, current_target), 0)\n",
    "    targets = targets.view(len(targets), 1)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "\n",
    "class Epsilon(ABC):\n",
    "  @abstractmethod\n",
    "  def clear(self):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def get_epsilon(self, key):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_step(self, key, data, delta):\n",
    "    pass\n",
    "  \n",
    "  @abstractmethod\n",
    "  def update_at_epoch(self, data):\n",
    "    pass\n",
    "  \n",
    "  # @abstractmethod\n",
    "  # def update_at_epsisode():\n",
    "  #   pass\n",
    "\n",
    "\n",
    "class Decay(Epsilon):\n",
    "  # Ref: Decay(0.5, 0.85)\n",
    "  '''\n",
    "  Epsilon Decay EE method with update/decay at epoch\n",
    "  '''\n",
    "  def __init__(self, initial, epoch_decay, step_decay=1.0):\n",
    "    self.initial = initial\n",
    "    self.epoch_decay, self.step_decay = epoch_decay, step_decay\n",
    "    self.epsilon = self.initial\n",
    "    \n",
    "  def clear(self):\n",
    "    self.epsilon = self.initial # should be 4 for origin setting\n",
    "    \n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon\n",
    "  \n",
    "  def update_at_step(self, key, data, delta):\n",
    "    # origin setting\n",
    "    pass\n",
    "    # exponentially\n",
    "    # self.epsilon *= self.step_decay\n",
    "    \n",
    "  def update_at_epoch(self, data):\n",
    "    # origin settings\n",
    "    # epoch = data\n",
    "    # self.epsilon = 4 / ((epoch + 1) ** (1 / 2))\n",
    "    # exponentially\n",
    "    self.epsilon *= self.epoch_decay\n",
    "\n",
    "\n",
    "class VDBE(Epsilon):\n",
    "  # VDBE(0.5, 0.01)\n",
    "  def __init__(self, initial, sigma):\n",
    "    self.initial = initial\n",
    "    self.sigma = sigma\n",
    "\n",
    "  def clear(self):\n",
    "    self.epsilon = defaultdict(lambda: self.initial)\n",
    "\n",
    "  def get_epsilon(self, key):\n",
    "    return self.epsilon[key]\n",
    "  \n",
    "  def update_at_step(self, key, data, delta):\n",
    "    td_error = data\n",
    "    coeff = math.exp(-abs(td_error) / self.sigma)\n",
    "    f = (1.0 - coeff) / (1.0 + coeff)\n",
    "    self.epsilon[key] = delta * f + (1.0 - delta) * self.epsilon[key]\n",
    "  \n",
    "  def update_at_epoch(self, data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "  def __init__(self, structure, exp_replay, epsilon, num_episode, epochs, batch_size, lr, switch_param_threshold, single_reward):\n",
    "    self.eval_net = Net()\n",
    "    self.target_net = Net() if not structure == 'vanilla' else None\n",
    "    self.structure = structure\n",
    "    self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n",
    "    self.loss_fn = nn.MSELoss()\n",
    "    self.exp_replay = exp_replay\n",
    "    self.epsilon = epsilon\n",
    "    self.num_episode = num_episode\n",
    "    self.epochs = epochs\n",
    "    self.batch_size = batch_size\n",
    "    self.switch_param_threshold = switch_param_threshold\n",
    "    self.single_reward = single_reward\n",
    "    self.hist = []\n",
    "    self.rec_list = []\n",
    "    self.ep_score_list = []\n",
    "    self.current_user_hit_list = [] # current_asid's hit actions\n",
    "    self.all_hit_list = [] # all asids' hist actions, asid actual buy lenght\n",
    "    self.avg_hit_list = [] # avg hit/all list\n",
    "    self.current_acc = []\n",
    "    self.all_acc_list = []\n",
    "    self.learn_step_counter = 0\n",
    "\n",
    "  # Environment Methods\n",
    "  def __episodes(self):\n",
    "    # return USER_LIST[:self.num_episode]\n",
    "    return np.random.choice(USER_LIST, self.num_episode, replace=False)\n",
    "  \n",
    "  def __user_episode_context(self):\n",
    "    self.user_all_streams = CONTEXT_REPS.xs(self.asid, level=\"asid\")\n",
    "    self.stream_list = self.user_all_streams.index\n",
    "    self.final_stream = max(self.stream_list)\n",
    "\n",
    "  def reward(self):\n",
    "    '''\n",
    "    Comparison function for reward, 考慮「所有」歷史購買紀錄\n",
    "    '''\n",
    "    real_bought_ids = BOUGHT_DICT[self.asid]\n",
    "    real_bought_ids_series = gen_exist_series(real_bought_ids, self.stream_items)\n",
    "    \n",
    "    reward_list = []\n",
    "    for a, b, c in zip(real_bought_ids_series, self.action_ids, self.action_ids.index):\n",
    "      reward_list.append(a & b)\n",
    "      if a & b: self.current_user_hit_list.append(c)\n",
    "      \n",
    "    # Reward Count \n",
    "    self.rec_cnt += 1\n",
    "    if sum(reward_list) > 0:\n",
    "      self.win_cnt += 1\n",
    "      self.ep_score += sum(reward_list)\n",
    "    self.current_acc.append(sum(reward_list))\n",
    "\n",
    "    # return list(map(lambda x: x * sum(reward_list), reward_list))\n",
    "    if self.single_reward:\n",
    "      return pd.Series(reward_list, index=self.stream_items)\n",
    "    else:\n",
    "      return pd.Series(list(map(lambda x: x * sum(reward_list), reward_list)), index=self.stream_items)\n",
    "\n",
    "  # Agent Methods\n",
    "  def __choose_actions(self):\n",
    "    if np.random.rand() <= self.epsilon.get_epsilon(self.asid):\n",
    "    # if len(self.exp_replay.memory) < 1:\n",
    "      # Explore by randomly select 10/n items from candidate_items\n",
    "      # Get all items from the stream\n",
    "      self.explore += 1\n",
    "      selected_actions = random.sample(self.stream_items, 10) if len(self.stream_items) > 10 else self.stream_items\n",
    "    else:\n",
    "      # Exploit by choosing action from the model's prediction\n",
    "      self.exploit += 1\n",
    "      selected_actions = self.__agent_predict()\n",
    "    x = pd.Series(0, index=self.stream_items)\n",
    "    x.loc[selected_actions] = 1\n",
    "    return x\n",
    "    \n",
    "  def q_value(self): \n",
    "    if type(self.epsilon) == Decay: return 0\n",
    "    with torch.no_grad():\n",
    "      predicts = self.eval_net(self.full_input).flatten()    \n",
    "    actions_idx = np.where(self.action_ids.values == 1)[0]\n",
    "    q_val = predicts[actions_idx].mean()\n",
    "    return q_val\n",
    "\n",
    "  def __agent_predict(self):\n",
    "    with torch.no_grad():\n",
    "      predicts = self.eval_net(self.full_input).flatten()\n",
    "    if len(predicts) > 10:\n",
    "      top10_idx = torch.topk(predicts, 10).indices.cpu()\n",
    "      actions = self.candidate_actions.iloc[top10_idx]['item_id'].values\n",
    "    else:\n",
    "      actions = self.candidate_actions['item_id'].values\n",
    "    return actions\n",
    "\n",
    "  def __train_agent_batch(self, inputs, targets):\n",
    "    self.optimizer.zero_grad()\n",
    "    outputs = self.eval_net(inputs)\n",
    "    loss = self.loss_fn(outputs, targets)\n",
    "    # Add CL Regularization Term\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "  # MAIN TRAIN\n",
    "  def train(self):\n",
    "    self.eval_net.to(DEVICE)\n",
    "    if self.target_net:\n",
    "      self.target_net.to(DEVICE)\n",
    "    self.eval_net.train(True)\n",
    "    self.epsilon.clear()\n",
    "\n",
    "    for e in self.epochs:\n",
    "      self.rec_cnt = 0\n",
    "      self.win_cnt = 0\n",
    "      self.loss = 0.\n",
    "      self.ep_score = 0\n",
    "      self.explore = 0\n",
    "      self.exploit = 0\n",
    "\n",
    "      print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
    "      # ------------------- Episode (User) -------------------------------\n",
    "      for asid in tqdm(self.__episodes()):\n",
    "        self.asid = asid\n",
    "        self.__user_episode_context()\n",
    "\n",
    "        # ----------------- Runs (User x All_Stream) ---------------------\n",
    "        for i, stream in enumerate(self.stream_list):\n",
    "          game_over = stream == self.final_stream\n",
    "          self.current_stream = stream\n",
    "          self.current_state = self.user_all_streams.loc[stream]\n",
    "          self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
    "          self.full_input, self.candidate_actions = get_input_tensor(self.current_state, self.current_stream, with_tensor=True)\n",
    "\n",
    "          # --------------- Explore/Exploit Section ----------------------\n",
    "          self.action_ids = self.__choose_actions()\n",
    "\n",
    "          # --------------- Get next state & info to store ---------------\n",
    "          reward = self.reward()\n",
    "          # Calculate score for each users\n",
    "          if game_over:\n",
    "            self.all_hit_list.append([len(set(self.current_user_hit_list)), len(BOUGHT_DICT[self.asid])])\n",
    "            self.current_user_hit_list = []\n",
    "            self.all_acc_list.append(self.current_acc)\n",
    "            self.current_acc = []\n",
    "\n",
    "          next_state = self.user_all_streams.loc[self.stream_list[i + 1]] if not game_over else []\n",
    "          next_stream = None if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
    "          # if next_stream is None:\n",
    "          #   next_state = []\n",
    "          # else: \n",
    "          #   next_state, _ = get_input_tensor(next_state, next_stream, with_tensor=True)\n",
    "          \n",
    "          self.exp_replay.remember([self.full_input, reward, next_state, next_stream], game_over)\n",
    "          # for updating target network\n",
    "          self.learn_step_counter += 1\n",
    "          if self.target_net and (self.learn_step_counter % self.switch_param_threshold == 0):\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "\n",
    "\n",
    "          # --------------- Load batch of experiences --------------------\n",
    "          inputs, targets = self.exp_replay.get_batch(self.eval_net, self.target_net, self.structure, batch_size=self.batch_size)\n",
    "          \n",
    "          # store pre-training value for td_error\n",
    "          old_Q = self.q_value()\n",
    "          batch_loss = self.__train_agent_batch(inputs, targets)\n",
    "          # store post-training value for td_error\n",
    "          new_Q = self.q_value()\n",
    "          self.loss += batch_loss\n",
    "\n",
    "          # --------------- Update with TD error -------------------------\n",
    "          self.epsilon.update_at_step(self.asid, (new_Q - old_Q), len(self.stream_items))\n",
    "          \n",
    "      self.epsilon.update_at_epoch(e)\n",
    "\n",
    "      # Track win history to later check if our model is improving at the game over time.\n",
    "      self.hist.append(self.win_cnt)\n",
    "      self.rec_list.append(self.rec_cnt)\n",
    "      self.ep_score_list.append(self.ep_score)\n",
    "\n",
    "      print(f'Epoch: {e}/{len(self.epochs)} | Loss {self.loss:.2f} | Epoch Hit Rate {(self.win_cnt/self.rec_cnt):.2f} | \\\n",
    "Cumulative Hit Rate {(sum(self.hist)/sum(self.rec_list)):.2f} | Explore {self.explore} | Exploit {self.exploit} | \\\n",
    "Score {self.ep_score} | Rec_cnt: {self.rec_cnt}')\n",
    "      start = e * 100\n",
    "      end = start + 100\n",
    "      hit = [a[0] for a in self.all_hit_list[start:end]]\n",
    "      bought = [a[1] for a in self.all_hit_list[start:end]]\n",
    "      print(sum(hit)/sum(bought))\n",
    "      self.avg_hit_list.append(sum(hit)/sum(bought))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 10000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 50  # Number of experiences we use for training per batch\n",
    "EPOCH = range(100)\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "LR = 1.0e-3\n",
    "SWITCH_PARAM_THRESHOLD = 100\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(380, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, 128)\n",
    "    self.fc4 = nn.Linear(128, 64)\n",
    "    self.fc5 = nn.Linear(64, 1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.tanh = nn.Tanh()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc3(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.fc4(x)\n",
    "    x = self.tanh(x)\n",
    "    x = self.fc5(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp: single experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.93 GiB total capacity; 4.00 GiB already allocated; 5.75 MiB free; 4.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9a7b3294d2a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                   \u001b[0mNUM_EPISODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                   LR, SWITCH_PARAM_THRESHOLD, single_reward=False)\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdqn_test2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-10fb327dfd70>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.93 GiB total capacity; 4.00 GiB already allocated; 5.75 MiB free; 4.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "EPS_target_rsum_x_positive\n",
    "\n",
    "- VDBE(0.5, 0.01)\n",
    "- Decay(0.5, 0.85)\n",
    "'''\n",
    "\n",
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)\n",
    "epsilon = Decay(0.99, 0.92)\n",
    "dqn_test2 = DQN('target', exp_replay, epsilon, \n",
    "                                  NUM_EPISODE, EPOCH, BATCH_SIZE, \n",
    "                                  LR, SWITCH_PARAM_THRESHOLD, single_reward=False)\n",
    "dqn_test2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hit = [a[0] for a in EPS_target_rsum_x_positive.all_hit_list]\n",
    "bought = [a[1] for a in EPS_target_rsum_x_positive.all_hit_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16709346991037133"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hit)/sum(bought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline100 = pd.read_pickle('../Experiments Results/res_baseline_100_vanilla.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "res = [a/b for a, b in zip(dqn_test.hist, dqn_test.rec_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f82b5884cf8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(EPS_target_rsum_x_positive.ep_score_list, label='hit')\n",
    "# plt.plot(EPS_target_rsum_x_positive.avg_hit_list, label='res')\n",
    "plt.plot(res, label='VDBE hit@10 ma 30')\n",
    "# for l in re:\n",
    "#   print(l)\n",
    "#   plt.axvline(x = l, color = 'red')\n",
    "# plt.plot(pd.Series(bought).rolling(1000).mean(), label='VDBE hit@10 ma 30')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Hit Ratio')\n",
    "plt.title('Hit@10 for BERT-baseline')\n",
    "# plt.ylim([0.4, 0.6])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Models/EPS_target_rsum_x_positive.pkl', 'wb') as file_pi:\n",
    "  pickle.dump(EPS_target_rsum_x_positive, file_pi, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n請不要關掉這ㄍ分頁 乾蝦哈咪搭\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "請不要關掉這ㄍ分頁 乾蝦哈咪搭\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "asd = pd.read_pickle('../Models/EPS_target_rsum_x_positive.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 22, 32, 36, 50, 61, 126, 141, 200, 272]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = 0\n",
    "re = []\n",
    "for a in length:\n",
    "  res1 += a\n",
    "  re.append(res1)\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = [len(a) for a in dqn_test2.all_acc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [a/10 for a in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [item for sublist in dqn_test2.all_acc_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.   Time: 18:02:52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69015efac5194e078a8521deef0581a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1 | Loss 17.80 | Epoch Hit Rate 0.42 | Cumulative Hit Rate 0.42 | Explore 270 | Exploit 2 | Score 145 | Rec_cnt: 272\n",
      "0.112876254180602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 232.12 s\n",
       "File: <ipython-input-32-10fb327dfd70>\n",
       "Function: train at line 104\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   104                                             def train(self):\n",
       "   105         1     637092.0 637092.0      0.0      self.eval_net.to(DEVICE)\n",
       "   106         1        468.0    468.0      0.0      if self.target_net:\n",
       "   107         1     474846.0 474846.0      0.0        self.target_net.to(DEVICE)\n",
       "   108         1      50158.0  50158.0      0.0      self.eval_net.train(True)\n",
       "   109         1       1963.0   1963.0      0.0      self.epsilon.clear()\n",
       "   110                                           \n",
       "   111         1        728.0    728.0      0.0      for e in self.epochs:\n",
       "   112         1        337.0    337.0      0.0        self.rec_cnt = 0\n",
       "   113         1        233.0    233.0      0.0        self.win_cnt = 0\n",
       "   114         1        192.0    192.0      0.0        self.loss = 0.\n",
       "   115         1        479.0    479.0      0.0        self.ep_score = 0\n",
       "   116         1        226.0    226.0      0.0        self.explore = 0\n",
       "   117         1        380.0    380.0      0.0        self.exploit = 0\n",
       "   118                                           \n",
       "   119         1     112442.0 112442.0      0.0        print(f'Epoch {e} started.   Time: {datetime.now(pytz.timezone(\"Asia/Taipei\")).strftime(\"%H:%M:%S\")}')\n",
       "   120                                                 # ------------------- Episode (User) -------------------------------\n",
       "   121        10   40857950.0 4085795.0      0.0        for asid in tqdm(self.__episodes()):\n",
       "   122        10      20624.0   2062.4      0.0          self.asid = asid\n",
       "   123        10   24891622.0 2489162.2      0.0          self.__user_episode_context()\n",
       "   124                                           \n",
       "   125                                                   # ----------------- Runs (User x All_Stream) ---------------------\n",
       "   126       272     496141.0   1824.0      0.0          for i, stream in enumerate(self.stream_list):\n",
       "   127       272     125664.0    462.0      0.0            game_over = stream == self.final_stream\n",
       "   128       272     105350.0    387.3      0.0            self.current_stream = stream\n",
       "   129       272   51587150.0 189658.6      0.0            self.current_state = self.user_all_streams.loc[stream]\n",
       "   130       272     369262.0   1357.6      0.0            self.stream_items = STREAM_ITEM_DICT[self.current_stream]\n",
       "   131       272 5546661606.0 20392138.3      2.4            self.full_input, self.candidate_actions = get_input_tensor(self.current_state, self.current_stream, with_tensor=True)\n",
       "   132                                           \n",
       "   133                                                     # --------------- Explore/Exploit Section ----------------------\n",
       "   134       272  162263236.0 596556.0      0.1            self.action_ids = self.__choose_actions()\n",
       "   135                                           \n",
       "   136                                                     # --------------- Get next state & info to store ---------------\n",
       "   137       272  701183401.0 2577880.2      0.3            reward = self.reward()\n",
       "   138                                                     # Calculate score for each users\n",
       "   139       262      90692.0    346.2      0.0            if game_over:\n",
       "   140        10      45615.0   4561.5      0.0              self.all_hit_list.append([len(set(self.current_user_hit_list)), len(BOUGHT_DICT[self.asid])])\n",
       "   141        10       8850.0    885.0      0.0              self.current_user_hit_list = []\n",
       "   142        10       6180.0    618.0      0.0              self.all_acc_list.append(self.current_acc)\n",
       "   143        10       2421.0    242.1      0.0              self.current_acc = []\n",
       "   144                                           \n",
       "   145       272   39939704.0 146837.1      0.0            next_state = self.user_all_streams.loc[self.stream_list[i + 1]] if not game_over else []\n",
       "   146       272    1030405.0   3788.3      0.0            next_stream = None if (i + 1) == len(self.stream_list) else self.stream_list[i + 1]\n",
       "   147                                                     # if next_stream is None:\n",
       "   148                                                     #   next_state = []\n",
       "   149                                                     # else: \n",
       "   150                                                     #   next_state, _ = get_input_tensor(next_state, next_stream, with_tensor=True)\n",
       "   151                                                     \n",
       "   152       272  825680910.0 3035591.6      0.4            self.exp_replay.remember([self.full_input, reward, next_state, next_stream], game_over)\n",
       "   153                                                     # for updating target network\n",
       "   154       272     244677.0    899.5      0.0            self.learn_step_counter += 1\n",
       "   155       270     309513.0   1146.3      0.0            if self.target_net and (self.learn_step_counter % self.switch_param_threshold == 0):\n",
       "   156         2     578580.0 289290.0      0.0              self.target_net.load_state_dict(self.eval_net.state_dict())\n",
       "   157                                           \n",
       "   158                                           \n",
       "   159                                                     # --------------- Load batch of experiences --------------------\n",
       "   160       272 224247331679.0 824438719.4     96.6            inputs, targets = self.exp_replay.get_batch(self.eval_net, self.target_net, self.structure, batch_size=self.batch_size)\n",
       "   161                                                     \n",
       "   162                                                     # store pre-training value for td_error\n",
       "   163       272     775583.0   2851.4      0.0            old_Q = self.q_value()\n",
       "   164       272  472496654.0 1737120.1      0.2            batch_loss = self.__train_agent_batch(inputs, targets)\n",
       "   165                                                     # store post-training value for td_error\n",
       "   166       272     545745.0   2006.4      0.0            new_Q = self.q_value()\n",
       "   167       272     222000.0    816.2      0.0            self.loss += batch_loss\n",
       "   168                                           \n",
       "   169                                                     # --------------- Update with TD error -------------------------\n",
       "   170       272     584374.0   2148.4      0.0            self.epsilon.update_at_step(self.asid, (new_Q - old_Q), len(self.stream_items))\n",
       "   171                                                     \n",
       "   172         1       4245.0   4245.0      0.0        self.epsilon.update_at_epoch(e)\n",
       "   173                                           \n",
       "   174                                                 # Track win history to later check if our model is improving at the game over time.\n",
       "   175         1       1568.0   1568.0      0.0        self.hist.append(self.win_cnt)\n",
       "   176         1       1256.0   1256.0      0.0        self.rec_list.append(self.rec_cnt)\n",
       "   177         1        962.0    962.0      0.0        self.ep_score_list.append(self.ep_score)\n",
       "   178                                           \n",
       "   179         1        496.0    496.0      0.0        print(f'Epoch: {e}/{len(self.epochs)} | Loss {self.loss:.2f} | Epoch Hit Rate {(self.win_cnt/self.rec_cnt):.2f} | \\\n",
       "   180                                           Cumulative Hit Rate {(sum(self.hist)/sum(self.rec_list)):.2f} | Explore {self.explore} | Exploit {self.exploit} | \\\n",
       "   181         1      56343.0  56343.0      0.0  Score {self.ep_score} | Rec_cnt: {self.rec_cnt}')\n",
       "   182         1        402.0    402.0      0.0        start = e * 100\n",
       "   183         1        300.0    300.0      0.0        end = start + 100\n",
       "   184         1       4136.0   4136.0      0.0        hit = [a[0] for a in self.all_hit_list[start:end]]\n",
       "   185         1       1993.0   1993.0      0.0        bought = [a[1] for a in self.all_hit_list[start:end]]\n",
       "   186         1      19538.0  19538.0      0.0        print(sum(hit)/sum(bought))\n",
       "   187         1       1176.0   1176.0      0.0        self.avg_hit_list.append(sum(hit)/sum(bought))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_replay2 = ReplayBuffer(max_memory=MAX_MEMORY)\n",
    "epsilon2 = Decay(0.99, 0.92)\n",
    "dqn_test2 = DQN('target', exp_replay2, epsilon2, \n",
    "                10, range(1), BATCH_SIZE, \n",
    "                LR, SWITCH_PARAM_THRESHOLD, single_reward=False)\n",
    "%lprun -f dqn_test2.train dqn_test2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.902651 s\n",
       "File: <ipython-input-17-d72273982dae>\n",
       "Function: get_batch at line 31\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    31                                             def get_batch(self, eval_net, target_net, structure, batch_size=10):\n",
       "    32                                               # How many experiences do we have?\n",
       "    33         1       1209.0   1209.0      0.0      len_memory = len(self.memory)\n",
       "    34                                               # Dimensions of our observed states, ie, the input to our model.\n",
       "    35                                               # Memory:  [\n",
       "    36                                               #   [ [ [stream, next_stream], [...state], action, reward, next_state_idx], game_over],\n",
       "    37                                               #   [ [ [stream, next_stream], [...state], action, reward, nexr_state_idx], game_over],\n",
       "    38                                               #   ...\n",
       "    39                                               # ]\n",
       "    40         1        348.0    348.0      0.0      env_dim = len(INPUT_DF_COL)\n",
       "    41                                           \n",
       "    42         1      48176.0  48176.0      0.0      inputs = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
       "    43         1     319422.0 319422.0      0.0      targets = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
       "    44                                               \n",
       "    45                                               \n",
       "    46                                               # We draw states to learn from randomly\n",
       "    47        50     165765.0   3315.3      0.0      for i, idx in enumerate(np.random.randint(0, len_memory, size=min(len_memory, batch_size))):  \n",
       "    48                                                 # Here we load one transition <s, a, r, s'> from memory\n",
       "    49        50      76133.0   1522.7      0.0        state_t, reward, next_state, next_stream = self.memory[idx][0]\n",
       "    50        50      13354.0    267.1      0.0        game_over = self.memory[idx][1]\n",
       "    51                                                 \n",
       "    52                                                 # puts state into input\n",
       "    53        50    2233486.0  44669.7      0.2        inputs = torch.cat((inputs, state_t), dim=0)\n",
       "    54                                           \n",
       "    55                                                 # if the game ended, the reward is the final reward\n",
       "    56        50      14083.0    281.7      0.0        if game_over:  # if game_over is True\n",
       "    57                                                   current_target = torch.tensor([reward]).to(DEVICE).float()\n",
       "    58                                                 else:\n",
       "    59        50  872667014.0 17453340.3     96.7          state_tp1, _ = get_input_tensor(next_state, next_stream, with_tensor=True)\n",
       "    60                                                   \n",
       "    61        50      32628.0    652.6      0.0          if target_net == None:\n",
       "    62                                                     with torch.no_grad():\n",
       "    63                                                       Q_sa = torch.max(eval_net(state_tp1))\n",
       "    64        50      17807.0    356.1      0.0          elif structure == 'target':\n",
       "    65        50     282932.0   5658.6      0.0            with torch.no_grad():\n",
       "    66        50   19629415.0 392588.3      2.2              Q_sa = torch.max(target_net(state_tp1))\n",
       "    67                                                   elif structure == 'double':\n",
       "    68                                                     with torch.no_grad():\n",
       "    69                                                       _, selected_actions = eval_net(state_tp1).max(dim=0, keepdim=True)\n",
       "    70                                                       Q_sa = target_net(state_tp1).gather(dim=0, index=selected_actions)          \n",
       "    71                                                   # r + gamma * max Q(s',a')\n",
       "    72        50    4702367.0  94047.3      0.5          current_target = torch.tensor([reward + self.discount * Q_sa]).to(DEVICE)\n",
       "    73                                                 # concat targets\n",
       "    74        50    2435046.0  48700.9      0.3        targets = torch.cat((targets, current_target), 0)\n",
       "    75         1      11467.0  11467.0      0.0      targets = targets.view(len(targets), 1)\n",
       "    76         1        251.0    251.0      0.0      return inputs, targets"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f exp_replay2.get_batch exp_replay2.get_batch(dqn_test2.eval_net, dqn_test2.target_net, dqn_test2.structure, batch_size=dqn_test2.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 0.0298468 s\n",
       "File: <ipython-input-6-389433be4f32>\n",
       "Function: get_input_tensor at line 9\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     9                                           def get_input_tensor(input_state, current_stream, with_tensor=False):\n",
       "    10                                             # Get item feats\n",
       "    11                                             # STREAM_ITEM_DICT: 要拿到對的 STREAM\n",
       "    12         1       2396.0   2396.0      0.0    item_list = STREAM_ITEM_DICT[current_stream]\n",
       "    13         1    1909910.0 1909910.0      6.4    item_feat = BERT_BY_IDX_DF.loc[item_list].reset_index().rename(columns={'index': 'item_id'})\n",
       "    14                                           \n",
       "    15                                             # Fill in other context\n",
       "    16         1   10619140.0 10619140.0     35.6    stream_item_feat = pd.DataFrame([input_state]*len(item_list)).reset_index(drop=True)\n",
       "    17                                             \n",
       "    18                                             # Merge with items\n",
       "    19         1    1221339.0 1221339.0      4.1    stream_item_feat = stream_item_feat.merge(item_feat, left_index=True, right_index=True).astype('float32')\n",
       "    20                                             \n",
       "    21                                             # Convert to tensor\n",
       "    22         1        191.0    191.0      0.0    if with_tensor: \n",
       "    23         1   16093498.0 16093498.0     53.9      stream_item_feat_tensor = df_to_tensor(stream_item_feat)\n",
       "    24         1        292.0    292.0      0.0      return stream_item_feat_tensor, stream_item_feat\n",
       "    25                                             else:\n",
       "    26                                               return stream_item_feat"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f get_input_tensor get_input_tensor(CONTEXT_REPS.iloc[0], CONTEXT_REPS.iloc[0].name[1], with_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
