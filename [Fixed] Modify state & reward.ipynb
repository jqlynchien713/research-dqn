{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80297ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb1e0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('data/6897-1y-c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13093d",
   "metadata": {},
   "source": [
    "# User representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340eb32e",
   "metadata": {},
   "source": [
    "## Handle Username Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a10db570-2f9f-47a5-81b4-1d1f8f335883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_invalid(string):\n",
    "    string = string.replace('-', '')\n",
    "    string = string.replace('/', '')\n",
    "    string = string.replace('*', '')\n",
    "    string = string.replace('劉德玲', '')\n",
    "    return string\n",
    "\n",
    "DATA['聯絡電話'] = DATA['聯絡電話'].apply(replace_invalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6deffe3",
   "metadata": {},
   "source": [
    "## Handle User Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3c652",
   "metadata": {},
   "source": [
    "### 處理 period （只取前六碼）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a04cc7a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA['period'] = DATA['下單日期'].astype(str).apply(lambda x: x[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64707882",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "### 處理 category\n",
    "* 要先做 label encoding 才能做 one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1daa08bb-6e23-4638-9887-b79064d130d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "DATA['category_label'] = le.fit_transform(DATA['商品分類'])\n",
    "DATA = pd.concat([DATA, pd.get_dummies(DATA['category_label'], prefix='cat')], axis = 1)\n",
    "\n",
    "DATA['shipment'] = le.fit_transform(DATA['運送方式'])\n",
    "DATA['payment'] = le.fit_transform(DATA['付款方式'])\n",
    "DATA = pd.get_dummies(DATA,\n",
    "                      prefix=['shipment', 'payment'],\n",
    "                      columns=['shipment', 'payment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307d482",
   "metadata": {},
   "source": [
    "### Create `item_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2b3ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating item ids\n",
    "shuffled_items = DATA['商品名稱'].sample(frac=1).reset_index(drop=True).unique()\n",
    "item_dict = { x: i for i, x in enumerate(shuffled_items) }\n",
    "\n",
    "DATA['item_id'] = DATA['商品名稱'].map(item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9a8f6",
   "metadata": {},
   "source": [
    "## Preprocessing for each users #2\n",
    "### 處理 Periods 切割"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15391f",
   "metadata": {},
   "source": [
    "### Combine periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0558ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the row axis labels \n",
    "LB_PERIOD = list(DATA['period'].unique())\n",
    "LB_USER = ['user']\n",
    "LB_PQ = ['total_price', 'total_quantity']\n",
    "LB_CAT = [f'cat_{x}' for x in list(range(228))]\n",
    "LB_SHIPMENT_PAYMENT = ['shipment_0', 'shipment_1', 'shipment_2', 'shipment_3', 'payment_0', 'payment_1']\n",
    "USER_LB = LB_PERIOD + LB_USER + LB_PQ + LB_CAT + LB_SHIPMENT_PAYMENT\n",
    "\n",
    "def generate_user_series(sample_user_key):\n",
    "    user_sample = DATA.loc[DATA['聯絡電話'] == sample_user_key]\n",
    "    user_sum = user_sample.loc[:, LB_SHIPMENT_PAYMENT+['總金額', '數量']+LB_CAT].sum()\n",
    "    \n",
    "    # Creating the Series \n",
    "    res_series = pd.Series([0]*254) \n",
    "    # Creating the row axis labels \n",
    "    res_series.index = USER_LB\n",
    "    \n",
    "    # Period\n",
    "    res_series[user_sample['period'].unique()] = 1\n",
    "    # User\n",
    "    res_series['user'] = sample_user_key\n",
    "    # Total Price & Quantities\n",
    "    res_series['total_price'] = user_sum['總金額']\n",
    "    res_series['total_quantity'] = user_sum['數量']\n",
    "    # Shipment & Payment\n",
    "    res_series[LB_SHIPMENT_PAYMENT] = user_sum[LB_SHIPMENT_PAYMENT]\n",
    "    # Categories\n",
    "    res_series[LB_CAT] = user_sum[LB_CAT]\n",
    "    \n",
    "    return res_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc8a20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_df():\n",
    "    user_df = pd.DataFrame(index=DATA['聯絡電話'].unique(), columns=USER_LB)\n",
    "    user_df = user_df.apply(lambda x: generate_user_series(x.name), axis=1, result_type='expand')\n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c247c8d",
   "metadata": {},
   "source": [
    "# Item Representation\n",
    "- 商品 id 來源為 `item_dict`: `商品名稱: item_id`\n",
    "- 可用欄位： `categories(one-hot)`, `price`, `被購買次數`\n",
    "- Columns: `['下單日期', '商品名稱', '規格', '單價', '數量', '折扣', '總金額', '專屬折扣', '運費', '信用卡手續費', '紅利折抵', '收款金額', '付款方式', '運送方式', '收件人', '寄送地址', '聯絡電話', '場次', '處理後名稱', '商品分類', 'period', 'category_label', 0-227, 'shipment_0', 'shipment_1', 'shipment_2', 'shipment_3', 'payment_0', 'payment_1', 'item_id']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c659067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LB_ITEMS = ['item_id', 'avg_price', 'count'] + LB_CAT\n",
    "\n",
    "def get_item_df():\n",
    "    item_df = pd.DataFrame(columns=LB_ITEMS)\n",
    "    item_df['item_id'] = DATA.item_id.unique()\n",
    "    # Count\n",
    "    item_count = DATA.groupby('item_id').size()\n",
    "    item_df['count'] = item_df['item_id'].apply(lambda x: item_count[x])\n",
    "    # Cat\n",
    "    item_df.loc[:, LB_CAT] = DATA.groupby('item_id').sum()[LB_CAT]\n",
    "    # Price\n",
    "    item_df['avg_price'] = DATA.groupby('item_id').mean()['單價']\n",
    "    return item_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509737a",
   "metadata": {},
   "source": [
    "# Context representation\n",
    "* Columns: `period, user, item, bought(reward)`\n",
    "* Generating item ids?\n",
    "\n",
    "#### Considering Other Context [pending]\n",
    "* Example: time, weekday, and the freshness of the news (the gap between request time and news publish time)\n",
    "* 候選欄位：下單時間、產品上架時間（新鮮度）、直播主相關資料 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "126da554",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB_AP_CAT = [f'ap_{x}' for x in LB_CAT]\n",
    "\n",
    "def get_context_df():\n",
    "    # Generating context representation\n",
    "    context_col = ['user', 'item', 'reward'] + LB_AP_CAT\n",
    "    context_df = pd.DataFrame(columns=context_col)\n",
    "\n",
    "    context_df['user'] = DATA['聯絡電話']\n",
    "    context_df['item'] = DATA['item_id']\n",
    "    context_df['discount'] = DATA['折扣']\n",
    "    context_df['date'] = DATA['下單日期']\n",
    "    context_df['period'] = DATA['period']\n",
    "    context_df['reward'] = 1\n",
    "    context_df = context_df.sort_values(by=['user', 'date'])\n",
    "    \n",
    "    # Add appeared cat in date as feature\n",
    "    item_cat_sum_by_date = DATA.groupby('下單日期').sum()[LB_CAT]\n",
    "    ap_cat_df = context_df.apply(lambda x: item_cat_sum_by_date.loc[x['date']], axis=1, result_type='expand')\n",
    "    context_df[LB_AP_CAT] = ap_cat_df\n",
    "    \n",
    "    return context_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2394b4f",
   "metadata": {},
   "source": [
    "---\n",
    "# Train DQN model\n",
    "* Input: `user_df` 253, `item_df` 231(BERT: 768), interact (?), `reward` 1\n",
    "* Output: recommend a list of items\n",
    "* Methods Needed\n",
    "    * Environment Function\n",
    "    * Choose Action\n",
    "    * Store Transition\n",
    "    * Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a7f02",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "515d84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve Performance\n",
    "# context_df is global variable\n",
    "# get current reward, next state and determine whether the episode ended\n",
    "def env_take_action(current_state_idx, current_user, action, reward_set='strict'):\n",
    "    # init return outputs\n",
    "    game_over = False\n",
    "    current_state = context_df.iloc[current_state_idx]\n",
    "    context_same_user_all = context_df.loc[context_df.user == current_user]\n",
    "    context_same_user_next = context_same_user_all.loc[(current_state_idx + 1):, :]\n",
    "    \n",
    "    # Reward set conditions\n",
    "    if reward_set == 'strict':\n",
    "        reward = 1 if current_state['item'] == action else 0\n",
    "    elif reward_set == 'loose-all':\n",
    "        # 不能跳 idx 因為每局的 action 不一樣\n",
    "        reward = 1 if action in context_same_user_all['item'].unique() else 0\n",
    "    elif reward_set == 'loose-after':\n",
    "        reward = 1 if action in context_same_user_next['item'].unique() else 0\n",
    "        \n",
    "    # Print if any reward added\n",
    "    if reward == 1: print('reward +')\n",
    "    \n",
    "    # Check if next state exist\n",
    "    if context_same_user_next.shape[0] == 0:\n",
    "        next_state_idx = None\n",
    "        reward = 0\n",
    "        game_over = True\n",
    "    else:\n",
    "        next_state_idx = context_same_user_next.iloc[0].idx\n",
    "    \n",
    "    return next_state_idx, reward, game_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68f24636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all inputs(user + item based on context_idx)\n",
    "# used at init state and store transition\n",
    "def get_full_inputs(context_idx, item_cat = False):\n",
    "    current_context = context_df.iloc[context_idx]\n",
    "    full_input = user_df.loc[user_df.user == current_context.user]\n",
    "    full_input.loc[:, 'discount'] = current_context['discount']\n",
    "    full_input.loc[:, 'date'] = current_context['date']\n",
    "    if item_cat:\n",
    "        # 原本 user context 就有 item cates (購買過的商品種類總和)\n",
    "        # 這次新增的 0-227 是該期的所有 item cates，因此需要在 0-227 加上 prefix\n",
    "        state_cate_cols = [f'state_cat_{x}' for x in range(228)]\n",
    "        state_cate = pd.DataFrame(current_context.loc[range(228)], columns=state_cate_cols) # Add appeared items\n",
    "        full_input = pd.concat([full_input, state_cate]).astype('float32')\n",
    "    return full_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1db7a",
   "metadata": {},
   "source": [
    "## Collecting Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b4b19f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_memory=100000, discount=.9):\n",
    "        \"\"\"\n",
    "        Setup\n",
    "        max_memory: the maximum number of experiences we want to store\n",
    "        memory: a list of experiences\n",
    "        discount: the discount factor for future experience\n",
    "        In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "        [...\n",
    "        [experience, game_over]\n",
    "        [experience, game_over]\n",
    "        ...]\n",
    "        \"\"\"\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        # Save a state to memory\n",
    "        self.memory.append([states, game_over])\n",
    "        # We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10, item_cat=False):\n",
    "\n",
    "        # How many experiences do we have?\n",
    "        len_memory = len(self.memory)\n",
    "\n",
    "        # Calculate the number of actions that can possibly be taken in the game.\n",
    "        num_actions = model.output_shape[-1]\n",
    "\n",
    "        # Dimensions of our observed states, ie, the input to our model.\n",
    "        # Memory:  [\n",
    "        #   [[ [...state], action, reward, next_state_idx], game_over],\n",
    "        #   [[ [...state], action, reward, nexr_state_idx], game_over],\n",
    "        #   ...\n",
    "        # ]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "\n",
    "        # We want to return an input and target vector with inputs from an observed state.\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "\n",
    "        # ...and the target r + gamma * max Q(s’,a’)\n",
    "        # Note that our target is a matrix, with possible fields not only for the action taken but also for\n",
    "        # the other possible actions. The actions not take the same value as the prediction to not affect them\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "\n",
    "        # We draw states to learn from randomly\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            \"\"\"\n",
    "            Here we load one transition <s, a, r, s’> from memory\n",
    "            state_t: initial state s\n",
    "            action_t: action taken a\n",
    "            reward_t: reward earned r\n",
    "            state_tp1: the state that followed s’\n",
    "            \"\"\"\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            state_t = state_t.astype('float32')\n",
    "\n",
    "            # We also need to know whether the game ended at this state\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            # add the state s to the input\n",
    "            inputs[i:i + 1] = state_t\n",
    "\n",
    "            # First we fill the target values with the predictions of the model.\n",
    "            # They will not be affected by training (since the training loss for them is 0)\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "\n",
    "            \"\"\"\n",
    "            If the game ended, the expected reward Q(s,a) should be the final reward r.\n",
    "            Otherwise the target value is r + gamma * max Q(s’,a’)\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "            # if the game ended, the reward is the final reward\n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # Find next state representation\n",
    "                state_tp1 = get_full_inputs(state_tp1, item_cat)\n",
    "                # Convert state_tp1 to float32\n",
    "                state_tp1 = state_tp1.astype('float32')\n",
    "                # Here Q_sa is max_a'Q(s', a')\n",
    "                Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "                # r + gamma * max Q(s’,a’)\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80edbaf",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16054a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the given game\n",
    "def train(model, exp_replay, epochs, batch_size, num_episode=100, verbose=1, item_cat=False, reward_set='strict'):\n",
    "    total_actions = item_df.shape[0]\n",
    "    total_episodes = user_df.shape[0]\n",
    "    # Reseting the win counter\n",
    "    win_cnt = 0\n",
    "    # We want to keep track of the progress of the AI over time, so we save its win count history \n",
    "    # indicated by number of goals scored\n",
    "    win_hist = []\n",
    "\n",
    "    # Epochs is the number of games we play\n",
    "    for e in range(epochs):\n",
    "        loss = 0.\n",
    "        # epsilon for exploration - dependent inversely on the training epoch\n",
    "        epsilon = 4 / ((e + 1) ** (1 / 2))\n",
    "\n",
    "        # handling episodes by assigning users from user_df\n",
    "        episodes = random.sample(range(total_episodes), num_episode)\n",
    "\n",
    "        # Episode start\n",
    "        print(f'Epoch {e} started.')\n",
    "\n",
    "        for user_episode in episodes:\n",
    "            game_over = False\n",
    "            # get current state s by observing our game environment\n",
    "            # TODO: Init state for each epoch, find the \n",
    "            user_phone = user_df.iloc[user_episode].user\n",
    "            next_state_idx = context_df.loc[context_df['user'] == user_phone].iloc[0].idx\n",
    "#             print('------------------Episode------------------')\n",
    "            \n",
    "            while not game_over:\n",
    "                # The learner is acting on the last observed game screen\n",
    "                # next_state is a vector containing representing the game screen\n",
    "                current_state_idx = next_state_idx\n",
    "                current_state = get_full_inputs(current_state_idx, item_cat)\n",
    "\n",
    "                # We choose our action from either exploration (random) or exploitation (model).\n",
    "                if np.random.rand() <= epsilon:\n",
    "                    # Explore a random action\n",
    "                    action_id = int(np.random.randint(0, total_actions, size=1))\n",
    "                else:\n",
    "                    # Choose action from the model's prediction\n",
    "                    # q contains the expected rewards for the actions\n",
    "                    q = model.predict(current_state)\n",
    "                    # We pick the action with the highest expected reward\n",
    "                    action_id = np.argmax(q[0])\n",
    "                    print(q[0])\n",
    "                    print(q[0].shape)\n",
    "                    print('\\n')\n",
    "\n",
    "                # apply action, get rewards r and new state s'\n",
    "                next_state_idx, reward, game_over = env_take_action(current_state_idx, user_phone, action_id, reward_set)\n",
    "                # If we managed to score a goal we add 1 to our win counter\n",
    "                if reward == 1:\n",
    "                    win_cnt += 1\n",
    "\n",
    "                \"\"\"\n",
    "                The experiences < s, a, r, s' > we make during gameplay are our training data.\n",
    "                Here we first save the last experience, and then load a batch of experiences to train our model\n",
    "                \"\"\"\n",
    "                # store experience\n",
    "                #   Full input\n",
    "                action = item_df.loc[item_df.item_id == action_id]\n",
    "                # state_input = pd.concat([current_state.reset_index(), action.reset_index()], axis=1).astype('float32')\n",
    "                exp_replay.remember([current_state.astype('float32'), action_id, reward, next_state_idx], game_over)\n",
    "\n",
    "                # Load batch of experiences\n",
    "                inputs, targets = exp_replay.get_batch(model, batch_size=batch_size, item_cat=item_cat)\n",
    "                # print(targets, targets.shape)\n",
    "\n",
    "                # train model on experiences\n",
    "                batch_loss = model.train_on_batch(inputs, targets)\n",
    "\n",
    "                loss += batch_loss\n",
    "\n",
    "        # Episode end\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(\"Epoch: {:03d}/{:03d} | Loss {:.4f} | Win count {}\".format(e, epochs, loss, win_cnt))\n",
    "        \n",
    "        # Track win history to later check if our model is improving at the game over time.\n",
    "        win_hist.append(win_cnt)\n",
    "    return win_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79db83e6",
   "metadata": {},
   "source": [
    "## Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3349a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = get_user_df()\n",
    "item_df = get_item_df()\n",
    "context_df = get_context_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "481cec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 10  # Number of experiences we use for training per batch\n",
    "EPOCH = 500\n",
    "TOTAL_ACTIONS = item_df.shape[0]\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "880357e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13272\\4090393269.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m              \u001b[0mnum_episode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPISODE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m              \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m              reward_set='strict')\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13272\\1298933559.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, exp_replay, epochs, batch_size, num_episode, verbose, item_cat, reward_set)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m# TODO: Init state for each epoch, find the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0muser_phone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muser_episode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mnext_state_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcontext_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0muser_phone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m#             print('------------------Episode------------------')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\joanna\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\joanna\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\joanna\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1498\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m     \u001b[1;31m# -------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)# Our model's architecture parameters\n",
    "input_size = 256 # The input shape for model - this comes from the output shape of the CNN Mobilenet\n",
    "\n",
    "# Setting up the model with keras.\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(HIDDEN_SIZE, input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dense(HIDDEN_SIZE, activation='tanh'))\n",
    "model.add(Dense(TOTAL_ACTIONS))\n",
    "model.compile(Adam(learning_rate=.000001), \"mse\")\n",
    "\n",
    "# Training the model\n",
    "hist = train(model, \n",
    "             exp_replay, \n",
    "             epochs=EPOCH, \n",
    "             batch_size=BATCH_SIZE, \n",
    "             num_episode=NUM_EPISODE, \n",
    "             verbose=1, \n",
    "             reward_set='strict')\n",
    "plt.plot(range(EPOCH), hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bb785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df2169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b4dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed386db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd710a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec03c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f17b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d20538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f8acc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51eff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07c5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236b124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a52e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42738e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df95cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae295c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65adaa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c9df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
