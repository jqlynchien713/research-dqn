{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9a7ad6-670e-4dd1-828d-4ee498a9fcba",
   "metadata": {},
   "source": [
    "# DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5709b2ed-10b1-4e5c-9e5d-9d223cf9462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import USER_STREAM_CONTEXT, ITEM_DF, ITEM_STREAM_DF, REAL_BOUGHT_DF, LAST_BOUGHT_STREAM, LB_ITEMS, USER_LIST, LB_CE, STREAM_LIST\n",
    "from utils import gen_exist_series, get_full_state, get_reward, calculate_interest_change, get_input, model_predict_top10, INPUT_DF_COL\n",
    "from replay import ReplayBuffer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b2f337-3ba6-4bca-8de1-514ccefeb20e",
   "metadata": {},
   "source": [
    "## Train DQN model\n",
    "* Input: `user_df` 253, `item_df` 231(BERT: 768), interact (?), `reward` 1\n",
    "* Output: recommend a list of items\n",
    "* Methods Needed\n",
    "    * Environment Function\n",
    "    * Choose Action\n",
    "    * Store Transition\n",
    "    * Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b9c35-0617-4c9a-ba18-445a3539616e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adba99bf-3034-444c-be67-a43efd5a273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Process\n",
    "\n",
    "def train(model, exp_replay, epochs, batch_size, num_episode=1000, verbose=1, reward_set='strict'):\n",
    "  # total_actions = ITEM_DF.shape[0]\n",
    "  total_episodes = len(USER_LIST)\n",
    "  # Reset win counter\n",
    "  win_cnt = 0\n",
    "  win_hist = []\n",
    "\n",
    "  for e in range(epochs):\n",
    "    loss = 0.\n",
    "    # TODO/MAIN: Apply user preference changes as epsilon\n",
    "    # epsilon for exploration - dependent inversely on the training epoch\n",
    "    epsilon = 4 / ((e + 1) ** (1 / 2))\n",
    "\n",
    "    # handling episodes by assigning users from USER_LIST\n",
    "    # Each user represent an Episode\n",
    "    episodes = random.sample(range(total_episodes), num_episode)\n",
    "\n",
    "    print(f'Epoch {e} started.')\n",
    "    # ------------------- Episode (User) -------------------------------\n",
    "    for user_episode in episodes:\n",
    "      # game_over = False\n",
    "      # get episode data by user phone number\n",
    "      user_phone = USER_LIST[user_episode]\n",
    "      user_all_streams = USER_STREAM_CONTEXT.xs(user_phone, level=\"聯絡電話\")\n",
    "      stream_list = user_all_streams.index\n",
    "      final_stream = LAST_BOUGHT_STREAM.loc[user_phone, '場次']\n",
    "      \n",
    "      \n",
    "      # ----------------- Runs (User x All_Stream) ---------------------\n",
    "      for i, stream in enumerate(stream_list):          \n",
    "        game_over = stream == final_stream\n",
    "                \n",
    "        # Get full state: current_state = user_stream + item_stream\n",
    "        # 用上一場紀錄預測下一場直播會購買的商品\n",
    "        current_state = get_full_state(user_all_streams, stream_list, i)\n",
    "        stream_items = current_state['cand_item']\n",
    "        \n",
    "        # --------------- Explore/Exploit Section ----------------------\n",
    "        if np.random.rand() <= epsilon:\n",
    "          # Explore by randomly select 10/n items from candidate_items\n",
    "          # Get all items from the stream\n",
    "          sample_actions = random.sample(stream_items, 10) if len(stream_items) > 10 else stream_items\n",
    "          action_ids = gen_exist_series(sample_actions, stream_items)\n",
    "        else:\n",
    "          # Exploit by choosing action from the model's prediction\n",
    "          pred_actions = model_predict_top10(model, current_state)\n",
    "          action_ids = gen_exist_series(pred_actions, stream_items)\n",
    "\n",
    "        # --------------- Get next state & info to store ---------------\n",
    "        reward = get_reward(user_phone, stream, action_ids)\n",
    "        next_state = get_full_state(user_all_streams, stream_list, i+1) if not game_over else []\n",
    "\n",
    "        if sum(reward) > 0:\n",
    "          win_cnt += 1\n",
    "\n",
    "        # --------------- Calculating Interest Changes -----------------\n",
    "        interest_score = calculate_interest_change(user_all_streams, stream_list, i)\n",
    "\n",
    "        # --------------- Store Experience -----------------------------\n",
    "        exp_replay.remember(interest_score,\n",
    "                            [current_state, action_ids, reward, next_state],\n",
    "                            game_over)\n",
    "        \n",
    "\n",
    "        # --------------- Load batch of experiences --------------------\n",
    "        inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "        # train model on experiences\n",
    "        batch_loss = model.train_on_batch(inputs, targets)\n",
    "        loss += batch_loss\n",
    "            \n",
    "    if verbose > 0:\n",
    "      print(\"Epoch: {:03d}/{:03d} | Loss {:.4f} | Win count {}\".format(e, epochs, loss, win_cnt))\n",
    "    \n",
    "    # Track win history to later check if our model is improving at the game over time.\n",
    "    win_hist.append(win_cnt)\n",
    "  return win_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d28e144-e732-4036-9765-8386ca3fbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "# parameters\n",
    "MAX_MEMORY = 1000  # Maximum number of experiences we are storing\n",
    "BATCH_SIZE = 10  # Number of experiences we use for training per batch\n",
    "EPOCH = 50\n",
    "TOTAL_ACTIONS = 1 # probability of ordering\n",
    "NUM_EPISODE = 100\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd4b39-e518-4b08-8356-bf61ce94210b",
   "metadata": {},
   "source": [
    "### Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0f1af-977d-40b8-aad5-8e5771c55fca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d46f13dee83e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m              \u001b[0mnum_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPISODE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m              \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m              reward_set='strict')\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cf9c4ca53530>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, exp_replay, epochs, batch_size, num_episode, verbose, reward_set)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# --------------- Load batch of experiences --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m# train model on experiences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-dqn/replay.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, model, batch_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# We draw states to learn from randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;31m# Here we load one transition <s, a, r, s'> from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "exp_replay = ReplayBuffer(max_memory=MAX_MEMORY)# Our model's architecture parameters\n",
    "input_size = 477 # The input shape for model - this comes from the output shape of the CNN Mobilenet\n",
    "\n",
    "# Setting up the model with keras.\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(HIDDEN_SIZE, input_shape=(input_size,), activation='relu'))\n",
    "model.add(Dense(HIDDEN_SIZE, activation='tanh'))\n",
    "model.add(Dense(TOTAL_ACTIONS))\n",
    "model.compile(Adam(learning_rate=.000001), \"mse\")\n",
    "\n",
    "\n",
    "# Training the model\n",
    "hist = train(model, \n",
    "             exp_replay, \n",
    "             epochs=EPOCH, \n",
    "             batch_size=BATCH_SIZE, \n",
    "             num_episode=NUM_EPISODE, \n",
    "             verbose=1, \n",
    "             reward_set='strict')\n",
    "plt.plot(range(EPOCH), hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa601d70-1d40-4fbe-a289-5478a8353291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb737c7a-7340-4cdd-9770-bdf426bec4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd9583-625b-4d9b-a67e-caf8f4e3ac37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f186de8-df64-4ff1-a0df-2457a36a1037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa523669-c179-45c0-af95-2ae273770a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7419220-7f26-4476-8011-5dfe4b1cda80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c6b18-e88b-436e-a34a-1e8ebb527c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471c923-0953-4fb6-adcb-1aab4796136c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b60988-b8fb-4035-b19b-ee73fb6c2f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f64715-795b-4508-9ce6-77438898f63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b49b19-994d-4717-9eee-b2555f466a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c3b40-cff1-4d05-98d1-209c4a72f050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8c5f0-10ea-446b-9b78-997eb4062b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5d26d15f81c32561881a39c85bdcf5a2542c694b6b4a6d5e9280707d5ee0206"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
